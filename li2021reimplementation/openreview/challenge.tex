\section{Challenges}
% Challenges: Challenges you faced when reimplmenting the paper and conducting the experiments. Were all details in the paper? Or did you have to look in the authors code or even contact them to find about some details? Was parts of the code quite hard to get them to work as intended? Did you have optimize and tune several hyperparameters? Which ones? Did the framework you used  make the implementation difficult in some ways?
It is not clear how many steps are there in each epoch. First the paper only states the total steps $K=2^{20}$ and the composition of one batch ($B$ labeled samples and $\mu B$ unlabeled samples). And the official code indicates there are $2^{16}$ labeled images observed by the model per epoch and a total of $2^{26}$ images observed which suggests that there are $2^{12}$ updates per epoch and $2^{19}$ updates in total. And this is not consistent with the total update steps $K$ stated in the paper. When performing weak augmentations to the input data, the probability for randomly translating images is not specified. And it also remains unclear the `5 different folds' mentioned in the paper, we are guessing it is a kind of cross validation while there is not too much evidence supporting this neither in the paper nor in the official code.

The paper doesn't contain sufficient details to reproduce all the experiments. Thus, it is necessary to look for details about reproducing the experiments in the official code. We have not optimized or tuned the hyperparameters, and all the hyperparameters are the same as those mentioned in the paper. 
Compared to the average error rates in the original paper, the reproduced results have a reasonable good performance on a larger number of labeled data (4000/250 labels) and better but also reasonable performance on fewer labeled data (40/10 labels) since the variance of error rates over 5 different folds for CIFAR-10 with 40 labels is $3.35\%$. 
% Some different implementations are considered. One implementation difference could be that we did not add weight decay parameters to the SGD optimizer. 
% Another implementation difference is that we used post-shifted Batch Normalization (PSBN) proposed in \citep{zhou2020does} instead of standard Batch Normalization (BN) in the wideResNet, aiming to prevent undesirable collapsed filters that are common in Deep Neural Networks with BN. Due to the time limitation, we didn't compare the performance of the model under PSBN and standard BN. 
% Moreover, in our experiment, we only consider RAugment that the magnitude is randomly selected in the predefined range, while the magnitude is learned during training in CTAugment.
Moreover, to compare with the results of ablation studies in the original paper, we also implement CTAugment, which supports a learnable magnitude. While we failed to confirm the result that CTAugment behaves better than RandAugment on CIFAR-10. We hypothetically guess this is because it could affect the consistency regularization because of different levels of distortions controlled by magnitude. 
\section{Related work} \label{sec:related_work}
As introduced in Sec. \ref{sec:intro}, confirmation error is a serious issue of "Match"-based SSL methods and our study is mainly about the confirmation error and FixMatch in the presence of noisy (pseudo) labels. Therefore, here we mainly introduce the noisy labeling and some related works for dealing with the noisy label and confirmation error in SSL.

\paragraph{Noisy labeling and noise-robust loss.}
Suppose a dataset $\mathcal{D} = \{(x_i,y_i)\}^n_{i=1}$ where $y_i$ is given by noisy labeling. To model noisy labeling process, we have $p(y_i | \widetilde{y_i})$ where $\widetilde{y_i}$ is the ground truth label under the assumption that the noise label is conditionally independent from the input data given the ground-true label; formally, $p(y_i = k | x_i, \widetilde{y_i}=j) = p(y_i=k | \widetilde{y_i}=j) = \eta_{kj}.$ In general, such noise is called class dependent, which is also named as the asymmetric noise\citep{zhang2018generalized}. In contrary, when $\eta_{kj} = \eta$, it is called symmetric noise. Under the symmetric noise assumption, \citet{ghosh2015making} studied the functional form of loss function and concluded that by using the symmetric loss function, one can get a global optima such that the learned model is noise tolerant. For example, the MAE loss function is a symmetric function while the cross entropy loss function is not. However, using MAE loss function has poor accuracy performance on classification tasks compared with the cross entropy loss function \citep{zhang2018generalized}. One can convince oneself with Eqn. (5) in \citep{zhang2018generalized}, i.e., the cross entropy loss function enables the optimization process weighting the sample importance while the MAE loss function considers samples equally. Furthermore, \citet{zhang2018generalized} combine MAE and cross entropy loss functions with L’Hôpital’s rule, i.e., 
\begin{eqnarray}
\mathcal{L}_q(f(x),j) =\frac{(1-f_j(x)^q)}{q},\label{eq:nrloss}
\end{eqnarray}
where $f(x)$ is the model, $j$ indexes the class, and $f_j(x)$ is the softmax output of $j$. Interestingly, when $q=1$ , $\mathcal{L}_q(f(x),j)$ is a MAE loss function; while $\lim_{q\to 0}\mathcal{L}_q(f(x),j)$ is a cross entropy loss. Therefore, one can manipulate trade off by selecting a good  hyper-parameter $q$. Furthermore, it also introduces a better loss function, the truncated $\mathcal{L}_q(f(x),j)$, which is essentially a practically improved version of  $\mathcal{L}_q(f(x),j)$. However, in theory the proposed method is based on the symmetric noise assumption \citep{zhang2018generalized}, which can be quite easy to be violated. This is a trade-off between using a stricter assumption and estimating noisy labelling mechanisms \citep{patrini2017making} (which is a challenge).

\paragraph{SSL for noisy labeling and a potential solution for asymmetric noise.}
\citet{li2020dividemix} consider the noisy label problem as a semi-supervised learning problem by finding the similarity of unlabeled samples in semi-supervised learning and noisy labels. Suppose that we can successfully separate the noisy and clean samples, we can treat the noisy ones as unlabeled data in semi-supervised learning, and then leverage the success of semi-supervised learning to tackle the noisy labeling problem. Firstly, by observing that the loss of clean samples tends to be lower than the noisy ones \citep{arazo2019unsupervised}, \citet{li2020dividemix} fit a Gaussian Mixture Model for the two components, the noisy group and the clean one. Then given a loss, it can be inferred whether the sample is a noisy one or a clean one. Consequently, following the mentioned idea, semi-supervised learning methods are applied to such a separated dataset. Moreover, \citet{li2020dividemix} consider the influence of asymmetric noise in the supervised learning phase. Because the bias introduced by the asymmetric noise can lead to severe consequences (confirmation errors). \citep{li2020dividemix} added a negative entropy penalty term $-\mathcal{H} = \sum_j f_j(x) \log f_j(x) $ for an input $x$ in the cross-entropy loss function at the beginning of training to avoid over-confident prediction, which works well emperically. To further reduce the influence of the confirmation error introduced by the symmetric noise, it uses the MixMatch \citep{berthelot2019mixmatch} procedure to train two independent DNNs and attractively exchange datasets with each other for filtering errors made by the other one. This is actually an ensemble method, which reduces the random noise in the prediction, especially in the presence of symmetric labelling noise. 

\paragraph{Model bias in SSL.}
\citet{kurakin2020remixmatch} propose a distribution alignment method utilizing a principle introduced by \citet{bridle1992unsupervised}. It formulates an ideal classifier which maximizes the mutual information of model inputs and model outputs. Furthermore, it argues that the second term of mutual information encourages a model to output with low entropy and high confidence, while another one encourages equal frequency across the entire training set as shown in 
\begin{eqnarray}
\mathcal{I}(y;x) &=& \iint \log \frac{p(y,x)}{p(y)p(x)} dy dx \nonumber \\
&=& \mathcal{H}[\mathbb{E}[p(y\mid x; \theta)]] - \mathbb{E}_x[\mathcal{H}[p(y\mid x; \theta)]], \label{eq:mi}
\end{eqnarray}
where $\theta$ is the model parameters. As what \citet{kurakin2020remixmatch} said, when the marginal distribution of a training dataset labels is not uniformly distributed, it is not proper to regularize the frequency. In our work, to deal with such case, we augment the training dataset and make the labels of labeled data in each batches to be uniformly distributed.
@article{li2020dividemix,
  title={Dividemix: Learning with noisy labels as semi-supervised learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  journal={ICLR},
  year={2020}
}

@misc{zoubin2020,
   title =  {Keynote: Machine Learning and A.I. At Uber},
   author = {Zoubin Ghahramani},
   howpublished = "\url{https://www.youtube.com/watch?v=4XTv5qgugCk&feature=youtu.be}",
   year = "2020"
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{kuznetsova2018open,
  title={The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale},
  author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Duerig, Tom and others},
  journal={arXiv preprint arXiv:1811.00982},
  year={2018}
}

@article{sohn2020fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Zhang, Han and Raffel, Colin},
  journal={arXiv preprint arXiv:2001.07685},
  year={2020}
}

@inproceedings{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  booktitle={Advances in neural information processing systems},
  pages={8778--8788},
  year={2018}
}

@misc{tarvainenweight,
  title={Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  publisher={CoRR}
}

@article{ghosh2015making,
  title={Making risk minimization tolerant to label noise},
  author={Ghosh, Aritra and Manwani, Naresh and Sastry, PS},
  journal={Neurocomputing},
  volume={160},
  pages={93--107},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{patrini2017making,
  title={Making deep neural networks robust to label noise: A loss correction approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1944--1952},
  year={2017}
}

@article{arazo2019unsupervised,
  title={Unsupervised label noise modeling and loss correction},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and O'Connor, Noel E and McGuinness, Kevin},
  journal={arXiv preprint arXiv:1904.11238},
  year={2019}
}

@inproceedings{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5049--5059},
  year={2019}
}

@article{tajbakhsh2016convolutional,
  title={Convolutional neural networks for medical image analysis: Full training or fine tuning?},
  author={Tajbakhsh, Nima and Shin, Jae Y and Gurudu, Suryakanth R and Hurst, R Todd and Kendall, Christopher B and Gotway, Michael B and Liang, Jianming},
  journal={IEEE transactions on medical imaging},
  volume={35},
  number={5},
  pages={1299--1312},
  year={2016},
  publisher={IEEE}
}

@InProceedings{pmlr-v124-kugelgen20a, title = {Semi-supervised learning, causality, and the conditional cluster assumption}, author = {von K\"{u}gelgen, Julius and Mey, Alexander and Loog, Marco and Sch\"{o}lkopf, Bernhard}, pages = {1--10}, year = {2020}, editor = {Jonas Peters and David Sontag}, volume = {124}, series = {Proceedings of Machine Learning Research}, address = {Virtual}, month = {03--06 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v124/kugelgen20a/kugelgen20a.pdf}, url = {http://proceedings.mlr.press/v124/kugelgen20a.html}, abstract = {While the success of semi-supervised learning (SSL) is still not fully understood, Sch√∂lkopf et al. (2012) have established a link to the principle of independent causal mechanisms. They conclude that SSL should be impossible when predicting a target variable from its causes, but possible when predicting it from its effects. Since both these cases are restrictive, we extend their work by considering classification using cause and effect features at the same time, such as predicting a disease from both risk factors and symptoms. While standard SSL exploits information contained in the marginal distribution of all inputs (to improve the estimate of the conditional distribution of the target given in-puts), we argue that in our more general setting we should use information in the conditional distribution of effect features given causal features. We explore how this insight generalises the previous understanding, and how it relates to and can be exploited algorithmically for SSL.} }

@inproceedings{ben2008does,
  title={Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.},
  author={Ben-David, Shai and Lu, Tyler and P{\'a}l, D{\'a}vid}
}

@article{chapelle2010semi,
  title={Semi-supervised Learning. Adaptive computation and machine learning},
  author={Chapelle, Olivier and Sch{\"o}lkopf, Bernhard and Zien, Alexander},
  journal={MIT Press, Cambridge, MA, USA. Cited in page (s)},
  volume={21},
  number={1},
  pages={2},
  year={2010}
}

@article{scholkopf2012causal,
  title={On causal and anticausal learning},
  author={Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  journal={arXiv preprint arXiv:1206.6471},
  year={2012}
}

@article{kurakin2020remixmatch,
  title={ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring},
  author={Kurakin, Alex and Raffel, Colin and Berthelot, David and Cubuk, Ekin Dogus and Zhang, Han and Sohn, Kihyuk and Carlini, Nicholas},
  year={2020}
}


@inproceedings{bridle1992unsupervised,
  title={Unsupervised Classifiers, Mutual Information and'Phantom Targets},
  author={Bridle, John S and Heading, Anthony JR and MacKay, David JC},
  booktitle={Advances in neural information processing systems},
  pages={1096--1101},
  year={1992}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={702--703},
  year={2020}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{grandvalet2005semi,
  title={Semi-supervised learning by entropy minimization},
  author={Grandvalet, Yves and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={529--536},
  year={2005}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@article{zhou2020does,
  title={How Does BN Increase Collapsed Neural Network Filters?},
  author={Zhou, Sheng and Wang, Xinjiang and Li, Wenjie and Feng, Litong and Luo, Ping},
  journal={arXiv preprint arXiv:2001.11216},
  year={2020}
}
# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it shoudl be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[¬Re]"
#  - For other article types, no instruction (but please, not too long)
title: "[Re] Reproducibility Study of ”Latent Space Smoothing for Individually Fair Representations”"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author
authors:
  - name: Didier Merk
    orcid: 0009-0005-0748-9018
    email: didier.merk@gmail.com
    affiliations: 1,*

  - name: Denny Smit
    orcid: 0009-0008-0939-9018
    email: dennysmit@hotmail.com
    affiliations: 1

  - name: Boaz Beukers
    orcid: 0009-0005-9879-9205
    email: boazbeuk007@gmail.com
    affiliations: 1

  - name: Tsatsral Mendsuren
    orcid: 0009-0007-0292-0213
    email: tsatsral.mendsuren@gmail.com
    affiliations: 1

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code:    1
    name:    University of Amsterdam, Science Park, FNWI Department
    address: Amsterdam, Netherlands

# List of keywords (adding the programming language might be a good idea)
keywords: machine learning, rescience c, rescience x, Latent space smoothing, LASSI, reproducibility, fairness, AI, deep learning, python, machine learning

# Code URL and DOI (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo,
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/Mametchiii/lassi-reproducibility
  - doi: 10.5281/zenodo.7950717
  - swh: swh:1:dir:ebe7321bfcc8268ca48b1b269c64b6fe1df79653

# Date URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
 - cite: Momchil Peychev, Anian Ruoss, Mislav Balunovi and Maximilian Baader and Martin Vechev. “Latent Space Smoothing for Individually Fair Representations”
 - bib:  peychev2022latent
 - url:  https://arxiv.org/abs/2111.13650v3
 - doi:  10.1007/978-3-031-19778-9_31

# Don't forget to surround abstract with double quotes
abstract: "- Scope of Reproducibility
The aim of this work is to study the reproducibility of the paper 'Latent Space Smoothing for Individually Fair Representations' by Peychev et al., in which a novel representation learning method called LASSI is proposed. We aim to verify the three main claims made in the original paper: (1) LASSI increases certified individual fairness, while keeping prediction accuracies high, (2) LASSI can handle various sensitive attributes and attribute vectors and (3) LASSI representations can achieve high certified individual fairness even when downstream tasks are not known. In addition, we aim to test the robustness of their claims by conducting additional experiments.
- Methodology 
To reproduce the experiments, we use the step-by-step guidelines supplied by the original authors on their github repository. The experimental setup and datasets used are identical to the work reported in the original paper. We write additional code to run experiments beyond the scope of the work done by Peychev et al. In order to comply with resource limitations, we reproduce only the experiments relevant to the main claims. In total a budget of 45 hours on an NVIDIA Titan RTX GPU is used.
- Results:
We are able to reproduce and verify the three main claims of the original paper, by reproducing the results within 5 percent of the reported values. The additional experiments were succesful and strengthen the claims that LASSI increases certified individual fairness compared to the baseline models. Outliers of the experiments are studied and found to be caused by biased and inaccurate input data.
- What was easy:
Reproducing the original experiments was made possible by the extensive documentation and guidelines created by the authors in their code and public GitHub repository. The theoretical background provided in their paper was clear and detailed, allowing a deeper understanding about the inner workings of their models and metrics.
- What was difficult:
The main difficulty was found within the complex structure of the original code files and the related functions across these files. The code needed to perform our additional experiments was therefore also complex and required us to alter many different functions in the original code."

# Bibliography file (yours)
bibliography: bibliography.bib
  
# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2022

# Coding language (main one only if several)
language: Python

  
# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review: 
  - url: https://openreview.net/forum?id=J-Lgb7Vc0wX

contributors:
  - name:
    orcid: 
    role: editor
  - name:
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer

# This information will be provided by the editor
dates:
  - received:
  - accepted:
  - published: 

# This information will be provided by the editor
article:
  - number: 1 # Article number will be automatically assigned during publication
  - doi:    10.5281/zenodo.7950717
  - url:    # Final PDF URL (Zenodo or rescience website?)

# This information will be provided by the editor
journal:
  - name:   "ReScience C"
  - issn:   2430-3658
  - volume: 9
  - issue:  2

% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/ymahlau/subgraphx}
\def \codeDOI{https://zenodo.org/badge/latestdoi/637344138}
\def \codeSWH{swh:1:dir:439719e0ad99cbd3d980619c24dec1744b408dd0}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{04 February 2023}
\def \dateACCEPTED{21 April 2023}
\def \datePUBLISHED{15 June 2023}
\def \articleTITLE{[Re] On Explainability of Graph Neural Networks via Subgraph Explorations}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2022}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2023}
\def \reviewURL{https://openreview.net/forum?id=zKBJw4Ht8s}
\def \articleABSTRACT{Yuan et al. claim their proposed method SubgraphX achieves (i) higher fidelity in explaining models for graph- and node classification tasks compared to other explanation techniques, namely GNNExplainer. Additionally, (ii) the computational effort of SubgraphX is at a 'reasonable level', which is not further specified by the original authors. We define this as at most ten times slower than GNNExplainer. 
We reimplemented the proposed algorithm in PyTorch. Then, we replicated the experiments performed by the authors on a smaller scale due to resource constraints. Additionally, we checked the performance on a new dataset and investigated the influence of hyperparameters. Lastly, we improved SubgraphX using greedy initialization and utilizing fidelity as a score function. 
We were able to reproduce the main claims on the MUTAG dataset, where SubgraphX has a better performance than GNNExplainer. Furthermore, SubgraphX has a reasonable runtime of about seven times longer than GNNExplainer. We successfully employed SubgraphX on the Karate Club dataset, where it outperforms GNNExplainer as well. The hyperparameter study revealed that the number of Monte-Carlo Tree search iterations and Monte-Carlo sampling steps are the most important hyperparameters and directly trade performance for runtime. Lastly, we show that our proposed improvements to SubgraphX significantly enhance fidelity and runtime. 
The authors' description of the algorithm was clear and concise. The original implementation is available in the DIG-library as a reference to our implementation. 
The authors performed extensive experiments, which we could not replicate in their full scale due to resource constraints. However, we were able to achieve similar results on a subset of the datasets used. Another issue was that despite the original code of the authors and datasets being publicly available, there were many compatibility issues. 
The original authors briefly reviewed our work and agreed with the findings.}
\def \replicationCITE{H. Yuan, H. Yu, J.Wang, K. Li, and S. Ji. “On Explainability of Graph Neural Networks via Subgraph Explorations.” In: Proceedings of the 38th International Conference on Machine Learning. Vol. 139. Proceedings of Machine Learning Research. PMLR, July 2021, pp. 12241–12252.}
\def \replicationBIB{Yuan21}
\def \replicationURL{http://proceedings.mlr.press/v139/yuan21c/yuan21c.pdf}
\def \replicationDOI{10.48550/ARXIV.2102.05152 identifier}
\def \contactNAME{Yannik Mahlau}
\def \contactEMAIL{yannik.mahlau@stud.uni-hannover.de}
\def \articleKEYWORDS{rescience c, rescience x, machine learning, explainable ai, graph neural networks, subgraphX, python}
\def \journalNAME{ReScience C}
\def \journalVOLUME{9}
\def \journalISSUE{2}
\def \articleNUMBER{41}
\def \articleDOI{}
\def \authorsFULL{Yannik Mahlau, Leonie Kayser and Lukas Berg}
\def \authorsABBRV{Y. Mahlau, L. Kayser and L. Berg}
\def \authorsSHORT{Mahlau, Kayser and Berg}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0000-0003-0425-5003}]{Yannik Mahlau}
\author[1,2,\orcid{0000-0002-4749-4920}]{Leonie Kayser}
\author[1,\orcid{0000-0002-5684-6975}]{Lukas Berg}
\affil[1]{Leibniz Universität Hannover, Hannover, Germany}
\affil[2]{Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany}

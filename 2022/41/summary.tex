\section*{\centering Reproducibility Summary}

\subsubsection*{Scope of Reproducibility}
Yuan et al. claim their proposed method SubgraphX achieves (i) higher fidelity in explaining models for graph- and node classification tasks compared to other explanation techniques, namely GNNExplainer. 
Additionally, (ii) the computational effort of SubgraphX is at a "reasonable level", which is not further specified by the original authors. We define this as at most ten times slower than GNNExplainer.

\subsubsection*{Methodology}
We reimplemented the proposed algorithm in PyTorch. 
Then, we replicated the experiments performed by the authors on a smaller scale due to resource constraints. 
Additionally, we checked the performance on a new dataset and investigated the influence of hyperparameters.
Lastly, we improved SubgraphX using greedy initialization and utilizing fidelity as a score function.

\subsubsection*{Results}
We were able to reproduce the main claims on the MUTAG dataset, where SubgraphX has a better performance than GNNExplainer.
Furthermore, SubgraphX has a reasonable runtime of about seven times longer than GNNExplainer. 
We successfully employed SubgraphX on the Karate Club dataset, where it outperforms GNNExplainer as well.
The hyperparameter study revealed that the number of Monte-Carlo Tree search iterations and Monte-Carlo sampling steps are the most important hyperparameters and directly trade performance for runtime.
Lastly, we show that our proposed improvements to SubgraphX significantly enhance fidelity and runtime.

\subsubsection*{What was easy}
The authors' description of the algorithm was clear and concise. The original implementation is available in the DIG-library as a reference.

\subsubsection*{What was difficult}
The authors performed extensive experiments, which we could not replicate in their full scale due to resource constraints. However, we were able to achieve similar results on a subset of the datasets used.
Another issue was that despite the original code of the authors and datasets being publicly available, there were many compatibility issues.

\subsubsection*{Communication with original authors}
The original authors briefly reviewed our work and agreed with the findings.

\def \codeURL{https://github.com/appliedAI-Initiative/mlrc22-like-shapley-love-the-core}
\def \codeDOI{}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{03 February 2023}
\def \dateACCEPTED{23 April 2023}
\def \datePUBLISHED{}
\def \articleTITLE{[Re] If You Like Shapley Then You’ll Love the Core}
\def \articleTYPE{Replication / ML Reproducibility Challenge 2022}
\def \articleDOMAIN{}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2023}
\def \reviewURL{https://openreview.net/forum?id=vWzZQAahuW}
\def \articleABSTRACT{We investigate the results of [1] in the field of data valuation. We repeat their experiments and conclude that the (Monte Carlo) Least Core is sensitive to important characteristics of the ML problem of interest, making it difficult to apply.
Scope of Reproducibility — We test all experimental claims about Monte Carlo approximations to the Least Core and their application to standard data valuation tasks.
Methodology — We use an open source implementation of several data valuation algorithms. We document all details on dataset choice and generation in this paper, and release all code as open source.
Results — We were able to reproduce the results on Least Core approximation. For the task of low‐value point identification we observed an inverted performance gap between least core and Shapley values. For high‐value identification, the least core slightly outperformed Shapley values. In two experiments, we must depart from the original paper and arrive at different conclusions.
What was easy — Open source libraries like DVC and ray enabled efficiently designing and running the experiments.
What was difficult — Data generation was difficult for dog‐vs‐fish because no code was available. Computing the Monte Carlo Least Core was very sensitive to the choice of utility function. Reproducing some experiments was difficult due to lack of details.
Communication with original authors — We asked the authors for details on the experimental setup and they kindly and promptly sent us the code used for the paper. This was very useful in understanding all steps taken and in uncovering some weaknesses in the experiments.}
\def \replicationCITE{}
\def \replicationBIB{}
\def \replicationURL{}
\def \replicationDOI{}
\def \contactNAME{Miguel de Benito Delgado}
\def \contactEMAIL{m.debenito@appliedai-institute.de}
\def \articleKEYWORDS{rescience c, rescience x}
\def \journalNAME{ReScience C}
\def \journalVOLUME{9}
\def \journalISSUE{2}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Anonymous Authors}
\def \authorsABBRV{Anonymous}
\def \authorsSHORT{Anonymous}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0000-0000-0000-0000}]{Anonymous}
\affil[1]{Anonymous Institution}

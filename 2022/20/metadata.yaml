# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it should be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[Â¬Re]"
#  - For other article types, no instruction (but please, not too long)
title: "[Re] Reproducibility study of Joint Multisided Exposure Fairness for Recommendation"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author
authors:
  - name: Alessia Hu
    orcid: 0009-0006-5410-3453
    email: alessia.hu@student.uva.nl
    affiliations: 1,*

  - name: Oline Ranum
    orcid: 0000-0001-8627-6259
    email: oline.a.ranum@gmail.com
    affiliations: 1

  - name: Chrysoula Pozrikidou
    orcid: 0009-0003-1509-912X
    email: chrysa.pozrikidou@student.uva.nl
    affiliations: 1

  - name: Miranda Zhou
    orcid: 0009-0007-8642-8970
    email: miranda.zhou@student.uva.nl
    affiliations: 1

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code: 1
    name: University of Amsterdam
    address: Amsterdam, Netherlands

# List of keywords (adding the programming language might be a good idea)
keywords: Python, Reproducibility, Information Retrieval, Fairness, rescience c, machine learning

# Code URL and DOI (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo,
#   see https://guides.github.com/activities/citable-code/
code:
  - url: "https://github.com/OlineRanum/FACT.git"
  - doi:
  - swh: swh:1:dir:ddaee99fffaa5becad67496efe000ca6c47341c7

# Date URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
  - cite: 'H. Wu, B. Mitra, C. Ma, F. Diaz, and X. Liu. "Joint Multisided Exposure Fairness for Recommendation." 2022' # Full textual citation
  - bib: wu2022joint # Bibtex key (if any) in your bibliography file
  - url: https://arxiv.org/abs/2205.00048 # URL to the PDF, try to link to a non-paywall version
  - doi: 10.48550/arXiv.2205.00048 # Regular digital object identifier

# Don't forget to surround abstract with double quotes
abstract: "Scope of Reproducibility : In this work, we study the reproducibility of Joint Multisided Exposure Fairness (JME) for Recommendation: a recent paper on fairness in ranking algorithms by Wu et al. We aim to verify the following claims suggested by the paper: (i) each of the six proposed exposure fairness metrics quantifies a different notion of unfairness, (ii) for each of the proposed metrics there exists a disparity- relevance trade-off, and (iii) recommender systems can be optimized toward different fairness goals by considering different combinations of the JME-fairness metrics. Methodology: We modify and extend upon the open-source implementation of the pipeline, published by the authors on GitHub. Our adjustments include restructuring the code base, adding experimental setup files, and removing several bugs. We run the experiments on a RTX 3070 GPU, at a reproducibility cost of 44.5 GPU hours. Results: We successfully reproduce the major trends of the core results, although some numerical deviations occur. In particular, we are able of providing support to two out of three claims. However, due to insufficient documentation and resources, we were unable to verify the third claim of the paper. We concurringly conclude that in order to determine the fairness of a recommender system, considering different fairness dimensions with a multi-stakeholder perspective is essential. What was easy: The JME-fairness metrics proposed in the paper are well-explained and fairly intuitive. Even without a background in fairness in AI and recommender systems, we were able to follow the pipeline and the main ideas presented. What was difficult: Details regarding the setup of the experiments are missing from the original codebase, and documentation is limited. In addition, for the reproduction of their third claim, familiarity with topics not analyzed in the paper is required. Communication with original authors : Per request by email, the authors provided some clarifications regarding experimental setups and the calculations performed in the experiments of the original paper. We received a response that answered part of our questions and a reference to a GitHub repository that is potentially suitable for demonstrating optimization with a JME-fairness loss."

# Bibliography file (yours)
bibliography: bibliography.bib

# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2022

# Coding language (main one only if several)
language: Python

# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review:
  - url: https://openreview.net/forum?id=A0Sjs3IJWb-

contributors:
  - name: Koustuv Sinha,\\ Maurits Bleeker,\\ Samarth Bhargav
    orcid:
    role: editor
  - name:
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer

# This information will be provided by the editor
dates:
  - received:  February 4, 2023
  - accepted:  April 19, 2023
  - published:  July 20, 2023

# This information will be provided by the editor
article:
  - number: 20
  - doi:
  - url:

# This information will be provided by the editor
journal:
  - name: "ReScience C"
  - issn: 2430-3658
  - volume: 9
  - issue: 2

@misc{hirotapaper,
  doi = {10.48550/ARXIV.2203.15395},
  url = {https://arxiv.org/abs/2203.15395},
  author = {Hirota, Yusuke and Nakashima, Yuta and Garcia, Noa},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Quantifying Societal Bias Amplification in Image Captioning},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{githubhirota,
  author = {Hirota, Yusuke and Nakashima, Yuta and Garcia, Noa},
  title = {lick caption bias},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rebnej/lick-caption-bias}}
}

@article{merity2017regularizing,
  title={Regularizing and optimizing LSTM language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}

@misc{menalsolikeshopping,
  doi = {10.48550/ARXIV.1707.09457},
  url = {https://arxiv.org/abs/1707.09457},
  author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ChenMuXiaoYeWuJu:2019, title={Improving Image Captioning with Conditional Generative Adversarial Nets}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4823}, DOI={10.1609/aaai.v33i01.33018142}, abstractNote={&lt;p&gt;In this paper, we propose a novel conditional-generativeadversarial-nets-based image captioning framework as an extension of traditional reinforcement-learning (RL)-based encoder-decoder architecture. To deal with the inconsistent evaluation problem among different objective language metrics, we are motivated to design some “discriminator” networks to automatically and progressively determine whether generated caption is human described or machine generated. Two kinds of discriminator architectures (CNN and RNNbased structures) are introduced since each has its own advantages. The proposed algorithm is generic so that it can enhance any existing RL-based image captioning framework and we show that the conventional RL training method is just a special case of our approach. Empirically, we show consistent improvements over all language evaluation metrics for different state-of-the-art image captioning models. In addition, the well-trained discriminators can also be viewed as objective image captioning evaluators.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Chen and Mu, Shuai and Xiao, Wanpeng and Ye, Zexiong and Wu, Liesi and Ju, Qi}, year={2019}, month={8}, pages={8142-8150} }

@misc{fromshowtotell,
  doi = {10.48550/ARXIV.2107.06912},
  url = {https://arxiv.org/abs/2107.06912},
  author = {Stefanini, Matteo and Cornia, Marcella and Baraldi, Lorenzo and Cascianelli, Silvia and Fiameni, Giuseppe and Cucchiara, Rita},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {From Show to Tell: A Survey on Deep Learning-based Image Captioning},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{NEURIPS2019680390c5,
 author = {Herdade, Simao and Kappeler, Armin and Boakye, Kofi and Soares, Joao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Image Captioning: Transforming Objects into Words},
 url = {https://proceedings.neurips.cc/paper/2019/file/680390c55bbd9ce416d1d69a9ab4760d-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{Rougier:2018,
  doi = {10.1038/d41586-018-04628-w},
  year = {2018},
  month = apr,
  publisher = {Springer Nature},
  volume = {556},
  number = {7701},
  pages = {309--309},
  author = {Rougier, Nicolas P. and Hinsen, Konrad},
  title = {Code reviewing puts extra demands on referees },
  journal = {Nature},
}

@article{Science:2018,
  doi = {10.1126/science.359.6377.725},
  author = {Hutson, Matthew},
  url = {http://science.sciencemag.org/content/359/6377/725},
  year = {2018},
  month = feb,
  volume = {359},
  number = {6377},
  title = {Artificial intelligence faces a replication crisis},
  journal = {Science},
}

@Misc{Roboto:2011,
  author =       {Christian Robertson},
  title =        {The Roboto family of fonts (Google)},
  url =          {https://github.com/google/roboto},
  year =         2011,
  note =         {Apache License, verison 2.0},
}

@Misc{SourceSerifPro:2014,
  author =       {Frank Grießhammer},
  title =        {Source Serif Pro (Adobe Systems)},
  url =          {https://github.com/adobe-fonts/source-serif-pro},
  year =         2014,
  note =         {SIL Open Font License, version 1.1},
}

@Misc{SourceCodePro:2012,
  author =       {Paul D. Hunt},
  title =        {Source Code Pro (Adobe Systems)},
  url =          {https://github.com/adobe-fonts/source-code-pro},
  year =         2012,
  note =         {SIL Open Font License, version 1.1},
}

@article{Topalidou:2015,
  author =       {Topalidou, Meropi and Rougier, Nicolas P.},
  title =        {{[Re] Interaction between cognitive and motor cortico-basal
                  ganglia loops during decision making: a computational study}},
  journal =      {ReScience},
  year =         2015,
  volume =       1,
  number =       1,
  doi =          {10.5281/zenodo.27944},
}

@article{Rougier:2017,
  doi =          {10.7717/peerj-cs.142},
  author =       {Nicolas P. Rougier and Konrad Hinsen and Frédéric Alexandre
                  and Thomas Arildsen and Lorena Barba and Fabien
                  C. Y. Benureau and C. Titus Brown and Pierre de Buyl and Ozan
                  Caglayan and Andrew P. Davison and Marc André Delsuc and
                  Georgios Detorakis and Alexandra K. Diem and Damien Drix and
                  Pierre Enel and Benoît Girard and Olivia Guest and Matt
                  G. Hall and Rafael Neto Henriques and Xavier Hinaut and Kamil
                  S Jaron and Mehdi Khamassi and Almar Klein and Tiina Manninen
                  and Pietro Marchesi and Dan McGlinn and Christoph Metzner and
                  Owen L. Petchey and Hans Ekkehard Plesser and Timothée Poisot
                  and Karthik Ram and Yoav Ram and Etienne Roesch and Cyrille
                  Rossant and Vahid Rostami and Aaron Shifman and Joseph
                  Stachelek and Marcel Stimberg and Frank Stollmeier and
                  Federico Vaggi and Guillaume Viejo and Julien Vitay and Anya
                  Vostinar and Roman Yurchak and Tiziano Zito},
  title =        {{Sustainable computational science: the ReScience
                  initiative}},
  journal =      {{PeerJ} Computer Science},
  month =        12,
  volume =       3,
  pages =        {e142},
  year =         2017,
  github =       {https://github.com/ReScience/ReScience-article-2},
  keywords =     {journal},
  tags =         {OS},
}

@article{Sinha:2022,
  author = {Sinha, Koustuv and Dodge, Jesse and Luccioni, Sasha and Forde, Jessica Zosa and Raparthy, Sharath Chandra and Pineau, Joelle and Stojnic, Robert},
  title = {{ML Reproducibility Challenge 2021}},
  journal = {ReScience C},
  year = {2022},
  month = may,
  volume = {8},
  number = {2},
  pages = {{48}},
  doi = {10.5281/zenodo.6574723},
  url = {https://zenodo.org/record/6574723/files/article.pdf},
  type = {Editorial},
  language = {Python},
  domain = {ML Reproducibility Challenge 2021},
  keywords = {rescience c, machine learning, deep learning, python, pytorch}
}

@article{Sinha:2021,
  author = {Sinha, Koustuv and Dodge, Jesse and Luccioni, Sasha and Forde, Jessica Zosa and Stojnic, Robert and Pineau, Joelle},
  title = {{ML Reproducibility Challenge 2020}},
  journal = {ReScience C},
  year = {2021},
  month = may,
  volume = {7},
  number = {2},
  pages = {{1}},
  doi = {10.5281/zenodo.4833117},
  url = {https://zenodo.org/record/4833117/files/article.pdf},
  type = {Editorial},
  keywords = {machine learning, neurips, reproducibility challenge}
}

@article{Sinha:2020,
  author = {Sinha, Koustuv and Pineau, Joelle and Forde, Jessica and Ke, Rosemary Nan and Larochelle, Hugo},
  title = {{NeurIPS 2019 Reproducibility Challenge}},
  journal = {ReScience C},
  year = {2020},
  month = may,
  volume = {6},
  number = {2},
  pages = {{11}},
  doi = {10.5281/zenodo.3818627},
  url = {https://zenodo.org/record/3818627/files/article.pdf},
  type = {Editorial},
  keywords = {machine learning, neurips, reproducibility challenge}
}

@article{Pineau:2019,
  author = {Pineau, Joelle and Sinha, Koustuv and Fried, Genevieve and Ke, Rosemary Nan and Larochelle, Hugo},
  title = {{ICLR Reproducibility Challenge 2019}},
  journal = {ReScience C},
  year = {2019},
  month = may,
  volume = {5},
  number = {2},
  pages = {{5}},
  doi = {10.5281/zenodo.3158244},
  url = {https://zenodo.org/record/3158244/files/article.pdf},
  type = {Editorial},
  keywords = {machine learning, ICLR, reproducibility challenge}
}


@article{NIC:2015,
  doi = {10.48550/ARXIV.1411.4555},
  url = {https://arxiv.org/abs/1411.4555},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Show and Tell: A Neural Image Caption Generator},
}


@article{OSCAR:2020,
  doi = {10.48550/ARXIV.2004.06165},
  url = {https://arxiv.org/abs/2004.06165},
  author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@InProceedings{SAT:2015,
  title = 	 {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author = 	 {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2048--2057},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {7},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/xuc15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/xuc15.html},
  abstract = 	 {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.}
}

@article{Att2inFC:2016,
  doi = {10.48550/ARXIV.1612.00563},
  url = {https://arxiv.org/abs/1612.00563},
  author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jarret and Goel, Vaibhava},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Self-critical Sequence Training for Image Captioning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{UpDn:2017,
  doi = {10.48550/ARXIV.1707.07998},
  url = {https://arxiv.org/abs/1707.07998},
  author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{transformer:2017,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{NICplusNICEqualizer:2018,
  doi = {10.48550/ARXIV.1807.00517},
  url = {https://arxiv.org/abs/1807.00517},
  author = {Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Women also Snowboard: Overcoming Bias in Captioning Models (Extended Abstract)},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{kokhlikyan2020captum,
    title={Captum: A unified and generic model interpretability library for PyTorch},
    author={Narine Kokhlikyan and Vivek Miglani and Miguel Martin and Edward Wang and Bilal Alsallakh and Jonathan Reynolds and Alexander Melnikov and Natalia Kliushkina and Carlos Araya and Siqi Yan and Orion Reblitz-Richardson},
    year={2020},
    eprint={2009.07896},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@incollection{NEURIPS20199015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

 @misc{winastwan:2023, title={Interpreting the prediction of Bert Model for Text Classification}, url={https://towardsdatascience.com/interpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074}, journal={Medium}, publisher={Towards Data Science}, author={Winastwan, Ruben}, year={2023}, month={1}} 

@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  doi={10.1162/neco.1997.9.8.1735}}

@misc{bertpaper,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{bleu:2002,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{meteor:2014,
    title = "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
    author = "Denkowski, Michael  and
      Lavie, Alon",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3348",
    doi = "10.3115/v1/W14-3348",
    pages = "376--380",
}

@article{cider:2014,
  doi = {10.48550/ARXIV.1411.5726},
  url = {https://arxiv.org/abs/1411.5726},
  author = {Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {CIDEr: Consensus-based Image Description Evaluation},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{rouge:2004,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}
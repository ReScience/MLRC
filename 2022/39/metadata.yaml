# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it shoudl be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[Â¬Re]"
#  - For other article types, no instruction (but please, not too long)
title: "[Re] CrossWalk: Fairness-enhanced Node Representation Learning"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author
authors:
  - name: Luca Pantea
    orcid: 0009-0007-2511-1953
    email: luca.pantea@student.uva.nl
    affiliations: 1,*

  - name: Andrei Blahovici
    orcid: 0009-0004-8216-0397
    email: andrei.blahovici@student.uva.nl
    affiliations: 1

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code: 1
    name: University of Amsterdam
    address: "Amsterdam, The Netherlands"

# List of keywords (adding the programming language might be a good idea)
keywords: rescience c, rescience x, machine learning, Fairness in Machine Learning, Representational Learning, Graph Algorithms

# Code URL and DOI (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo,
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/Dawlau/FACT-AI
  - doi: 10.5281/zenodo.7932857
  - swh: swh:1:dir:6367a5e042f3b14cd6d86349199e08597fc0f3f6

# Date URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
  - cite: "A. Khajehnejad, M. Khajehnejad, M. Babaei, K. P. Gummadi, A. Weller, and B. Mirzasoleiman. CrossWalk: Fairness-enhanced Node Representation Learning. 2021" # Full textual citation
  - bib: crosswalk # Bibtex key (if any) in your bibliography file
  - url: "https://arxiv.org/pdf/2105.02725.pdf" # URL to the PDF, try to link to a non-paywall version
  - doi: 10.48550/arXiv.2105.02725 # Regular digital object identifier

# Don't forget to surround abstract with double quotes
abstract: 'Scope of Reproducibility
  This work aims to reproduce the findings of the paper "CrossWalk: Fairness-enhanced Node Representation Learning" by investigating the two main claims made by the authors about CrossWalk, which suggest that (i) CrossWalk enhances fairness in three graph algorithms, while only suffering from small decreases in performance, and that (ii) CrossWalk preserves the necessary structural properties of the graph while reducing disparity.

  Methodology
  The authors made the CrossWalk repository available, which contained most of the datasets used for their experimentation, and the scripts needed to run the experiments. However, the codebase lacked documentation and was missing logic for running all experiments and visualizing the results. We, therefore, re-implement their code from scratch and deploy it as a python package which can be run to obtain all the showcased results.

  Results
  Our work suggests that the first claim of the paper, which states that Crosswalk minimizes disparity and thus enhances fairness is partially reproducible, and only for the tasks of Node classification and Influence maximization as the parameters specified in the paper do not always yield similar results. Then, the second claim of the paper which states that Crosswalk attains the necessary structural properties of the graph is fully reproducible through our experiments.

  What was easy
  The original paper contained the necessary information about hyperparameters, which coupled with the publicly available repository made it straightforward to refactor the code and understand the idea of the proposed method.

  What was difficult
  The difficulty stems from the lack of structure and documentation in the provided code which made the original experiments hard to reproduce. Furthermore, there were missing files in the provided datasets. Also, some experiments were not reproducible at all through the provided code. One more important aspect is that the experiments are CPU intensive which made the reproducibility even harder.

  Communication with original authors
  Albeit rather late, the authors provided meaningful feedback on our questions about implementation details and initial results.'

# Bibliography file (yours)
bibliography: bibliography.bib

# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2022

# Coding language (main one only if several)
language: Python

# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review:
  - url: "https://openreview.net/forum?id=tpk45Zll8eh"

contributors:
  - name: Koustuv Sinha,\\ Maurits Bleeker,\\ Samarth Bhargav
    orcid:
    role: editor
  - name:
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer

# This information will be provided by the editor
dates:
  - received:  February 4, 2023
  - accepted:  April 19, 2023
  - published:  July 20, 2023

# This information will be provided by the editor
article:
  - number: 39
  - doi: 10.5281/zenodo.8173749
  - url: https://zenodo.org/record/8173749/files/article.pdf

# This information will be provided by the editor
journal:
  - name: "ReScience C"
  - issn: 2430-3658
  - volume: 9
  - issue: 2

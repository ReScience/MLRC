% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/noanonkes/fact-guarantee}
\def \codeDOI{}
\def \codeSWH{swh:1:dir:c769bc1fc87a24b6811f318d7ad56ea9a70954e7}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha,\\ Maurits Bleeker,\\ Samarth Bhargav}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{04 February 2023}
\def \dateACCEPTED{19 April 2023}
\def \datePUBLISHED{20 July 2023}
\def \articleTITLE{[¬Re] A Reproducibility Case Study of “Fairness Guarantees under Demographic Shift”}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2022}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2023}
\def \reviewURL{https://openreview.net/forum?id=MMuv-v99Hy}
\def \articleABSTRACT{Scope of Reproducibility — This work studies the reproducibility of the paper Fairness guarantees under demographic shift (2022) by Giguere et al. Specifically, the authors discuss Shifty, an algorithm that provides high‐confidence guarantees that a user‐specified fairness constraint will hold in the case of a demographic shift between training and deployment data. The authors claim that Shifty achieves this without any significant loss of accuracy when compared to a number of other baseline algorithms. \ Methodology — Using the open‐source code provided by the authors, experiments were conducted to collect the results of Shifty and a number of other baseline algorithms when deployed on three different datasets. Results were collected in the form of accuracy, failure rate, and the probability of not finding a fair solution. The experiments in this reproducibility study were conducted on a total of 115 CPU hours. \ Results — The claim that Shifty guarantees fairness with high confidence is strongly confirmed by the reproduction results of this study. It was also found in this reproducibility study that Shifty achieves accuracy scores comparable to those of other fairness algorithms. \ What was easy — The open‐source code was structured in a way that allowed us to make alterations to the experimental setup or the implementations of the models. The original datasets were also provided in a structured manner and were already standardized. \ What was difficult — Modifications to the code were necessary in order to run this code efficiently and without errors; in the original code, there were packages missing, redundant functions and files, and mistakes in the handling of the user‐specified fairness constraints. \ Communication with original authors — The authors did not respond to our inquiries, resulting in no communication with the original authors.}
\def \replicationCITE{S. Giguere, B. Metevier, Y. Brun, P. S. Thomas, S. Niekum, and B. C. da Silva. “Fairness Guarantees under Demographic Shift.” In: International Conference on Learning Representations. 2022.}
\def \replicationBIB{giguere2022fairness}
\def \replicationURL{https://openreview.net/pdf?id=wbPObLm6ueA}
\def \replicationDOI{n/a}
\def \contactNAME{Zjos van de Sande}
\def \contactEMAIL{zjos.van.de.sande@student.uva.nl}
\def \articleKEYWORDS{rescience c, rescience x, machine learning, deep learning, python, pytorch}
\def \journalNAME{ReScience C}
\def \journalVOLUME{9}
\def \journalISSUE{2}
\def \articleNUMBER{18}
\def \articleDOI{10.5281/zenodo.8206607}
\def \authorsFULL{Dennis Agafonov et al.}
\def \authorsABBRV{D. Agafonov et al.}
\def \authorsSHORT{Agafonov et al.}
\title{\articleTITLE}
\date{}
\author[1,\textdagger,\orcid{0009-0004-3390-6113}]{Dennis Agafonov}
\author[1,\textdagger,\orcid{0009-0007-3567-9796}]{Jelke Matthijsse}
\author[1,\textdagger,\orcid{0009-0002-4370-1643}]{Noa Nonkes}
\author[1,\textdagger,\orcid{0009-0000-6702-8444}]{Zjos van de Sande}
\affil[1]{University of Amsterdam, Amsterdam, The Netherlands}
\affil[{}\textdagger]{Equal contributions}

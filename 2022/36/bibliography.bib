@inproceedings{Hirota2022,
  title={Quantifying Societal Bias Amplification in Image Captioning},
  author={Hirota, Yusuke and Nakashima, Yuta and Garcia, Noa},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13450--13459},
  year={2022}
 }
 
@book{Data-feminism,
  author = {Catherine D'ignacio and Lauren F. Klein},
  year = {2020},
  title = {Data feminism},
  publisher = {MIT press}
}

@article{Burns2018,
  author = {Burns, Kaylee and Hendricks, Lisa Anne and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  title = {Women also snowboard: Overcoming bias in captioning models},
  journal = {ECCV},
  year = {2018}
}

@article{NIC,
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  title = {Show and tell: A neural image caption generator},
  journal = {CVPR},
  year = {2015}
}

@article{SAT,
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  title = {Show, attend and tell: Neural image caption generation with visual attention},
  journal = {ICML},
  year = {2015}
}

@article{Rennie2017,
  author = {Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  title = {Self-critical sequence training for image captioning},
  journal = {CVPR},
  year = {2017}
}

@article{UpDn,
  author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  title = {Bottom-up and top-down attention for image captioning and visual question answering},
  journal = {CVPR},
  year = {2018}
}

@article{Transformer,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  title = {Attention is all you need},
  journal = {NeurIPS},
  year = {2017}
}

@article{OSCAR,
  author = {Li, Wiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  title = {Oscar: Object-semantics aligned pre-training for vision-language tasks},
  journal = {ECCV},
  year = {2020}
}

@inproceedings{zhao2021captionbias,
   author = {Dora Zhao and Angelina Wang and Olga Russakovsky},
   title = {Understanding and Evaluating Racial Biases in Image Captioning},
   booktitle = {International Conference on Computer Vision (ICCV)},
   year = {2021}
}

@article{MSCOCO,
  author = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll√°r, Piotr and Zitnick, C. Lawrence},
  title = {Microsoft COCO captions: Data collection and evaluation server},
  type = {arXiv preprint},
    archivePrefix = "arXiv",
    eprint        = "1504.00325",
    year = {2015}
}

@article{LSTM,
  author = {Hochreiter, Sepp and Schmidhuber, J\"urgen},
   title = {Long short-term memory},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  year = {1997}
}

@article{BERT,
  author = {Delvin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {BERT: pre-training of deep bidirectional transformers for language understanding},
  journal = {NAACL-HLT},
  volume = {1},
  year = {2019}
}

@article{GenderShades,
  author = {Buolamwini, Joy and Gebry, Timnit},
  title = {Gender shades: Intersectional accuracy disparities in commercial gender classification},
  journal = {ACM FAccT},
  year = {2018}
}
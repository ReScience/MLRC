\def \codeURL{https://github.com/rescience-c/template}
\def \codeDOI{}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{03 February 2023}
\def \dateACCEPTED{}
\def \datePUBLISHED{}
\def \articleTITLE{Reproducibility Study of \enquote{Label-Free Explainability for Unsupervised Models}} % \centering 
\def \articleTYPE{Replication / ML Reproducibility Challenge 2022}
\def \articleDOMAIN{}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2023}
\def \reviewURL{}
\def \articleABSTRACT{
\subsubsection*{Scope of Reproducibility}
In this work, we evaluate the reproducibility of the paper \textit{Label-Free Explainability for Unsupervised Models} by Crabbe and van der Schaar \citep{main_paper}. Our goal is to reproduce the paper's four main claims in a label-free setting:(1) feature importance scores determine salient features of a model's input, (2) example importance scores determine salient training examples to explain a test example, (3) interpretability of saliency maps  is hard for disentangled VAEs, (4) distinct pretext tasks donâ€™t have interchangeable representations.

\subsubsection*{Methodology}
The authors of the paper provide an implementation in PyTorch for their proposed techniques and experiments. We reuse and extend their code for our additional experiments. Our reproducibility study comes at a total computational cost of 110 GPU hours, using an NVIDIA Titan RTX. 

\subsubsection{Results}
We reproduced the original paper's work through our experiments. We find that the main claims of the paper largely hold. We assess the robustness and generalizability of some of the claims, through our additional experiments. In that case, we find that one claim is not generalizable and another is not reproducible for the graph dataset.

\subsubsection*{What was easy}
The original paper is well-structured. The code implementation is well-organized and with clear instructions on how to get started. This was helpful to understand the paper's work and begin experimenting with their proposed methods.

\subsubsection*{What was difficult}
We found it difficult to extrapolate some of the authors' proposed techniques to datasets other than those used by them.  Also, we were not able to reproduce the results for one of the experiments. We couldn't find the exact reason for it by running explorative experiments due to time and resource constraints.

\subsubsection*{Communication with original authors}
We reached out to the authors once about our queries regarding one experimental setup and to understand the assumptions and contexts of some sub-claims in the paper. We received a prompt response which satisfied most of our questions. 

}
\def \replicationCITE{}
\def \replicationBIB{}
\def \replicationURL{}
\def \replicationDOI{}
\def \contactNAME{}
\def \contactEMAIL{}
\def \articleKEYWORDS{rescience c, rescience x}
\def \journalNAME{ReScience C}
\def \journalVOLUME{9}
\def \journalISSUE{2}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Anonymous Authors}
\def \authorsABBRV{Anonymous}
\def \authorsSHORT{Anonymous}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0000-0000-0000-0000}]{Anonymous}
\affil[1]{Anonymous Institution}

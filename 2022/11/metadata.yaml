# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it shoudl be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[¬Re]"
#  - For other article types, no instruction (but please, not too long)
title: "[Re] Reproducibility Study of “Label-Free Explainability for
Unsupervised Models”"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author
authors:

  - name: Valentinos Pariza
    orcid: 0009-0008-3440-9935
    email: valentinos.pariza@student.uva.nl
    affiliations: 1,*      # * is for contact author

  - name: Avik Pal
    orcid: 0009-0003-1320-3819
    email: avik.pal@student.uva.nl
    affiliations: 1
  
  - name: Madhura Pawar
    orcid: 0009-0001-1540-3318
    email: madhura.pawar@student.uva.nl
    affiliations: 1      # * is for contact author

  - name: Quim Serra Faber
    orcid: 0009-0002-0762-8004
    email: quim.serra.faber@student.uva.nl
    affiliations: 1      # * is for contact author

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code:    1
    name:  University of Amsterdam
    address:  Amsterdam, The Netherlands
  - code:    1
    name:  Equal contribution
    address:  ""
  
# List of keywords (adding the programming language might be a good idea)
keywords: rescience c, machine learning, reproducibility, feature importance, example importance, disentangled VAEs, label-free, unsupervised, post-hoc explainability, python

# Code URL and DOI (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo,
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/valentinosPariza/Re-Label-Free-XAI
  - doi: 
  - swh: swh:1:dir:8aa22d2a6b71c52b0863a06ab40f40ada1ec5355

# Date URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
 - cite: "Crabbe, Jonathan and Mihaela van der Schaar. “Label-Free Explainability for Unsupervised Models.” International Conference on Machine Learning (2022)."
 - bib:  Jonathan3:online # Bibtex key (if any) in your bibliography file
 - url:  https://arxiv.org/abs/2203.01928 # URL to the PDF, try to link to a non-paywall version
 - doi:  10.48550/arXiv.2203.01928 # Regular digital object identifier

# Don't forget to surround abstract with double quotes
abstract: "Scope of Reproducibility
In this work, we evaluate the reproducibility of the paper Label-Free Explainability for Unsupervised Models by Crabbe and van der Schaar. Our goal is to reproduce the paper's four main claims in a label-free setting:(1) feature importance scores determine salient features of a model's input, (2) example importance scores determine salient training examples to explain a test example, (3) interpretability of saliency maps is hard for disentangled VAEs, (4) distinct pretext tasks don’t have interchangeable representations.

Methodology
The authors of the paper provide an implementation in PyTorch for their proposed techniques and experiments. We reuse and extend their code for our additional experiments. Our reproducibility study comes at a total computational cost of 110 GPU hours, using an NVIDIA Titan RTX. 

Results
We reproduced the original paper's work through our experiments. We find that the main claims of the paper largely hold. We assess the robustness and generalizability of some of the claims, through our additional experiments. In that case, we find that one claim is not generalizable and another is not reproducible for the graph dataset.

What was easy
The original paper is well-structured. The code implementation is well-organized and with clear instructions on how to get started. This was helpful to understand the paper's work and begin experimenting with their proposed methods.

What was difficult
We found it difficult to extrapolate some of the authors' proposed techniques to datasets other than those used by them.  Also, we were not able to reproduce the results for one of the experiments. We couldn't find the exact reason for it by running explorative experiments due to time and resource constraints.

Communication with original authors
We reached out to the authors once about our queries regarding one experimental setup and to understand the assumptions and contexts of some sub-claims in the paper. We received a prompt response which satisfied most of our questions."

# Bibliography file (yours)
bibliography: bibliography.bib
  
# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2022

# Coding language (main one only if several)
language: Python

  
# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review: 
  - url: https://openreview.net/forum?id=qP34dvJpHd

contributors:
  - name: Koustuv Sinha,\\ Maurits Bleeker,\\ Samarth Bhargav
    orcid:
    role: editor
  - name:
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer

# This information will be provided by the editor
dates:
  - received:  February 4, 2023
  - accepted:  April 19, 2023
  - published:  July 20, 2023

# This information will be provided by the editor
article:
  - number: 11
  - doi:    # DOI from Zenodo
  - url:    # Final PDF URL (Zenodo or rescience website?)

# This information will be provided by the editor
journal:
  - name: "ReScience C"
  - issn: 2430-3658
  - volume: 9
  - issue: 2

% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/MartinSpendl/ML-reproducibility-challenge-23}
\def \codeDOI{10.5281/zenodo.7949229}
\def \codeSWH{swh:1:dir:a67381741fc59350e147883d925d6246f60bb3ef}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha,\\ Maurits Bleeker,\\ Samarth Bhargav}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{04 February 2023}
\def \dateACCEPTED{19 April 2023}
\def \datePUBLISHED{20 July 2023}
\def \articleTITLE{[Re] Easy Bayesian Transfer Learning with Informative Priors}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2022}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2023}
\def \reviewURL{https://openreview.net/forum?id=JpaQ8GFOVu}
\def \articleABSTRACT{Scope of Reproducibility — In this work, we study the reproducibility of the paper: Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors. The paper proposes a three‐step pipeline for replacing standard transfer learning with a pre‐trained prior. The first step is training a prior, the second is re‐scaling of a prior, and the third is inference. The authors claim that increasing the rank and the scaling factor improves performance on the downstream task. They also argue that using Bayesian learning with informative prior leads to a more data‐efficient and improved performance compared to standard SGD transfer learning or using non‐informative prior. We reproduce the main claims on one of the four data sets in the paper. Methodology — We used a combination of the authors’ and our code. The authors provided a training pipeline for the user but not the code to fully reproduce the paper. We modified the training pipeline to suit our needs and created a testing pipeline to evaluate the models. We reproduced the results for the Oxford‐102‐Flowers data set on an Nvidia RTX 3070 GPU using approximately 310 GPU hours for the main results. Results — Our results confirm most of the claims tested, although we could not achieve the exact same accuracy due to missing hyper‐parameters. We reproduced the trend in how scaling the prior impacts the performance and how a learned prior outperforms a non‐learned prior. On contrary, we could not reproduce the effect of rank in low‐rank covariance approximation on model performance, as well as the beneficial boost in performance of Bayesian learning compared to the standard SGD. What was easy — The authors’ implementation provides various training and logging parameters. It is also helpful that the authors provided both the learned priors and scripts for the download, split and preprocessing of the data sets used in the study. What was difficult — Setting the environment for the used packages to work correctly was difficult. Although many parameters are available for running the pipeline, their descriptions are misguiding, therefore a lot of time went into clarifying the parameter function and debugging different settings. The training also took a while, especially when training 5 models per data point. Communication with original authors — We contacted the authors via e‐mail about their pipeline and their use of hyper‐parameters but did not hear back.}
\def \replicationCITE{Shwartz-Ziv, Ravid, et al. "Pre-train your loss: Easy bayesian transfer learning with informative priors." Advances in Neural Information Processing Systems 35 (2022)}
\def \replicationBIB{}
\def \replicationURL{https://arxiv.org/abs/2205.10279v1}
\def \replicationDOI{10.48550/arXiv.2205.10279}
\def \contactNAME{Martin Špendl}
\def \contactEMAIL{martin.spendl@fri.uni-lj.si}
\def \articleKEYWORDS{rescience c, bayesian, deep learning, python, pytorch, machine learning}
\def \journalNAME{ReScience C}
\def \journalVOLUME{9}
\def \journalISSUE{2}
\def \articleNUMBER{9}
\def \articleDOI{10.5281/zenodo.8173668}
\def \authorsFULL{Martin Špendl and Klementina Pirc}
\def \authorsABBRV{M. Špendl and K. Pirc}
\def \authorsSHORT{Špendl and Pirc}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0009-0008-0796-8985}]{Martin Špendl}
\author[1,\orcid{0009-0008-8202-3439}]{Klementina Pirc}
\affil[1]{University of Ljubljana, Faculty of Computer and Information Science, Ljubljana, Slovenia}

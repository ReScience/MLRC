# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it should be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[¬Re]"
#  - For other article types, no instruction (but please, not too long)
title: "[¬Re] Reproducibility study of ‘Proto2Proto: Can you recognize the car, the way I do?’"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author
authors:
  - name: David Bikker
    orcid: 0009-0007-2665-1951
    email: bikkerdavid@gmail.com
    affiliations: 1,2

  - name: Gerson de Kleuver
    orcid: 0009-0008-0665-8846
    email: gersondekleuver@gmail.com
    affiliations: 1,2,*

  - name: Wenhua Hu
    orcid: 0009-0001-7584-5804
    email: w.hu1224@gmail.com
    affiliations: 1,2

  - name: Bram Veenman
    orcid: 0009-0009-2039-766X
    email: bramveenman@gmail.com
    affiliations: 1,2

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code: 1
    name: Universiteit van Amsterdam
    address: Amsterdam, Netherlands
  - code: 2
    name: Equal contributions

# List of keywords (adding the programming language might be a good idea)
keywords: reproducibility, python, prototype, rescience c, machine learning, computer vision

# Code URL and DOI (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo,
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/gersondekleuver/Fact
  - doi: 10.5281/zenodo.8079872
  - swh: swh:1:dir:07b0f527f206c44a96373df590b251c5c3bf669d;

# Date URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
  - cite: "Keswani, M., Ramakrishnan, S., Reddy, N. P., & Balasubramanian, V. N. (2022). Proto2Proto: Can you recognize the car, the way I do? arXiv (Cornell University)."
  - bib: keswani2022proto2proto
  - url: "https://arxiv.org/pdf/2204.11830.pdf"
  - doi: "https://doi.org/10.48550/arXiv.2204.11830"

# Don't forget to surround abstract with double quotes
abstract: "Scope of Reproducibility — This paper analyses the reproducibility of the study Proto2Proto: Can you recognize the car, the way I do? The main contributions and claims of the study are: 1) Using Proto2Proto, a shallower student model is more faithful to the teacher in terms of interpretability than a baseline student model while also showing the same or better accuracy; 2) Global Explanation loss forces student prototypes to be close to teacher prototypes; 3) Patch-Prototype Correspondence loss enforces the local representations of the student to be similar to those of the teacher; 4) The proposed evaluation metrics determine the faithfulness of the student to the teacher in terms of interpretability. Methodology — A public code repository was available for the paper, which provided a working but incomplete and minimally documented codebase. With some modifications we were able to carry out the experiments that were best supported by the codebase. We spent a total of 60 computational GPU hours on reproduction. Results —  The results we were able to produce support claim 1, albeit weakly. Further results are in line with the paper, but we found them to go against claim 3. In addition, we carried out a theoretical analysis which provides support for claim 4. Finally, we were unable to carry out our intended experiment to verify claim 2. What was easy — The original paper was clearly structured and understandable. The experiments for which configurations were provided were simple to conduct. What was difficult — The public codebase contained minimal documentation. Moreover, the use of variable names did not correspond between the code and the paper. Furthermore, the codebase lacked elements vital to reproducing some experiments. Another significant constraint were the computational requirements needed to reproduce the original experiments. Finally, the code required to reproduce one of the visualizations was not provided. Communication with original authors — We contacted the authors to ask for trained model weights and missing hyperparameters for several experiments. We did not receive a response."

# Bibliography file (yours)
bibliography: bibliography.bib

# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2022

# Coding language (main one only if several)
language: python

# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review:
  - url: https://openreview.net/forum?id=a_9YF58u61

contributors:
  - name: Koustuv Sinha,\\ Maurits Bleeker,\\ Samarth Bhargav
    orcid:
    role: editor
  - name:
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer

dates:
  - received: February 4, 2023
  - accepted: April 19, 2023
  - published: July 20, 2023

article:
  - number: 22
  - doi:
  - url:

# This information will be provided by the editor
journal:
  - name: "ReScience C"
  - issn: 2430-3658
  - volume: 9
  - issue: 2

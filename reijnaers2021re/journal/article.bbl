% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{plumb2020explaining}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=f6803630ccbfe3be3d5ff3b92cebfafe}{%
           family={Plumb},
           familyi={P\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=5a63d51259fb009f05eb0dc553682b9a}{%
           family={Terhorst},
           familyi={T\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=42444e8073f744a3db61a499f44443f9}{%
           family={Sankararaman},
           familyi={S\bibinitperiod},
           given={Sriram},
           giveni={S\bibinitperiod}}}%
        {{hash=70e0fe957473b80f1b1b5dcb8e99d31e}{%
           family={Talwalkar},
           familyi={T\bibinitperiod},
           given={Ameet},
           giveni={A\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=2d366eea8a8bd912dc8499cb3200ebd2}{%
           family={III},
           familyi={I\bibinitperiod},
           given={Hal\bibnamedelima Daumé},
           giveni={H\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=d77c73a14b0f048484c1d34aba3071d4}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Aarti},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{c8555ec2e253b057834033d92654689e}
      \strng{fullhash}{b3de8715a8a2ee07999625024211cbeb}
      \strng{bibnamehash}{b3de8715a8a2ee07999625024211cbeb}
      \strng{authorbibnamehash}{b3de8715a8a2ee07999625024211cbeb}
      \strng{authornamehash}{c8555ec2e253b057834033d92654689e}
      \strng{authorfullhash}{b3de8715a8a2ee07999625024211cbeb}
      \strng{editorbibnamehash}{6a2c7e018f72565d1676eee4dc69f8f4}
      \strng{editornamehash}{6a2c7e018f72565d1676eee4dc69f8f4}
      \strng{editorfullhash}{6a2c7e018f72565d1676eee4dc69f8f4}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A common workflow in data exploration is to learn a low-dimensional representation of the data, identify groups of points in that representation, and examine the differences between the groups to determine what they represent. We treat this workflow as an interpretable machine learning problem by leveraging the model that learned the low-dimensional representation to help identify the key differences between the groups. To solve this problem, we introduce a new type of explanation, a Global Counterfactual Explanation (GCE), and our algorithm, Transitive Global Translations (TGT), for computing GCEs. TGT identifies the differences between each pair of groups using compressed sensing but constrains those pairwise differences to be consistent among all of the groups. Empirically, we demonstrate that TGT is able to identify explanations that accurately explain the model while being relatively sparse, and that these explanations match real patterns in the data.}
      \field{booktitle}{Proceedings of the 37th International Conference on Machine Learning}
      \field{month}{13--18 Jul}
      \field{series}{Proceedings of Machine Learning Research}
      \field{title}{Explaining Groups of Points in Low-Dimensional Representations}
      \field{volume}{119}
      \field{year}{2020}
      \field{pages}{7762\bibrangedash 7771}
      \range{pages}{10}
      \verb{file}
      \verb http://proceedings.mlr.press/v119/plumb20a/plumb20a.pdf
      \endverb
      \verb{urlraw}
      \verb http://proceedings.mlr.press/v119/plumb20a.html
      \endverb
      \verb{url}
      \verb http://proceedings.mlr.press/v119/plumb20a.html
      \endverb
    \endentry
    \entry{6547979}{article}{}
      \name{author}{1}{}{%
        {{hash=6978ffe24dbefc27c8947b9474ebfd10}{%
           family={{O'Leary}},
           familyi={O\bibinitperiod},
           given={D.\bibnamedelimi E.},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{6978ffe24dbefc27c8947b9474ebfd10}
      \strng{fullhash}{6978ffe24dbefc27c8947b9474ebfd10}
      \strng{bibnamehash}{6978ffe24dbefc27c8947b9474ebfd10}
      \strng{authorbibnamehash}{6978ffe24dbefc27c8947b9474ebfd10}
      \strng{authornamehash}{6978ffe24dbefc27c8947b9474ebfd10}
      \strng{authorfullhash}{6978ffe24dbefc27c8947b9474ebfd10}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{IEEE Intelligent Systems}
      \field{number}{2}
      \field{title}{Artificial Intelligence and Big Data}
      \field{volume}{28}
      \field{year}{2013}
      \field{pages}{96\bibrangedash 99}
      \range{pages}{4}
      \verb{doi}
      \verb 10.1109/MIS.2013.39
      \endverb
    \endentry
    \entry{van2009dimensionality}{article}{}
      \name{author}{3}{}{%
        {{hash=a33ababcca5b83a7777452957fb2eef2}{%
           family={Van\bibnamedelimb Der\bibnamedelima Maaten},
           familyi={V\bibinitperiod\bibinitdelim D\bibinitperiod\bibinitdelim M\bibinitperiod},
           given={Laurens},
           giveni={L\bibinitperiod}}}%
        {{hash=42ac0ed20b82df7a89b9a9f44b0cfa07}{%
           family={Postma},
           familyi={P\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=31c34e2159faea45d29fc9d24fe62451}{%
           family={Van\bibnamedelimb den\bibnamedelima Herik},
           familyi={V\bibinitperiod\bibinitdelim d\bibinitperiod\bibinitdelim H\bibinitperiod},
           given={Jaap},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{2ceec32fee0ebeb0796bd339b8418ab8}
      \strng{fullhash}{2ceec32fee0ebeb0796bd339b8418ab8}
      \strng{bibnamehash}{2ceec32fee0ebeb0796bd339b8418ab8}
      \strng{authorbibnamehash}{2ceec32fee0ebeb0796bd339b8418ab8}
      \strng{authornamehash}{2ceec32fee0ebeb0796bd339b8418ab8}
      \strng{authorfullhash}{2ceec32fee0ebeb0796bd339b8418ab8}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear tech- niques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identi- fying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved. Real-world data, such as speech signals, digital photographs, or fMRI scans, usually has a high dimen- sionality. In order to handle such real-world data adequately, its dimensionality needs to be reduced. Dimensionality reduction is the transformation of high-dimensional data into a meaningful representa- tion of reduced dimensionality. Ideally, the reduced representation should have a dimensionality that corresponds to the intrinsic dimensionality of the data. The intrinsic dimensionality of data is the mini- mum number of parameters needed to account for the observed properties of the data [49]. Dimension- ality reduction is important in many domains, since it mitigates the curse of dimensionality and other undesired properties of high-dimensional spaces [69]. As a result, dimensionality reduction facilitates, among others, classification, visualization, and compression of high-dimensional data. Traditionally, di- mensionality reduction was performed using linear techniques such as Principal Components Analysis (PCA) [98], factor analysis [117], and classical scaling [126]. However, these linear techniques cannot adequately handle complex nonlinear data. In the last decade, a large number of nonlinear techniques for dimensionality reduction have been proposed. See for an overview, e.g., [26, 110, 83, 131]. In contrast to the traditional linear techniques, the nonlinear techniques have the ability to deal with complex nonlinear data. In particular for real- world data, the nonlinear dimensionality reduction techniques may offer an advantage, because real- world data is likely to form a highly nonlinear manifold. Previous studies have shown that nonlinear techniques outperform their linear counterparts on complex artificial tasks. For instance, the Swiss roll dataset comprises a set of points that lie on a spiral-like two-dimensional manifold that is embedded within a three-dimensional space. A vast number of nonlinear techniques are perfectly able to find this embedding, whereas linear techniques fail to do so. In contrast to these successes on artificial datasets, successful applications of nonlinear dimensionality reduction techniques on natural datasets are less convincing. Beyond this observation, it is not clear to what extent the performances of the various dimensionality reduction techniques differ on artificial and natural tasks (a comparison is performed in [94], but this comparison is very limited in scope with respect to the number of techniques and tasks that are addressed). Motivated by the lack of a systematic comparison of dimensionality reduction techniques, this paper presents a comparative study of the most important linear dimensionality reduction technique (PCA), and twelve frontranked nonlinear dimensionality reduction techniques. The aims of the paper are (1) to investigate to what extent novel nonlinear dimensionality reduction techniques outperform the tradi- tional PCA on real-world datasets and (2) to identify the inherent weaknesses of the twelve nonlinear dimensionality reduction techniques. The investigation is performed by both a theoretical and an empir- ical evaluation of the dimensionality reduction techniques. The identification is performed by a careful analysis of the empirical results on specifically designed artificial datasets and on a selection of real- world datasets. Next to PCA, the paper investigates the following twelve nonlinear techniques: (1) Kernel PCA, (2) Isomap, (3) Maximum Variance Unfolding, (4) diffusion maps, (5) Locally Linear Embedding, (6) Laplacian Eigenmaps, (7) Hessian LLE, (8) Local Tangent Space Analysis, (9) Sammon mapping, (10) multilayer autoencoders, (11) Locally Linear Coordination, and (12) manifold charting.}
      \field{journaltitle}{J Mach Learn Res}
      \field{title}{Dimensionality reduction: a comparative review}
      \field{volume}{10}
      \field{year}{2009}
      \field{pages}{66\bibrangedash 71}
      \range{pages}{6}
      \keyw{PCA data_analysis machine_learning methods review visualization}
    \endentry
    \entry{MANCISIDOR2021114020}{article}{}
      \name{author}{4}{}{%
        {{hash=9300d81d498dd9e4f0816e2443531af5}{%
           family={Mancisidor},
           familyi={M\bibinitperiod},
           given={Rogelio\bibnamedelima A.},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=2cf9395e6d8fe45121104146279d72b9}{%
           family={Kampffmeyer},
           familyi={K\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=13231a6294600969441e96213fd0b347}{%
           family={Aas},
           familyi={A\bibinitperiod},
           given={Kjersti},
           giveni={K\bibinitperiod}}}%
        {{hash=bdba8de26a82987337d7a09e90c8f032}{%
           family={Jenssen},
           familyi={J\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{0f49028e18f769e55ac1567e31a97bf6}
      \strng{fullhash}{d8523b068050c4ba9af5f0641ad153c8}
      \strng{bibnamehash}{d8523b068050c4ba9af5f0641ad153c8}
      \strng{authorbibnamehash}{d8523b068050c4ba9af5f0641ad153c8}
      \strng{authornamehash}{0f49028e18f769e55ac1567e31a97bf6}
      \strng{authorfullhash}{d8523b068050c4ba9af5f0641ad153c8}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Learning data representations that reflect the customers’ creditworthiness can improve marketing campaigns, customer relationship management, data and process management or the credit risk assessment in retail banks. In this research, we show that it is possible to steer data representations in the latent space of the Variational Autoencoder (VAE) using a semi-supervised learning framework and a specific grouping of the input data called Weight of Evidence (WoE). Our proposed method learns a latent representation of the data showing a well-defied clustering structure. The clustering structure captures the customers’ creditworthiness, which is unknown a priori and cannot be identified in the input space. The main advantages of our proposed method are that it captures the natural clustering of the data, suggests the number of clusters, captures the spatial coherence of customers’ creditworthiness, generates data representations of unseen customers and assign them to one of the existing clusters. Our empirical results, based on real data sets reflecting different market and economic conditions, show that none of the well-known data representation models in the benchmark analysis are able to obtain well-defined clustering structures like our proposed method. Further, we show how banks can use our proposed methodology to improve marketing campaigns and credit risk assessment.}
      \field{issn}{0957-4174}
      \field{journaltitle}{Expert Systems with Applications}
      \field{title}{Learning latent representations of bank customers with the Variational Autoencoder}
      \field{volume}{164}
      \field{year}{2021}
      \field{pages}{114020}
      \range{pages}{1}
      \verb{doi}
      \verb https://doi.org/10.1016/j.eswa.2020.114020
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0957417420307910
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0957417420307910
      \endverb
      \keyw{Variational Autoencoder,Data representations,Clustering,Machine learning}
    \endentry
    \entry{he2019mlframeworks}{article}{}
      \name{author}{1}{}{%
        {{hash=339d072cae7a701881476cb4e4958adc}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Horace},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{339d072cae7a701881476cb4e4958adc}
      \strng{fullhash}{339d072cae7a701881476cb4e4958adc}
      \strng{bibnamehash}{339d072cae7a701881476cb4e4958adc}
      \strng{authorbibnamehash}{339d072cae7a701881476cb4e4958adc}
      \strng{authornamehash}{339d072cae7a701881476cb4e4958adc}
      \strng{authorfullhash}{339d072cae7a701881476cb4e4958adc}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{howpublished}{\url{https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/ }}
      \field{journaltitle}{The Gradient}
      \field{title}{The State of Machine Learning Frameworks in 2019}
      \field{year}{2019}
    \endentry
    \entry{ding2018interpretable}{article}{}
      \name{author}{3}{}{%
        {{hash=239a6c8297bba2efc12eab9860aa3b90}{%
           family={Ding},
           familyi={D\bibinitperiod},
           given={Jiarui},
           giveni={J\bibinitperiod}}}%
        {{hash=8c066f798710ece4dc0fa6384d6f49d2}{%
           family={Condon},
           familyi={C\bibinitperiod},
           given={Anne},
           giveni={A\bibinitperiod}}}%
        {{hash=6495095496c06f7b43d2455d7a76348c}{%
           family={Shah},
           familyi={S\bibinitperiod},
           given={Sohrab\bibnamedelima P.},
           giveni={S\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{e360a4dbf9572bc6a93c6c29068b8fb9}
      \strng{fullhash}{e360a4dbf9572bc6a93c6c29068b8fb9}
      \strng{bibnamehash}{e360a4dbf9572bc6a93c6c29068b8fb9}
      \strng{authorbibnamehash}{e360a4dbf9572bc6a93c6c29068b8fb9}
      \strng{authornamehash}{e360a4dbf9572bc6a93c6c29068b8fb9}
      \strng{authorfullhash}{e360a4dbf9572bc6a93c6c29068b8fb9}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Single-cell RNA-sequencing has great potential to discover cell types, identify cell states, trace development lineages, and reconstruct the spatial organization of cells. However, dimension reduction to interpret structure in single-cell sequencing data remains a challenge. Existing algorithms are either not able to uncover the clustering structures in the data or lose global information such as groups of clusters that are close to each other. We present a robust statistical model, scvis, to capture and visualize the low-dimensional structures in single-cell gene expression data. Simulation results demonstrate that low-dimensional representations learned by scvis preserve both the local and global neighbor structures in the data. In addition, scvis is robust to the number of data points and learns a probabilistic parametric mapping function to add new data points to an existing embedding. We then use scvis to analyze four single-cell RNA-sequencing datasets, exemplifying interpretable two-dimensional representations of the high-dimensional single-cell RNA-sequencing data.}
      \field{issn}{2041-1723}
      \field{journaltitle}{Nature Communications}
      \field{month}{5}
      \field{number}{1}
      \field{title}{Interpretable dimensionality reduction of single cell transcriptome data with deep generative models}
      \field{urlday}{13}
      \field{urlmonth}{4}
      \field{urlyear}{2021}
      \field{volume}{9}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{2002}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41467-018-04368-5
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41467-018-04368-5
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41467-018-04368-5
      \endverb
    \endentry
    \entry{doi:10.1198/106186006X113430}{article}{}
      \name{author}{3}{}{%
        {{hash=f15b129e88b4820c7ed982265e79986b}{%
           family={Zou},
           familyi={Z\bibinitperiod},
           given={Hui},
           giveni={H\bibinitperiod}}}%
        {{hash=0cb8fe4210baa81c4b0e67913b4d2768}{%
           family={Hastie},
           familyi={H\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=88eea600247d798f8b2ce0b2dc614492}{%
           family={Tibshirani},
           familyi={T\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Taylor & Francis}%
      }
      \strng{namehash}{6629879fee81473207a091642daba4f1}
      \strng{fullhash}{6629879fee81473207a091642daba4f1}
      \strng{bibnamehash}{6629879fee81473207a091642daba4f1}
      \strng{authorbibnamehash}{6629879fee81473207a091642daba4f1}
      \strng{authornamehash}{6629879fee81473207a091642daba4f1}
      \strng{authorfullhash}{6629879fee81473207a091642daba4f1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of Computational and Graphical Statistics}
      \field{number}{2}
      \field{title}{Sparse Principal Component Analysis}
      \field{volume}{15}
      \field{year}{2006}
      \field{pages}{265\bibrangedash 286}
      \range{pages}{22}
      \verb{doi}
      \verb 10.1198/106186006X113430
      \endverb
      \verb{eprint}
      \verb https://doi.org/10.1198/106186006X113430
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1198/106186006X113430
      \endverb
      \verb{url}
      \verb https://doi.org/10.1198/106186006X113430
      \endverb
    \endentry
    \entry{Roweis2323}{article}{}
      \name{author}{2}{}{%
        {{hash=c89a3b977a0617edc15b2d82b36929f3}{%
           family={Roweis},
           familyi={R\bibinitperiod},
           given={Sam\bibnamedelima T.},
           giveni={S\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=cd2d8ad2373696effc18491288917dd1}{%
           family={Saul},
           familyi={S\bibinitperiod},
           given={Lawrence\bibnamedelima K.},
           giveni={L\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {American Association for the Advancement of Science}%
      }
      \strng{namehash}{91a4781c6ebe461831ea3f4c50e3294d}
      \strng{fullhash}{91a4781c6ebe461831ea3f4c50e3294d}
      \strng{bibnamehash}{91a4781c6ebe461831ea3f4c50e3294d}
      \strng{authorbibnamehash}{91a4781c6ebe461831ea3f4c50e3294d}
      \strng{authornamehash}{91a4781c6ebe461831ea3f4c50e3294d}
      \strng{authorfullhash}{91a4781c6ebe461831ea3f4c50e3294d}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.}
      \field{issn}{0036-8075}
      \field{journaltitle}{Science}
      \field{number}{5500}
      \field{title}{Nonlinear Dimensionality Reduction by Locally Linear Embedding}
      \field{volume}{290}
      \field{year}{2000}
      \field{pages}{2323\bibrangedash 2326}
      \range{pages}{4}
      \verb{doi}
      \verb 10.1126/science.290.5500.2323
      \endverb
      \verb{eprint}
      \verb https://science.sciencemag.org/content/290/5500/2323.full.pdf
      \endverb
      \verb{urlraw}
      \verb https://science.sciencemag.org/content/290/5500/2323
      \endverb
      \verb{url}
      \verb https://science.sciencemag.org/content/290/5500/2323
      \endverb
    \endentry
    \entry{cayton2005algorithms}{article}{}
      \name{author}{1}{}{%
        {{hash=3afa2079494a10557b852a667f753932}{%
           family={Cayton},
           familyi={C\bibinitperiod},
           given={Lawrence},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{3afa2079494a10557b852a667f753932}
      \strng{fullhash}{3afa2079494a10557b852a667f753932}
      \strng{bibnamehash}{3afa2079494a10557b852a667f753932}
      \strng{authorbibnamehash}{3afa2079494a10557b852a667f753932}
      \strng{authornamehash}{3afa2079494a10557b852a667f753932}
      \strng{authorfullhash}{3afa2079494a10557b852a667f753932}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Univ. of California at San Diego Tech. Rep}
      \field{number}{1-17}
      \field{title}{Algorithms for manifold learning}
      \field{volume}{12}
      \field{year}{2005}
      \field{pages}{1}
      \range{pages}{1}
    \endentry
    \entry{10.1007/BFb0020217}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=ca31cc11ec9370460148c3a9c48fce45}{%
           family={Schölkopf},
           familyi={S\bibinitperiod},
           given={Bernhard},
           giveni={B\bibinitperiod}}}%
        {{hash=1817acab40d9caaac71e1597049b7f5e}{%
           family={Smola},
           familyi={S\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=9f1b6144a45b1967e989e74552e37ada}{%
           family={Müller},
           familyi={M\bibinitperiod},
           given={Klaus-Robert},
           giveni={K\bibinithyphendelim R\bibinitperiod}}}%
      }
      \name{editor}{4}{}{%
        {{hash=6b2c6d8dbc66b301f2a7077d805f0ba7}{%
           family={Gerstner},
           familyi={G\bibinitperiod},
           given={Wulfram},
           giveni={W\bibinitperiod}}}%
        {{hash=1d907d83264677e99320c5edfeae1c9a}{%
           family={Germond},
           familyi={G\bibinitperiod},
           given={Alain},
           giveni={A\bibinitperiod}}}%
        {{hash=8d6c1df0c111e79b87cf7bfe33f96045}{%
           family={Hasler},
           familyi={H\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=9a51bc4241a1699f5960eff8bc835e98}{%
           family={Nicoud},
           familyi={N\bibinitperiod},
           given={Jean-Daniel},
           giveni={J\bibinithyphendelim D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{8bbec8bef3418303f4926c0829c71fe1}
      \strng{fullhash}{8bbec8bef3418303f4926c0829c71fe1}
      \strng{bibnamehash}{8bbec8bef3418303f4926c0829c71fe1}
      \strng{authorbibnamehash}{8bbec8bef3418303f4926c0829c71fe1}
      \strng{authornamehash}{8bbec8bef3418303f4926c0829c71fe1}
      \strng{authorfullhash}{8bbec8bef3418303f4926c0829c71fe1}
      \strng{editorbibnamehash}{05896244b034fe22dc1d1c13be15e088}
      \strng{editornamehash}{e298d3e0877a998d5359b2f6efe41f1b}
      \strng{editorfullhash}{05896244b034fe22dc1d1c13be15e088}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.}
      \field{booktitle}{Artificial Neural Networks --- ICANN'97}
      \field{isbn}{978-3-540-69620-9}
      \field{title}{Kernel principal component analysis}
      \field{year}{1997}
      \field{pages}{583\bibrangedash 588}
      \range{pages}{6}
    \endentry
    \entry{shekhar2016comprehensive}{article}{}
      \name{author}{14}{}{%
        {{hash=f8b48ce5648f5b47e0ffdf2f608c477f}{%
           family={Shekhar},
           familyi={S\bibinitperiod},
           given={Karthik},
           giveni={K\bibinitperiod}}}%
        {{hash=818e324af724a29e99b01efe2a007e7f}{%
           family={Lapan},
           familyi={L\bibinitperiod},
           given={Sylvain\bibnamedelima W.},
           giveni={S\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=576e50105169fa8f61d6393212272535}{%
           family={Whitney},
           familyi={W\bibinitperiod},
           given={Irene\bibnamedelima E.},
           giveni={I\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=b790a02211352bfdd35eeddd5b342f8a}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Nicholas\bibnamedelima M.},
           giveni={N\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=e9225335e0413d2a87ca9e4abe2f2e05}{%
           family={Macosko},
           familyi={M\bibinitperiod},
           given={Evan\bibnamedelima Z.},
           giveni={E\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=8d0b6874d5acbb7548d431aa1479b8b7}{%
           family={Kowalczyk},
           familyi={K\bibinitperiod},
           given={Monika},
           giveni={M\bibinitperiod}}}%
        {{hash=1a1d0a8e56c4b1e21b0544f4fa501f81}{%
           family={Adiconis},
           familyi={A\bibinitperiod},
           given={Xian},
           giveni={X\bibinitperiod}}}%
        {{hash=53585fddd1b34060c2296220b2d36394}{%
           family={Levin},
           familyi={L\bibinitperiod},
           given={Joshua\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=b1221e9e59330320ac5d1684c9c03ca2}{%
           family={Nemesh},
           familyi={N\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=7c77d987ab6a0b9250466f2caaad69ba}{%
           family={Goldman},
           familyi={G\bibinitperiod},
           given={Melissa},
           giveni={M\bibinitperiod}}}%
        {{hash=5d0e21bc71fd899339c3d8825e1e3560}{%
           family={McCarroll},
           familyi={M\bibinitperiod},
           given={Steven\bibnamedelima A.},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=5a49f5a1f5b0085585ab4b33f03d0216}{%
           family={Cepko},
           familyi={C\bibinitperiod},
           given={Constance\bibnamedelima L.},
           giveni={C\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=f08f6961a7cb7d9674fb102073c60ca3}{%
           family={Regev},
           familyi={R\bibinitperiod},
           given={Aviv},
           giveni={A\bibinitperiod}}}%
        {{hash=a571782923d42814e171c890d25f4b0b}{%
           family={Sanes},
           familyi={S\bibinitperiod},
           given={Joshua\bibnamedelima R.},
           giveni={J\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {eng}%
      }
      \strng{namehash}{f7873de9b02004b9e8b04d4ae3c646f9}
      \strng{fullhash}{438fe8f87f449256ee05e3993f781dc9}
      \strng{bibnamehash}{f7873de9b02004b9e8b04d4ae3c646f9}
      \strng{authorbibnamehash}{f7873de9b02004b9e8b04d4ae3c646f9}
      \strng{authornamehash}{f7873de9b02004b9e8b04d4ae3c646f9}
      \strng{authorfullhash}{438fe8f87f449256ee05e3993f781dc9}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Patterns of gene expression can be used to characterize and classify neuronal types. It is challenging, however, to generate taxonomies that fulfill the essential criteria of being comprehensive, harmonizing with conventional classification schemes, and lacking superfluous subdivisions of genuine types. To address these challenges, we used massively parallel single-cell RNA profiling and optimized computational methods on a heterogeneous class of neurons, mouse retinal bipolar cells (BCs). From a population of ∼25,000 BCs, we derived a molecular classification that identified 15 types, including all types observed previously and two novel types, one of which has a non-canonical morphology and position. We validated the classification scheme and identified dozens of novel markers using methods that match molecular expression to cell morphology. This work provides a systematic methodology for achieving comprehensive molecular classification of neurons, identifies novel neuronal types, and uncovers transcriptional differences that distinguish types within a class.}
      \field{issn}{1097-4172}
      \field{journaltitle}{Cell}
      \field{month}{8}
      \field{number}{5}
      \field{title}{Comprehensive {Classification} of {Retinal} {Bipolar} {Neurons} by {Single}-{Cell} {Transcriptomics}}
      \field{volume}{166}
      \field{year}{2016}
      \field{pages}{1308\bibrangedash 1323.e30}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1016/j.cell.2016.07.054
      \endverb
      \keyw{Amacrine Cells,Animals,Cluster Analysis,Female,Genetic Markers,Male,Mice,Mice,Inbred Strains,Mice,Transgenic,Retinal Bipolar Cells,Sequence Analysis,RNA,Single-Cell Analysis,Transcription,Genetic,Transcriptome}
    \endentry
    \entry{damiaan_j_w_reijnaers_2021_4686025}{software}{}
      \name{author}{3}{}{%
        {{hash=62f1daf69d313221070571c62e8dc368}{%
           family={Reijnaers},
           familyi={R\bibinitperiod},
           given={Damiaan\bibnamedelimb J.\bibnamedelimi W.},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=2daa18c6690755bf40c6d8b3351be50f}{%
           family={Pavert},
           familyi={P\bibinitperiod},
           given={Daniël\bibnamedelima B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
           prefix={van\bibnamedelima de},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=51b5040c921f5646da859fdab0ebd6bf}{%
           family={Scheuer},
           familyi={S\bibinitperiod},
           given={Giguru},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Zenodo}%
      }
      \strng{namehash}{008a116bcca96f514af7bad5dcaf7fa9}
      \strng{fullhash}{008a116bcca96f514af7bad5dcaf7fa9}
      \strng{bibnamehash}{008a116bcca96f514af7bad5dcaf7fa9}
      \strng{authorbibnamehash}{008a116bcca96f514af7bad5dcaf7fa9}
      \strng{authornamehash}{008a116bcca96f514af7bad5dcaf7fa9}
      \strng{authorfullhash}{008a116bcca96f514af7bad5dcaf7fa9}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{4}
      \field{title}{{damiaanr/fact-ai: Reproduction of (Plumb et al., 2020)}}
      \field{version}{1.0.0}
      \field{year}{2021}
      \verb{doi}
      \verb 10.5281/zenodo.4686025
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.5281/zenodo.4686025
      \endverb
      \verb{url}
      \verb https://doi.org/10.5281/zenodo.4686025
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput


\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\babel@aux{american}{}
\pgfsyspdfmark {pgfid1}{8391059}{49131591}
\newmarginnote{note.1.1}{{1}{8391059sp}}
\abx@aux@cite{han2016eie}
\abx@aux@segm{0}{0}{han2016eie}
\abx@aux@cite{ashbyexploiting}
\abx@aux@segm{0}{0}{ashbyexploiting}
\abx@aux@cite{Srinivas_2017_CVPR_Workshops}
\abx@aux@segm{0}{0}{Srinivas_2017_CVPR_Workshops}
\abx@aux@cite{han2016eie}
\abx@aux@segm{0}{0}{han2016eie}
\abx@aux@cite{ashbyexploiting}
\abx@aux@segm{0}{0}{ashbyexploiting}
\abx@aux@cite{Srinivas_2017_CVPR_Workshops}
\abx@aux@segm{0}{0}{Srinivas_2017_CVPR_Workshops}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{abadi2016tensorflow}
\abx@aux@segm{0}{0}{abadi2016tensorflow}
\abx@aux@cite{abadi2016tensorflow}
\abx@aux@segm{0}{0}{abadi2016tensorflow}
\abx@aux@cite{Pytorch}
\abx@aux@segm{0}{0}{Pytorch}
\abx@aux@cite{Pytorch}
\abx@aux@segm{0}{0}{Pytorch}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{wandb}
\abx@aux@segm{0}{0}{wandb}
\abx@aux@cite{wandb}
\abx@aux@segm{0}{0}{wandb}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Scope of reproducibility}{3}{section.2}\protected@file@percent }
\newlabel{sec:claims}{{2}{3}{Scope of reproducibility}{section.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{Wide_ResNet_BMVC2016_87}
\abx@aux@segm{0}{0}{Wide_ResNet_BMVC2016_87}
\abx@aux@cite{Wide_ResNet_BMVC2016_87}
\abx@aux@segm{0}{0}{Wide_ResNet_BMVC2016_87}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\abx@aux@cite{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.1}Mask Initialization}{4}{subsubsection.3.0.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.2}Mask Updates}{4}{subsubsection.3.0.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.3}Pruning Strategy}{4}{subsubsection.3.0.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.4}Growth Strategy}{4}{subsubsection.3.0.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Settings}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model descriptions}{4}{subsection.4.1}\protected@file@percent }
\abx@aux@cite{goyal2017accurate}
\abx@aux@segm{0}{0}{goyal2017accurate}
\abx@aux@cite{goyal2017accurate}
\abx@aux@segm{0}{0}{goyal2017accurate}
\abx@aux@cite{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{He_2019_CVPR}
\abx@aux@segm{0}{0}{He_2019_CVPR}
\abx@aux@cite{He_2019_CVPR}
\abx@aux@segm{0}{0}{He_2019_CVPR}
\abx@aux@cite{optuna_2019}
\abx@aux@segm{0}{0}{optuna_2019}
\abx@aux@cite{optuna_2019}
\abx@aux@segm{0}{0}{optuna_2019}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Test accuracy of reference and our implementations on CIFAR-10,} tabulated for three different sparsities. Note that the runs listed here do not use a separate validation set while training.\relax }}{5}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:replication_verify}{{1}{5}{\textbf {Test accuracy of reference and our implementations on CIFAR-10,} tabulated for three different sparsities. Note that the runs listed here do not use a separate validation set while training.\relax }{table.caption.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Datasets and Training descriptions}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Hyperparameters}{5}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Baseline implementations}{5}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Computational requirements}{5}{subsection.4.5}\protected@file@percent }
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {WideResNet-22-2 on CIFAR10}, tabulated for two density $(1-s)$ values. We group methods by their FLOP requirement and in each group, we mark the best accuracy in bold. Similar to \citeauthor {rigl}\supercite {rigl}, we assume that algorithms utilize sparsity during training. All results are obtained by methods implemented in our unified codebase.\relax }}{6}{table.caption.2}\protected@file@percent }
\newlabel{tab:cifar10-main-results}{{2}{6}{\textbf {WideResNet-22-2 on CIFAR10}, tabulated for two density $(1-s)$ values. We group methods by their FLOP requirement and in each group, we mark the best accuracy in bold. Similar to \citet {rigl}, we assume that algorithms utilize sparsity during training. All results are obtained by methods implemented in our unified codebase.\relax }{table.caption.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{6}{section.5}\protected@file@percent }
\newlabel{sec:results}{{5}{6}{Results}{section.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}WideResNet-22 on CIFAR-10}{6}{subsection.5.1}\protected@file@percent }
\newlabel{cifar-10-results}{{5.1}{6}{WideResNet-22 on CIFAR-10}{subsection.5.1}{}}
\abx@aux@cite{frankle2018lottery}
\abx@aux@segm{0}{0}{frankle2018lottery}
\abx@aux@cite{frankle2018lottery}
\abx@aux@segm{0}{0}{frankle2018lottery}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Test Accuracy vs Sparsity on CIFAR-10,} plotted for Random initialization \textbf  {(left)}, ERK initialization \textbf  {(center)}, and for training $2\times $ longer \textbf  {(right)}. Owing to random growth, SET can be unstable when training for longer durations with higher sparsities. Overall, \textit  {RigL}\textsubscript  {$2 \times $} (ERK) achieves highest test accuracy.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:cifar10-main-results}{{1}{7}{\textbf {Test Accuracy vs Sparsity on CIFAR-10,} plotted for Random initialization \textbf {(left)}, ERK initialization \textbf {(center)}, and for training $2\times $ longer \textbf {(right)}. Owing to random growth, SET can be unstable when training for longer durations with higher sparsities. Overall, \textit {RigL}\textsubscript {$2 \times $} (ERK) achieves highest test accuracy.\relax }{figure.caption.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}ResNet-50 on CIFAR100}{7}{subsection.5.2}\protected@file@percent }
\newlabel{cifar-100-results}{{5.2}{7}{ResNet-50 on CIFAR100}{subsection.5.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces entry for figure}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:cifar100-main-results}{{2}{7}{entry for figure}{figure.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Benchmarking sparse ResNet-50s on CIFAR-100,} tabulated by performance and cost \textbf  {(below)}, and plotted across densities \textbf  {(right)}. In each group below, \textit  {RigL} outperforms or matches existing sparse-to-sparse and dense-to-sparse methods. Notably, \textit  {RigL}\textsubscript  {$3\times $} at 90\% sparsity and \textit  {RigL}\textsubscript  {$2\times $} at 80\% sparsity surpass iterative pruning with similar FLOP consumption. \textit  {RigL}\textsubscript  {$2\times $} (ERK) further improves performance but requires a larger training budget. \relax }}{7}{table.3}\protected@file@percent }
\newlabel{tab:cifar100-main-results}{{3}{7}{\textbf {Benchmarking sparse ResNet-50s on CIFAR-100,} tabulated by performance and cost \textbf {(below)}, and plotted across densities \textbf {(right)}. In each group below, \textit {RigL} outperforms or matches existing sparse-to-sparse and dense-to-sparse methods. Notably, \textit {RigL}\textsubscript {$3\times $} at 90\% sparsity and \textit {RigL}\textsubscript {$2\times $} at 80\% sparsity surpass iterative pruning with similar FLOP consumption. \textit {RigL}\textsubscript {$2\times $} (ERK) further improves performance but requires a larger training budget. \relax }{table.3}{}}
\abx@aux@cite{TPE_Bergstra}
\abx@aux@segm{0}{0}{TPE_Bergstra}
\abx@aux@cite{TPE_Bergstra}
\abx@aux@segm{0}{0}{TPE_Bergstra}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hyperparameter Tuning}{8}{subsection.5.3}\protected@file@percent }
\newlabel{hyperparameter-tuning}{{5.3}{8}{Hyperparameter Tuning}{subsection.5.3}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }}{8}{table.caption.4}\protected@file@percent }
\newlabel{tab:effect-alpha-deltaT}{{4}{8}{\textbf {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }{table.caption.4}{}}
\newlabel{tab:replication_verify}{{4}{8}{\textbf {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }{table.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}$(\alpha , \Delta T)$ vs Sparsities}{8}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Learning Rate vs Sparsity on CIFAR-10.} Runs using a learning rate $> 0.1$ do not converge and are not plotted here. There is little benefit in tuning the learning rate for each sparsity, and $0.1, 0.05$ are good choices overall.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:lr-sweep}{{3}{8}{\textbf {Learning Rate vs Sparsity on CIFAR-10.} Runs using a learning rate $> 0.1$ do not converge and are not plotted here. There is little benefit in tuning the learning rate for each sparsity, and $0.1, 0.05$ are good choices overall.\relax }{figure.caption.5}{}}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Learning Rate vs Sparsities}{9}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Results beyond Original Paper}{9}{section.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sparsity Distribution vs FLOP Consumption}{9}{subsection.6.1}\protected@file@percent }
\newlabel{effect-sparsity-distribution}{{6.1}{9}{Sparsity Distribution vs FLOP Consumption}{subsection.6.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Test Accuracy vs FLOP consumption of WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100,} compared for Random and ERK initializations. For the same FLOP budget, models trained with ERK initialization must be more sparse, resulting in inferior performance.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:erk-vs-random-FLOPs}{{4}{9}{\textbf {Test Accuracy vs FLOP consumption of WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100,} compared for Random and ERK initializations. For the same FLOP budget, models trained with ERK initialization must be more sparse, resulting in inferior performance.\relax }{figure.caption.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Effect of Redistribution}{9}{subsection.6.2}\protected@file@percent }
\newlabel{effect-redistribution}{{6.2}{9}{Effect of Redistribution}{subsection.6.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Effect of redistribution during \textit  {RigL} updates, evaluated on CIFAR10 and CIFAR100}. By utilising sparse gradient or sparse momentum based redistribution, \textit  {RigL} (Random) matches \textit  {RigL} (ERK)'s performance. Among Random and ERK initialized experiments, we mark the best metrics under each sparsity and dataset in bold. \relax }}{9}{table.caption.7}\protected@file@percent }
\newlabel{tab:effect-redistribution}{{5}{9}{\textbf {Effect of redistribution during \textit {RigL} updates, evaluated on CIFAR10 and CIFAR100}. By utilising sparse gradient or sparse momentum based redistribution, \textit {RigL} (Random) matches \textit {RigL} (ERK)'s performance. Among Random and ERK initialized experiments, we mark the best metrics under each sparsity and dataset in bold. \relax }{table.caption.7}{}}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Effect of redistribution on \textit  {RigL}'s performance,} evaluated using WideResNet-22-2 on CIFAR10 at 80\% sparsity. \textbf  {(left)} FLOPs required per forward pass, shown relative to the dense baseline, rises quickly and saturates within a few epochs (\textasciitilde 10k steps) for both sparse gradient and sparse momentum based redistribution. \textbf  {(right)} Comparison of the final density distribution against Random and ERK counterparts. ``b'' refers to block and ``l'' layer here.\relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:density-dist-evolution}{{5}{10}{\textbf {Effect of redistribution on \textit {RigL}'s performance,} evaluated using WideResNet-22-2 on CIFAR10 at 80\% sparsity. \textbf {(left)} FLOPs required per forward pass, shown relative to the dense baseline, rises quickly and saturates within a few epochs (\textasciitilde 10k steps) for both sparse gradient and sparse momentum based redistribution. \textbf {(right)} Comparison of the final density distribution against Random and ERK counterparts. ``b'' refers to block and ``l'' layer here.\relax }{figure.caption.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{10}{section.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.1}What was easy}{10}{subsubsection.7.0.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.2}What was difficult}{10}{subsubsection.7.0.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.3}Communication with original authors}{11}{subsubsection.7.0.3}\protected@file@percent }
\abx@aux@cite{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\abx@aux@cite{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\abx@aux@cite{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{ILSVRC15}
\abx@aux@segm{0}{0}{ILSVRC15}
\abx@aux@cite{ILSVRC15}
\abx@aux@segm{0}{0}{ILSVRC15}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {A}Architecture Specific Details---ResNet-50 on CIFAR100}{13}{appendix.A}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \textbf  {ResNet-50 architecture used on CIFAR100}. Building blocks are shown in brackets, with the numbers of blocks stacked. Downsampling is performed by conv3\_1, conv4\_1, and conv5\_1 with a stride of 2. \relax }}{13}{table.caption.9}\protected@file@percent }
\newlabel{tab:architecture}{{6}{13}{\textbf {ResNet-50 architecture used on CIFAR100}. Building blocks are shown in brackets, with the numbers of blocks stacked. Downsampling is performed by conv3\_1, conv4\_1, and conv5\_1 with a stride of 2. \relax }{table.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {B}FLOP Counting Procedure}{13}{appendix.B}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Inference FLOPs}{13}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Small-Dense, RigL, SET, Static}{13}{subsubsection.B.1.1}\protected@file@percent }
\abx@aux@cite{gray2017gpu}
\abx@aux@segm{0}{0}{gray2017gpu}
\abx@aux@cite{Vooturi_2019_ICCV}
\abx@aux@segm{0}{0}{Vooturi_2019_ICCV}
\abx@aux@cite{gray2017gpu}
\abx@aux@segm{0}{0}{gray2017gpu}
\abx@aux@cite{Vooturi_2019_ICCV}
\abx@aux@segm{0}{0}{Vooturi_2019_ICCV}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}SNFS, Pruning}{14}{subsubsection.B.1.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Train FLOPs}{14}{subsection.B.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.1}Small-Dense, Static}{14}{subsubsection.B.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.2}SET}{14}{subsubsection.B.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.3}RigL}{14}{subsubsection.B.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.4}SNFS}{14}{subsubsection.B.2.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.5}Pruning}{14}{subsubsection.B.2.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {C}Trial Space of Hyperparameter Tuning}{14}{appendix.C}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {D}Dynamic Structured Sparsity}{14}{appendix.D}\protected@file@percent }
\newlabel{structured-sparsity}{{D}{14}{Dynamic Structured Sparsity}{appendix.D}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces \textbf  {Modifying \textit  {RigL} for structured sparsity, compared on CIFAR-10 and CIFAR-100 datasets.} \textit  {RigL}-struct fails to match the accuracy of \textit  {RigL} and just matches Small-Dense in performance. \relax }}{14}{table.caption.11}\protected@file@percent }
\newlabel{tab:structured-sparsity}{{7}{14}{\textbf {Modifying \textit {RigL} for structured sparsity, compared on CIFAR-10 and CIFAR-100 datasets.} \textit {RigL}-struct fails to match the accuracy of \textit {RigL} and just matches Small-Dense in performance. \relax }{table.caption.11}{}}
\abx@aux@read@bbl@mdfivesum{B6C4E9616B6E195807BA1DF17E554739}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{han2016eie}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ashbyexploiting}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Srinivas_2017_CVPR_Workshops}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rigl}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Mocanu2018SET}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{dettmers2020sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{to_prune_or_not}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{abadi2016tensorflow}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Pytorch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wandb}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Krizhevsky09learningmultiple}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Wide_ResNet_BMVC2016_87}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{He_2016_CVPR}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{goyal2017accurate}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ioffe2015batch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{He_2019_CVPR}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{optuna_2019}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{frankle2018lottery}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{TPE_Bergstra}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ILSVRC15}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gray2017gpu}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Vooturi_2019_ICCV}{none/global//global/global}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Trial space of tuning $(\alpha , \Delta T)$}, shown as a countor plot. Here, black circle corresponds to $(\alpha =0.3,\Delta T = 100)$, while black triangle corresponds to the optimal hyper-parameter pair found. We plot the convex hull of the trial space, so in a few cases the reference point lies on the border of this space.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:alpha-deltaT-contour}{{6}{15}{\textbf {Trial space of tuning $(\alpha , \Delta T)$}, shown as a countor plot. Here, black circle corresponds to $(\alpha =0.3,\Delta T = 100)$, while black triangle corresponds to the optimal hyper-parameter pair found. We plot the convex hull of the trial space, so in a few cases the reference point lies on the border of this space.\relax }{figure.caption.10}{}}
\gdef \@abspage@last{15}

\subsection{Hyperparameter Tuning}\label{hyperparameter-tuning}

\input{../openreview/tables/alpha_deltaT.tex}

\subsubsection{$(\alpha, \Delta T)$ vs Sparsities} To understand the impact of the two additional hyperparameters included in \textit{RigL}, we use a Tree of Parzen Estimator (TPE sampler, \citet{TPE_Bergstra}) via Optuna to tune $(\alpha, \Delta T)$. We do this for sparsities $(1 - s) \in \{0.1,0.2,0.5\}$, and a fixed learning rate of $0.1$. Additionally, we set the sampling domain for $\alpha$ and $\Delta T$ as $[0.1,0.6]$ and $\{50,100, 150,...,1000\}$ respectively. We use 15 trials for each sparsity value, with our objective function as the validation accuracy averaged across 3 random seeds.\\

\input{../openreview/figs/lr_sweep.tex}

Table \ref{tab:effect-alpha-deltaT} shows the test accuracies of tuned hyperparameters. While the reference hyperparameters (original authors, $\alpha=0.3, \Delta T=100$) differ from the obtained optimal hyperparameters, the difference in performance is marginal, especially for ERK initialization. This in agreement with the original paper, which finds $\alpha \in \{0.3, 0.5\}, \Delta T = 100$ to be suitable choices. We include contour plots detailing the hyperparameter trial space in the supplementary material.

\subsubsection{Learning Rate vs Sparsities} We further examine if the final performance improves by tuning the learning rate ($\eta$) individually for each sparsity-initialization pair. We employ a grid search over $\eta \in $ \{$0.1,0.05,0.01,0.005$\} and $(\alpha, \Delta T) \in$ \{$(0.3, 100)$, $(0.4,200)$, $(0.4, 500)$, $(0.5, 750)$\}. As seen in Figure \ref{fig:lr-sweep}, $\eta = 0.1$ and $\eta = 0.05$ are close to optimal values for a wide range of sparsities and initializations. Since these learning rates also correspond to good choices for the Dense baseline, one can employ similar values when training with \textit{RigL}.


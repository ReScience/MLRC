\section{Discussion}

Evaluated on image classification, the central claims of \citet{rigl} hold true---\textit{RigL} outperforms existing sparse-to-sparse training methods and can also surpass other dense-to-sparse training methods with extended training. \textit{RigL} is fairly robust to its choice of hyperparameters, as they can be set independent of sparsity or initialization. We find that the choice of initialization has a greater impact on the final performance and compute requirement than the method itself. Considering the performance boost obtained by redistribution, proposing distributions that attain maximum performance given a FLOP budget could be an interesting future direction.\\

For computational reasons, our scope is restricted to small datasets such as CIFAR-10/100. \textit{RigL}'s applicability outside image classification---in Computer Vision and beyond (machine translation etc.) is not covered here.

\subsubsection{What was easy}
The authors' code covered most of the experiments in their paper and helped us validate the correctness of our replicated codebase. Additionally, the original paper is quite complete, straightforward to follow, and lacked any major errors.

\subsubsection{What was difficult}

Implementation details such as whether momentum buffers were accumulated sparsely or densely had a substantial impact on the performance of SNFS. Finding the right $\epsilon$ for ERK initialization required handling of edge cases---when a layer's capacity is exceeded. Hyperparameter tuning $(\alpha, \Delta T)$ involved multiple seeds and was compute-intensive.

\subsubsection{Communication with original authors}

We acknowledge and thank the original authors for their responsive communication, which helped clarify a great deal of implementation and evaluation specifics. Particularly, FLOP counting for various methods while taking into account the changing sparsity distribution. We also discussed experiments extending the original paper---as to whether the authors had carried out a similar study before.
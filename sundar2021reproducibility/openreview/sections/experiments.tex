\section{Experimental Settings}

\subsection{Model descriptions}

For experiments on CIFAR-10 (\citet{Krizhevsky09learningmultiple}), we use a Wide Residual Network (\citet{Wide_ResNet_BMVC2016_87}) with depth 22 and width multiplier 2, abbreviated as WRN-22-2. For experiments on CIFAR-100 (\citet{Krizhevsky09learningmultiple}), we use a modified variant of ResNet-50 (\citet{He_2016_CVPR}), with the initial $7\times 7$ convolution replaced by two $3 \times 3$ convolutions (architecture details provided in the supplementary material). 

\subsection{Datasets and Training descriptions}

We conduct our experiments on the CIFAR-10 and CIFAR-100 image classification datasets. For CIFAR-10, we use a train/val/test split of 45k/5k/10k samples. In comparison, the authors use no dedicated validation set, with 50k samples and 10k samples comprising the train set and test set, respectively. This causes a slight performance discrepancy between our reproduction and the metrics reported by the authors (dense baseline has a test accuracy of 93.4\% vs 94.1\% reported). However, our replication matches the paper's performance when 50k samples are used for the train set (Table \ref{tab:replication_verify}). We use a validation split of 10k samples for CIFAR-100 as well.\\

\input{../openreview/tables/replication_verify.tex}

On both datasets, we train models for 250 epochs each, optimized by SGD with momentum.  Our training pipeline uses standard data augmentation, which includes random flips and crops. When training on CIFAR-100, we additionally include a learning rate warmup for 2 epochs and label smoothening of 0.1 (\citet{goyal2017accurate}). We also initialize the last batch normalization layer (\citet{ioffe2015batch}) in each BottleNeck block to 0, following \citet{He_2019_CVPR}.

\subsection{Hyperparameters}

\textit{RigL} includes two additional hyperparameters ($\alpha, \Delta T$) in comparison to regular dense network training. In Sections \ref{cifar-10-results} and \ref{cifar-100-results}, we set $\alpha=0.3, \Delta T = 100$, based on the original paper. Optimizer specific hyperparameters---learning rate, learning rate schedule, and momentum---are also set according to the original paper. In Section \ref{hyperparameter-tuning}, we tune these hyperparameters with Optuna (\citet{optuna_2019}). We also examine whether indivdually tuning the learning rate for each sparsity value offers any significant benefit.

\subsection{Baseline implementations}

We compare \textit{RigL} against various baselines in our experiments: SET (\citet{Mocanu2018SET}), SNFS (\citet{dettmers2020sparse}), and Magnitude-based Iterative-pruning (\citet{to_prune_or_not}). We also compare against two weaker baselines, viz., \textit{Static Sparse} training and \textit{Small-Dense} networks. The latter has the same structure as the dense model but uses fewer channels in convolutional layers to lower parameter count. We implement iterative pruning with the pruning interval kept same as the masking interval for a fair comparison. 

\subsection{Computational requirements}

We run our experiments on a SLURM cluster node---equipped with 4 NVIDIA GTX1080 GPUs and a 32 core Intel CPU. Each experiment on CIFAR-10 and CIFAR-100 consumes about 1.6 GB and 7 GB of VRAM respectively and is run for 3 random seeds to capture performance variance. We require about 6 and 8 days of total compute time to produce all results, including hyper-parameter sweeps and extended experiments, on CIFAR-10 and CIFAR-100 respectively.
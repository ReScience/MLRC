\subsection{WideResNet-22 on CIFAR-10}\label{cifar-10-results}

\input{../openreview/tables/cifar10.tex}
   
Results on the CIFAR-10 dataset are provided in Table \ref{tab:cifar10-main-results}. Tabulated metrics are averaged across 3 random seeds and reported with their standard deviation. All sparse networks use random initialization, unless indicated otherwise.\\

While SET improves over the performance of static sparse networks and small-dense networks, methods utilizing gradient information (SNFS, \textit{RigL}) obtain better test accuracies. SNFS can outperform \textit{RigL}, but requires a much larger training budget, since it (a) requires dense gradients at each training step, (b) redistributes layer-wise sparsity during mask updates. For all sparse methods, excluding SNFS, using ERK initialization improves performance, but with increased FLOP consumption. We calculate theoretical FLOP requirements in a manner similar to \citet{rigl} (exact procedure is described in the appendix). \\

Figure \ref{fig:cifar10-main-results} contains test accuracies of select methods across two additional sparsity values: ($0.5, 0.95$). At lower sparsities (higher densities), \textit{RigL} matches the performance of the dense baseline. Performance further improves by training for longer durations. Particularly, training \textit{RigL} (ERK) twice as long at 90\% sparsity exceeds the performance of iterative pruning while requiring similar theoretical FLOPs. This validates the original authors' claim that \textit{RigL} (a sparse-to-sparse training method) outperforms pruning (a dense-to-sparse training method). 

\input{../openreview/figs/cifar10_main.tex}
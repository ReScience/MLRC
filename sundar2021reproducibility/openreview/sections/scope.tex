\section{Scope of reproducibility}
\label{sec:claims}

In order to verify the central claims presented in the paper we focus on the following target questions:

\begin{itemize}
    \item Does \textit{RigL} outperform existing sparse-to-sparse training techniques---such as SET (\citet{Mocanu2018SET}) and SNFS (\citet{dettmers2020sparse})---and match the accuracy of dense-to-sparse training methods such as iterative pruning (\citet{to_prune_or_not})?
    
    \item \textit{RigL} requires two additional hyperparameters to tune. We investigate the sensitivity of final performance to these hyperparameters across a variety of target sparsities (Section \ref{hyperparameter-tuning}).
    
    \item How does the choice of sparsity initialization affect the final performance for a fixed parameter count and a fixed training budget (Section \ref{effect-sparsity-distribution})?
    
    \item Does redistributing layer-wise sparsity during connection updates (\citet{dettmers2020sparse}) improve \textit{RigL}'s performance? Can the final layer-wise distribution serve as a good sparsity initialization scheme (Section \ref{effect-redistribution})? 

\end{itemize}
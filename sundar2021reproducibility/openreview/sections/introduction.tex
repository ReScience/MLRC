\section{Introduction}

\jdcomment{A  few  sentences  placing  the  work  in  context. Limit it to a few paragraphs at most; your report is on reproducing a piece of work, you donâ€™t have to motivate that work.}

Sparse neural networks are a promising alternative to conventional dense networks---having comparatively greater parameter efficiency and lesser floating-point operations (FLOPs) (\citet{han2016eie,ashbyexploiting,Srinivas_2017_CVPR_Workshops}). Unfortunately, present techniques to produce sparse networks of commensurate accuracy involve multiple cycles of training dense networks and subsequent pruning. Consequently, such techniques offer no advantage over training dense networks, either computationally or memory-wise. \\

In the paper \citet{rigl}, the authors propose \textit{RigL}, an algorithm for training sparse networks from scratch. The proposed method outperforms both prior art in training sparse networks, as well as existing dense-to-sparse training algorithms. By utilising dense gradients only during connectivity updates and avoiding any global sparsity redistribution, \textit{RigL} can maintain a fixed computational cost and parameter count throughout training.\\

As a part of the ML Reproducibility Challenge, we replicate \textit{RigL} from scratch and investigate if dynamic-sparse training confers significant practical benefits compared to existing sparsifying techniques.
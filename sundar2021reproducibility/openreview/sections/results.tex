\section{Results}
\label{sec:results}

Given a fixed training FLOP budget, \textit{RigL} surpasses existing dynamic sparse training methods over a range of target sparsities, on both CIFAR-10 and 100 (Sections \ref{cifar-10-results}, \ref{cifar-100-results}). By training longer, \textit{RigL} matches or marginally outperforms iterative pruning. However, unlike pruning, its FLOP consumption is constant throughout. This a prime reason for using sparse networks, and makes training larger networks feasible. Finally, as evaluated on CIFAR-10, the original authors' choice of hyper-parameters are close to optimal for multiple target sparsities and initialization schemes (Section \ref{hyperparameter-tuning}).

\jdcomment{Start with a high-level overview of your results. \sout{Does your work support the claims you listed in section 2?} Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next "Discussion" section. 

\textbf{Results reproducing original paper}
For each experiment, say 1) which claim in Section~\ref{sec:claims} it supports, and 2) if it successfully reproduced the associated experiment in the original paper. \sout{how it relates to one of the claims and explain what your result is.} 
For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.
Logically group related results into sections.} 

\input{../openreview/sections/results_cifar10.tex}
\input{../openreview/sections/results_cifar100.tex}
\input{../openreview/sections/hyperparameter_tuning.tex}


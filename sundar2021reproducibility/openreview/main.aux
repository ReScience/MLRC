\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Scope of reproducibility}{3}{section.2}\protected@file@percent }
\newlabel{sec:claims}{{2}{3}{Scope of reproducibility}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.1}Mask Initialization}{3}{subsubsection.3.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.2}Mask Updates}{4}{subsubsection.3.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.3}Pruning Strategy}{4}{subsubsection.3.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.4}Growth Strategy}{4}{subsubsection.3.0.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Test accuracy of reference and our implementations on CIFAR-10,} tabulated for three different sparsities. Note that the runs listed here do not use a separate validation set while training.\relax }}{5}{table.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:replication_verify}{{1}{5}{\textbf {Test accuracy of reference and our implementations on CIFAR-10,} tabulated for three different sparsities. Note that the runs listed here do not use a separate validation set while training.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Settings}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model descriptions}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Datasets and Training descriptions}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Hyperparameters}{5}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Baseline implementations}{6}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Computational requirements}{6}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{6}{section.5}\protected@file@percent }
\newlabel{sec:results}{{5}{6}{Results}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}WideResNet-22 on CIFAR-10}{6}{subsection.5.1}\protected@file@percent }
\newlabel{cifar-10-results}{{5.1}{6}{WideResNet-22 on CIFAR-10}{subsection.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {WideResNet-22-2 on CIFAR10}, tabulated for two density $(1-s)$ values. We group methods by their FLOP requirement and in each group, we mark the best accuracy in bold. Similar to {rigl}, we assume that algorithms utilize sparsity during training. All results are obtained by methods implemented in our unified codebase.\relax }}{7}{table.caption.9}\protected@file@percent }
\newlabel{tab:cifar10-main-results}{{2}{7}{\textbf {WideResNet-22-2 on CIFAR10}, tabulated for two density $(1-s)$ values. We group methods by their FLOP requirement and in each group, we mark the best accuracy in bold. Similar to \citet {rigl}, we assume that algorithms utilize sparsity during training. All results are obtained by methods implemented in our unified codebase.\relax }{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Test Accuracy vs Sparsity on CIFAR-10,} plotted for Random initialization \textbf  {(left)}, ERK initialization \textbf  {(center)}, and for training $2\times $ longer \textbf  {(right)}. Owing to random growth, SET can be unstable when training for longer durations with higher sparsities. Overall, \textit  {RigL}\textsubscript  {$2 \times $} (ERK) achieves highest test accuracy.\relax }}{8}{figure.caption.10}\protected@file@percent }
\newlabel{fig:cifar10-main-results}{{1}{8}{\textbf {Test Accuracy vs Sparsity on CIFAR-10,} plotted for Random initialization \textbf {(left)}, ERK initialization \textbf {(center)}, and for training $2\times $ longer \textbf {(right)}. Owing to random growth, SET can be unstable when training for longer durations with higher sparsities. Overall, \textit {RigL}\textsubscript {$2 \times $} (ERK) achieves highest test accuracy.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}ResNet-50 on CIFAR100}{8}{subsection.5.2}\protected@file@percent }
\newlabel{cifar-100-results}{{5.2}{8}{ResNet-50 on CIFAR100}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces entry for figure}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:cifar100-main-results}{{2}{8}{entry for figure}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Benchmarking sparse ResNet-50s on CIFAR-100,} tabulated by performance and cost \textbf  {(below)}, and plotted across densities \textbf  {(right)}. In each group below, \textit  {RigL} outperforms or matches existing sparse-to-sparse and dense-to-sparse methods. Notably, \textit  {RigL}\textsubscript  {$3\times $} at 90\% sparsity and \textit  {RigL}\textsubscript  {$2\times $} at 80\% sparsity surpass iterative pruning with similar FLOP consumption. \textit  {RigL}\textsubscript  {$2\times $} (ERK) further improves performance but requires a larger training budget. \relax }}{8}{table.3}\protected@file@percent }
\newlabel{tab:cifar100-main-results}{{3}{8}{\textbf {Benchmarking sparse ResNet-50s on CIFAR-100,} tabulated by performance and cost \textbf {(below)}, and plotted across densities \textbf {(right)}. In each group below, \textit {RigL} outperforms or matches existing sparse-to-sparse and dense-to-sparse methods. Notably, \textit {RigL}\textsubscript {$3\times $} at 90\% sparsity and \textit {RigL}\textsubscript {$2\times $} at 80\% sparsity surpass iterative pruning with similar FLOP consumption. \textit {RigL}\textsubscript {$2\times $} (ERK) further improves performance but requires a larger training budget. \relax }{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hyperparameter Tuning}{9}{subsection.5.3}\protected@file@percent }
\newlabel{hyperparameter-tuning}{{5.3}{9}{Hyperparameter Tuning}{subsection.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }}{9}{table.caption.11}\protected@file@percent }
\newlabel{tab:effect-alpha-deltaT}{{4}{9}{\textbf {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }{table.caption.11}{}}
\newlabel{tab:replication_verify}{{4}{9}{\textbf {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}$(\alpha , \Delta T)$ vs Sparsities}{9}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Learning Rate vs Sparsities}{9}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results beyond Original Paper}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sparsity Distribution vs FLOP Consumption}{10}{subsection.6.1}\protected@file@percent }
\newlabel{effect-sparsity-distribution}{{6.1}{10}{Sparsity Distribution vs FLOP Consumption}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Effect of Redistribution}{10}{subsection.6.2}\protected@file@percent }
\newlabel{effect-redistribution}{{6.2}{10}{Effect of Redistribution}{subsection.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Learning Rate vs Sparsity on CIFAR-10.} Runs using a learning rate $> 0.1$ do not converge and are not plotted here. There is little benefit in tuning the learning rate for each sparsity, and $0.1, 0.05$ are good choices overall.\relax }}{10}{figure.caption.12}\protected@file@percent }
\newlabel{fig:lr-sweep}{{3}{10}{\textbf {Learning Rate vs Sparsity on CIFAR-10.} Runs using a learning rate $> 0.1$ do not converge and are not plotted here. There is little benefit in tuning the learning rate for each sparsity, and $0.1, 0.05$ are good choices overall.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Test Accuracy vs FLOP consumption of WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100,} compared for Random and ERK initializations. For the same FLOP budget, models trained with ERK initialization must be more sparse, resulting in inferior performance.\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:erk-vs-random-FLOPs}{{4}{11}{\textbf {Test Accuracy vs FLOP consumption of WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100,} compared for Random and ERK initializations. For the same FLOP budget, models trained with ERK initialization must be more sparse, resulting in inferior performance.\relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Effect of redistribution during \textit  {RigL} updates, evaluated on CIFAR10 and CIFAR100}. By utilising sparse gradient or sparse momentum based redistribution, \textit  {RigL} (Random) matches \textit  {RigL} (ERK)'s performance. Among Random and ERK initialized experiments, we mark the best metrics under each sparsity and dataset in bold. \relax }}{11}{table.caption.14}\protected@file@percent }
\newlabel{tab:effect-redistribution}{{5}{11}{\textbf {Effect of redistribution during \textit {RigL} updates, evaluated on CIFAR10 and CIFAR100}. By utilising sparse gradient or sparse momentum based redistribution, \textit {RigL} (Random) matches \textit {RigL} (ERK)'s performance. Among Random and ERK initialized experiments, we mark the best metrics under each sparsity and dataset in bold. \relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Effect of redistribution on \textit  {RigL}'s performance,} evaluated using WideResNet-22-2 on CIFAR10 at 80\% sparsity. \textbf  {(left)} FLOPs required per forward pass, shown relative to the dense baseline, rises quickly and saturates within a few epochs (\textasciitilde 10k steps) for both sparse gradient and sparse momentum based redistribution. \textbf  {(right)} Comparison of the final density distribution against Random and ERK counterparts. ``b'' refers to block and ``l'' layer here.\relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig:density-dist-evolution}{{5}{12}{\textbf {Effect of redistribution on \textit {RigL}'s performance,} evaluated using WideResNet-22-2 on CIFAR10 at 80\% sparsity. \textbf {(left)} FLOPs required per forward pass, shown relative to the dense baseline, rises quickly and saturates within a few epochs (\textasciitilde 10k steps) for both sparse gradient and sparse momentum based redistribution. \textbf {(right)} Comparison of the final density distribution against Random and ERK counterparts. ``b'' refers to block and ``l'' layer here.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{12}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.1}What was easy}{12}{subsubsection.7.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.2}What was difficult}{12}{subsubsection.7.0.2}\protected@file@percent }
\bibdata{ref}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.0.3}Communication with original authors}{13}{subsubsection.7.0.3}\protected@file@percent }
\gdef \@abspage@last{13}

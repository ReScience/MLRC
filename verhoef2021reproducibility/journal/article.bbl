% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{jain-wallace-2019-attention}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=473b58dab2062f9cc6cabc0d011e1e2a}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Sarthak},
           giveni={S\bibinitperiod}}}%
        {{hash=dcf3fece96fc55df5a003eccf64d2961}{%
           family={Wallace},
           familyi={W\bibinitperiod},
           given={Byron\bibnamedelima C.},
           giveni={B\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Minneapolis, Minnesota}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{1d828c7a7e523964766c41ae09cfc534}
      \strng{fullhash}{1d828c7a7e523964766c41ae09cfc534}
      \strng{bibnamehash}{1d828c7a7e523964766c41ae09cfc534}
      \strng{authorbibnamehash}{1d828c7a7e523964766c41ae09cfc534}
      \strng{authornamehash}{1d828c7a7e523964766c41ae09cfc534}
      \strng{authorfullhash}{1d828c7a7e523964766c41ae09cfc534}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.}
      \field{booktitle}{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}
      \field{month}{6}
      \field{title}{{A}ttention is not {E}xplanation}
      \field{year}{2019}
      \field{pages}{3543\bibrangedash 3556}
      \range{pages}{14}
      \verb{doi}
      \verb 10.18653/v1/N19-1357
      \endverb
      \verb{urlraw}
      \verb https://www.aclweb.org/anthology/N19-1357
      \endverb
      \verb{url}
      \verb https://www.aclweb.org/anthology/N19-1357
      \endverb
    \endentry
    \entry{mohankumar_towards_2020}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=ab5cedcb79c48223368c7935464cd96e}{%
           family={Mohankumar},
           familyi={M\bibinitperiod},
           given={Akash\bibnamedelima Kumar},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=47aa43bfede7156ec6f73333a45633a7}{%
           family={Nema},
           familyi={N\bibinitperiod},
           given={Preksha},
           giveni={P\bibinitperiod}}}%
        {{hash=c93f348c15c27503d83ebe87cc0688df}{%
           family={Narasimhan},
           familyi={N\bibinitperiod},
           given={Sharan},
           giveni={S\bibinitperiod}}}%
        {{hash=620c69e905e599a451eba3f56f8e43f0}{%
           family={Khapra},
           familyi={K\bibinitperiod},
           given={Mitesh\bibnamedelima M.},
           giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=419bcd94d9bbad163d1da02c0145d407}{%
           family={Srinivasan},
           familyi={S\bibinitperiod},
           given={Balaji\bibnamedelima Vasan},
           giveni={B\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=c8addc8f1d3acf0213ef98aadb8f59a6}{%
           family={Ravindran},
           familyi={R\bibinitperiod},
           given={Balaraman},
           giveni={B\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{0abdfff2365c52ea91fb53af2e4ada54}
      \strng{fullhash}{6a0b1f06c3d21f399d7001d838cb4c03}
      \strng{bibnamehash}{6a0b1f06c3d21f399d7001d838cb4c03}
      \strng{authorbibnamehash}{6a0b1f06c3d21f399d7001d838cb4c03}
      \strng{authornamehash}{0abdfff2365c52ea91fb53af2e4ada54}
      \strng{authorfullhash}{6a0b1f06c3d21f399d7001d838cb4c03}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model{'}s predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model{'}s prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model{'}s predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model{'}s predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model{'}s predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model{'}s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model{'}s predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model{'}s predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention}
      \field{booktitle}{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}
      \field{month}{7}
      \field{title}{Towards Transparent and Explainable Attention Models}
      \field{year}{2020}
      \field{pages}{4206\bibrangedash 4216}
      \range{pages}{11}
      \verb{doi}
      \verb 10.18653/v1/2020.acl-main.387
      \endverb
      \verb{urlraw}
      \verb https://www.aclweb.org/anthology/2020.acl-main.387
      \endverb
      \verb{url}
      \verb https://www.aclweb.org/anthology/2020.acl-main.387
      \endverb
    \endentry
    \entry{wiegreffe-pinter-2019-attention}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=cd2bb336655f68d528050759802b83f4}{%
           family={Wiegreffe},
           familyi={W\bibinitperiod},
           given={Sarah},
           giveni={S\bibinitperiod}}}%
        {{hash=4d9c3b5b75abff3e8df33bd05ed75d32}{%
           family={Pinter},
           familyi={P\bibinitperiod},
           given={Yuval},
           giveni={Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Hong Kong, China}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{a347527a94a18d2138eb13e831d544ac}
      \strng{fullhash}{a347527a94a18d2138eb13e831d544ac}
      \strng{bibnamehash}{a347527a94a18d2138eb13e831d544ac}
      \strng{authorbibnamehash}{a347527a94a18d2138eb13e831d544ac}
      \strng{authornamehash}{a347527a94a18d2138eb13e831d544ac}
      \strng{authorfullhash}{a347527a94a18d2138eb13e831d544ac}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model{'}s prediction, and consequently reach insights regarding the model{'}s decision-making process. A recent paper claims that {`}Attention is not Explanation{'} (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one{'}s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don{'}t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.}
      \field{booktitle}{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}
      \field{month}{11}
      \field{title}{Attention is not not Explanation}
      \field{year}{2019}
      \field{pages}{11\bibrangedash 20}
      \range{pages}{10}
      \verb{doi}
      \verb 10.18653/v1/D19-1002
      \endverb
      \verb{urlraw}
      \verb https://www.aclweb.org/anthology/D19-1002
      \endverb
      \verb{url}
      \verb https://www.aclweb.org/anthology/D19-1002
      \endverb
    \endentry
    \entry{serrano-smith-2019-attention}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=19d581e0676c92cae5bb51e75fa36170}{%
           family={Serrano},
           familyi={S\bibinitperiod},
           given={Sofia},
           giveni={S\bibinitperiod}}}%
        {{hash=76caee508bed7d0995f7c21cc5e6208c}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Noah\bibnamedelima A.},
           giveni={N\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Florence, Italy}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{af4e0c38344e286756e589701c39dd6e}
      \strng{fullhash}{af4e0c38344e286756e589701c39dd6e}
      \strng{bibnamehash}{af4e0c38344e286756e589701c39dd6e}
      \strng{authorbibnamehash}{af4e0c38344e286756e589701c39dd6e}
      \strng{authornamehash}{af4e0c38344e286756e589701c39dd6e}
      \strng{authorfullhash}{af4e0c38344e286756e589701c39dd6e}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components{'} representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components{'} overall importance to a model, it is by no means a fail-safe indicator.}
      \field{booktitle}{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}
      \field{month}{7}
      \field{title}{Is Attention Interpretable?}
      \field{year}{2019}
      \field{pages}{2931\bibrangedash 2951}
      \range{pages}{21}
      \verb{doi}
      \verb 10.18653/v1/P19-1282
      \endverb
      \verb{urlraw}
      \verb https://www.aclweb.org/anthology/P19-1282
      \endverb
      \verb{url}
      \verb https://www.aclweb.org/anthology/P19-1282
      \endverb
    \endentry
    \entry{ribeiro2016should}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=9203dc724f298030e81ba470feb309af}{%
           family={Ribeiro},
           familyi={R\bibinitperiod},
           given={Marco\bibnamedelima Tulio},
           giveni={M\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=0b9694812f0dc77974d99bb875a76d48}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Sameer},
           giveni={S\bibinitperiod}}}%
        {{hash=c9ab8ac486d7f85ac914b1a8e51e512b}{%
           family={Guestrin},
           familyi={G\bibinitperiod},
           given={Carlos},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{fullhash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{bibnamehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{authorbibnamehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{authornamehash}{70906198418f30a779bc9bda0f6ae1f1}
      \strng{authorfullhash}{70906198418f30a779bc9bda0f6ae1f1}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining}
      \field{title}{" Why should I trust you?" Explaining the predictions of any classifier}
      \field{year}{2016}
      \field{pages}{1135\bibrangedash 1144}
      \range{pages}{10}
    \endentry
    \entry{zhou2018nlp}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=29f8ef062b0e39d035c23b4debe7df68}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Qimin},
           giveni={Q\bibinitperiod}}}%
        {{hash=d0d5a12b1f5bae45ea7780158b3b35c8}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{20ebd039d08b5119e175bba4fd602b6c}
      \strng{fullhash}{20ebd039d08b5119e175bba4fd602b6c}
      \strng{bibnamehash}{20ebd039d08b5119e175bba4fd602b6c}
      \strng{authorbibnamehash}{20ebd039d08b5119e175bba4fd602b6c}
      \strng{authornamehash}{20ebd039d08b5119e175bba4fd602b6c}
      \strng{authorfullhash}{20ebd039d08b5119e175bba4fd602b6c}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis}
      \field{title}{NLP at IEST 2018: BiLSTM-attention and LSTM-attention via soft voting in emotion classification}
      \field{year}{2018}
      \field{pages}{189\bibrangedash 194}
      \range{pages}{6}
    \endentry
    \entry{lee2019ncuee}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=faaac289477c521b5c1355bbf30f5d0e}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Lung-Hao},
           giveni={L\bibinithyphendelim H\bibinitperiod}}}%
        {{hash=82ecbae219cd3e89336a57aaa4226294}{%
           family={Lu},
           familyi={L\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=8505dc4318d072d64bc3943e98c60462}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Po-Han},
           giveni={P\bibinithyphendelim H\bibinitperiod}}}%
        {{hash=ad24974b5cbba2c0257df99d54a3a218}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Po-Lei},
           giveni={P\bibinithyphendelim L\bibinitperiod}}}%
        {{hash=c6fdf41376993e0606e9d8dcd452f0b7}{%
           family={Shyu},
           familyi={S\bibinitperiod},
           given={Kuo-Kai},
           giveni={K\bibinithyphendelim K\bibinitperiod}}}%
      }
      \strng{namehash}{11d8712c8cdd026512201d12b13d3e2c}
      \strng{fullhash}{c644a7160d6a6116441d914466a81d28}
      \strng{bibnamehash}{c644a7160d6a6116441d914466a81d28}
      \strng{authorbibnamehash}{c644a7160d6a6116441d914466a81d28}
      \strng{authornamehash}{11d8712c8cdd026512201d12b13d3e2c}
      \strng{authorfullhash}{c644a7160d6a6116441d914466a81d28}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 18th BioNLP Workshop and Shared Task}
      \field{title}{NCUEE at MEDIQA 2019: medical text inference using ensemble BERT-BiLSTM-Attention model}
      \field{year}{2019}
      \field{pages}{528\bibrangedash 532}
      \range{pages}{5}
    \endentry
    \entry{8554396}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=dd8a23ccfbc8da1f228d360d71045012}{%
           family={{Aziz Sharfuddin}},
           familyi={A\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
        {{hash=7dff02eef721d60ea7ecfec2e56a67c1}{%
           family={{Nafis Tihami}},
           familyi={N\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=9fbfecbb985cb0fe973c47eed34ae4f2}{%
           family={{Saiful Islam}},
           familyi={S\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{2cadf7578e966b56440dfa8d4ded2563}
      \strng{fullhash}{2cadf7578e966b56440dfa8d4ded2563}
      \strng{bibnamehash}{2cadf7578e966b56440dfa8d4ded2563}
      \strng{authorbibnamehash}{2cadf7578e966b56440dfa8d4ded2563}
      \strng{authornamehash}{2cadf7578e966b56440dfa8d4ded2563}
      \strng{authorfullhash}{2cadf7578e966b56440dfa8d4ded2563}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{2018 International Conference on Bangla Speech and Language Processing (ICBSLP)}
      \field{title}{A Deep Recurrent Neural Network with BiLSTM model for Sentiment Classification}
      \field{year}{2018}
      \field{pages}{1\bibrangedash 4}
      \range{pages}{4}
      \verb{doi}
      \verb 10.1109/ICBSLP.2018.8554396
      \endverb
    \endentry
    \entry{vaswani_attention_2017}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=7f28e84700536646dd6620a0db07ad09}{%
           family={Vaswani},
           familyi={V\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=62efade83d70f0323fe248755e6c90c5}{%
           family={Shazeer},
           familyi={S\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
        {{hash=06649ebab1ea5cac0250746a19764975}{%
           family={Parmar},
           familyi={P\bibinitperiod},
           given={Niki},
           giveni={N\bibinitperiod}}}%
        {{hash=831027ee0ebf22375e2a86afc1881909}{%
           family={Uszkoreit},
           familyi={U\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=2fd2982e30ebcec93ec1cf76e0d797fd}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Llion},
           giveni={L\bibinitperiod}}}%
        {{hash=27b07e4eacbf4ef7a1438e3badb7dd8d}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Aidan\bibnamedelima N.},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=540fcd72e1fa4bbed46604f4e6cff817}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Łukasz},
           giveni={Ł\bibinitperiod}}}%
        {{hash=95595a0fefb86187cbc36e551017d332}{%
           family={Polosukhin},
           familyi={P\bibinitperiod},
           given={Illia},
           giveni={I\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Red Hook, NY, USA}%
      }
      \list{publisher}{1}{%
        {Curran Associates Inc.}%
      }
      \strng{namehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{fullhash}{cb26e47f6b8133865271fc8483132297}
      \strng{bibnamehash}{cb26e47f6b8133865271fc8483132297}
      \strng{authorbibnamehash}{cb26e47f6b8133865271fc8483132297}
      \strng{authornamehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{authorfullhash}{cb26e47f6b8133865271fc8483132297}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.}
      \field{booktitle}{Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}}
      \field{isbn}{978-1-5108-6096-4}
      \field{month}{12}
      \field{series}{{NIPS}'17}
      \field{title}{Attention is all you need}
      \field{year}{2017}
      \field{pages}{6000\bibrangedash 6010}
      \range{pages}{11}
    \endentry
    \entry{abnar_quantifying_2020}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=f975fd62621fc055339fdf5f3f64d41d}{%
           family={Abnar},
           familyi={A\bibinitperiod},
           given={Samira},
           giveni={S\bibinitperiod}}}%
        {{hash=44ad51ff065eed44fec5244bbae59fd4}{%
           family={Zuidema},
           familyi={Z\bibinitperiod},
           given={Willem},
           giveni={W\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{0c514d65ea282376df5c6895d9467080}
      \strng{fullhash}{0c514d65ea282376df5c6895d9467080}
      \strng{bibnamehash}{0c514d65ea282376df5c6895d9467080}
      \strng{authorbibnamehash}{0c514d65ea282376df5c6895d9467080}
      \strng{authornamehash}{0c514d65ea282376df5c6895d9467080}
      \strng{authorfullhash}{0c514d65ea282376df5c6895d9467080}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.}
      \field{booktitle}{Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}}
      \field{month}{7}
      \field{title}{Quantifying {Attention} {Flow} in {Transformers}}
      \field{year}{2020}
      \field{pages}{4190\bibrangedash 4197}
      \range{pages}{8}
      \verb{doi}
      \verb 10.18653/v1/2020.acl-main.385
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/FrankVerhoef/Zotero/storage/WI8TBE5Y/Abnar en Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.aclweb.org/anthology/2020.acl-main.385
      \endverb
      \verb{url}
      \verb https://www.aclweb.org/anthology/2020.acl-main.385
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput


% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/MotherOfUnicorns/FACT_AI_project}
\def \codeDOI{}
\def \codeSWH{swh:1:dir:19a8073757cc142f8f50eb44014aca87cad0b8bf}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{}
\def \dateACCEPTED{}
\def \datePUBLISHED{}
\def \articleTITLE{[Re] Reproducibility study - Does enforcing diversity in hidden states of LSTM-Attention models improve transparency?}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2020, Natural Language Processing}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2021}
\def \reviewURL{https://openreview.net/forum?id=lE0wqKGROKa}
\def \articleABSTRACT{It has been shown that the weights in attention mechanisms do not necessarily offer a faithful explanation of the model's predictions. In the paper Towards Transparent and Explainable Attention Models the authors propose two methods to enhance faithfulness and plausibility of the explanations provided by an LSTM model combined with a basic attention mechanism. The authors claim that applying these methods, Orthogonalization and Diversity Driven Training, results in attention weights that offer more faithful and plausible explanations of the model's predictions, without sacrificing model performance. For this reproducibility study, we focus on the main claims made in this paper, following four investigative routes: Replication, Code review, Evaluation methodology, Generalization to other architectures. We confirm that Orthogonalization and Diversity Driven Training achieve similar accuracies as the Vanilla LSTM, while lowering conicity. However, we cannot reproduce the results of several of the experiments in the paper that underlie their claim of better transparency. In addition, a close inspection of the code base reveals some potentially problematic inconsistencies. Despite this, under certain conditions, we do confirm that Orthogonalization and Diversity Driven Training can be useful methods to increase transparency. How to formulate these conditions more generally remains unclear and deserves further research. One pattern that seems to emerge is that the benefits of Orthogonalization or Diversity Driven Training are most apparent for the relatively simpler tasks with a single input sequence, such as sentiment classification. The potential to improve faithfulness of the weights might be high in those cases as it not a given that attention weights carry any meaning for these tasks.}
\def \replicationCITE{A. K. Mohankumar, P. Nema, S. Narasimhan, M. M. Khapra, B. V. Srinivasan, and B. Ravindran. Towards Transparent and Explainable Attention Models. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics, July 2020, pp. 4206â€“4216. DOI: 10.18653/v1/2020.acl-main.387. URL: https://www.aclweb.org/anthology/2020.acl-main.387.}
\def \replicationBIB{mohankumar_towards_2020}
\def \replicationURL{https://www.aclweb.org/anthology/2020.acl-main.387}
\def \replicationDOI{10.18653/v1/2020.acl-main.387}
\def \contactNAME{Pieter Bouwman}
\def \contactEMAIL{pieterbouwman98@gmail.com}
\def \articleKEYWORDS{Attention, NLP, Transparency, Explainability, Faithfulness, Plausibility, Reproducibility, LSTM, ReScience}
\def \journalNAME{None}
\def \journalVOLUME{}
\def \journalISSUE{}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Pieter Bouwman et al.}
\def \authorsABBRV{P. Bouwman et al.}
\def \authorsSHORT{Bouwman et al.}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0000-0002-1697-792X}]{Pieter Bouwman}
\author[1,\orcid{0000-0002-2715-3168}]{Yun Li}
\author[1,\orcid{0000-0002-7791-2580}]{Rogier van der Weerd}
\author[1,\orcid{0000-0001-6235-7177}]{Frank Verhoef}
\affil[1]{University of Amsterdam, Amsterdam, The Netherlands}

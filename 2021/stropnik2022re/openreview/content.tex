\section{Reproducibility Summary}

\subsection*{Scope of Reproducibility}

The studied paper proposes a novel output layer for graph neural networks (the graph edit network - GEN). The objective of this reproduction is to assess the possibility of its re-implementation in the Python programming language and the adherence of the provided code to the methodology, described in the source material. Additionally, we rigorously evaluate the functions used to create the synthetic data sets, on which the models are evaluated. Finally, we also pay attention to the claim that the proposed architecture scales well to larger graphs.  


\subsection*{Methodology}


For most of our work, we were able to use the code, provided in the supplementary repository. We also offer our own variations of the experimental setup, with an alternative method of risk estimation. A portion of the report is also devoted to a more exhaustive description of the included data generating functions, otherwise not offered original paper.
s. 

\subsection*{Results}

We were able to reproduce GEN's out-performance of a chosen baseline and its perfect scores on synthetic data sets. We also confirm the author's claims of the sub-quadratic scaling of GEN's forward passes and deduce that they reported the scaling of back-passes too favourably. We conclude our work with scepticism of the chosen experiments' suitability to evaluate the model's performance and discuss our findings.


\subsection*{What was easy}



All the provided code has extensive documentation which made the paper's experiments easy to reproduce. The entire code base is readable, modular and adheres to established practices on code readability. The authors also provide some unit tests for all of their models and have pre-implemented several useful diagnostic measures.


\subsection{What was difficult}


Running some of the provided code on a consumer-grade laptop (as reported in the original work) was prohibitively expensive. The lack of transparency about the code base's runtimes made our work here much more difficult. Another time-consuming task was the debugging of a section of author-provided code. We've helped the authors identify the problem, which has now been resolved.

\subsection{Communication with original authors}

The authors were prompt with their responses, welcomed our efforts in reproducing their work and made themselves available for any questions. Upon our request, they happily provided additional implementations, not originally available in their repository, and offered their counter-arguments to some methodological concerns that we expressed to them.
\newpage
% \textit{\textbf{The following section formatting is \textbf{optional}, you can also define sections as you deem fit.
% \\
% Focus on what future researchers or practitioners would find useful for reproducing or building upon the paper you choose.}}
\subsection{Submission Checklist}

Double check the file \texttt{journal/metadata.yaml} to contain the following information:

\begin{itemize}
\item Title should start with "\texttt{[Re]}"
\item Author information, along with ORCID id
\item Author affiliations
\item Code URL, Software Heritage Foundation link
\item Abstract
\item Review URL (the OpenReview URL of your report)
\end{itemize}

\subsection{Continuous Integration}

We use Github Actions CI to check your submission and compile the pdf file subsequently.
You can also run the tests locally by running \texttt{python check\_yaml.py}, and then running \texttt{./build.sh} to compile Latex.

\clearpage

\section{Content}
\section{Introduction}


% A few sentences placing the work in high-level context. Limit it to a few paragraphs at most; your report is on reproducing a piece of work, you donâ€™t have to motivate that work.

The studied paper proposes a novel output layer for graph neural networks (GNNs), the graph edit network (GEN). This layer yields a sequence of graph edits $\delta$ .
Particularly, the graph edit schema considered in the work is the one initially proposed in \cite{sanfeliu1983distance}, describing notions of node insertions ($\texttt{ins}_{x}$), deletions ($\texttt{del}_{x}$) and replacements ($\texttt{repl}_{i,x}$), as well as edge insertions ($\texttt{eins}_{i,j}$) and deletions ($\texttt{edel}_{i,j}$). Note that the subscripts $x$ in node edits refer to the attributes of the edited node (in $\texttt{repl}_{i,x},$ the additional subscript $i$ denotes the to-be replaced attributes), and $i,j$ in the edge edits refer to the indices of nodes between which the edited edge can be found. 

These finite sequences of edits, also referred to as edit scripts $\bar{\delta}_{t}$ $= [\delta^{1}_{t}, \delta^{2}_{t}, \dots, \delta^{n}_{t}]$, are general enough to describe any graph-to-graph transformation and are not only very interpretable for humans, but also computationally efficient. Both of these properties establish GENs as a useful tool for work in the domain of graph time series prediction. More particularly, GENs perform time series prediction under the Markovian assumption, which states that knowing the graph $G_{t}$ and the mapping function $\psi_{t},$ derived from the edit script $\bar{\delta}_{t}$, is sufficient for predicting the graph found in the next step of the time series as 
\begin{equation*}
    G_{t+1} = \psi_{t}(G_{t}); \quad \psi_{t} := \delta^{1}_{t} \circ \delta^{2}_{t} \circ \dots \circ \delta^{n}_{t}; \quad \forall \delta^{i}_{t} \in \bar{\delta}_{t},
\end{equation*} where the subscript \textit{t} denotes the time-dependant index in the time series.
\section{Scope of Reproducibility}
\label{sec:claims}

The authors of the reproduced work formally prove theorems, stating that finding a mapping $\psi$ between pairs of time-adjacent graphs is sufficient for constructing training data for GENs. 
They propose that their GNN architecture be trained to reproduce specific teaching signals for this function $\psi$, which may be derived from any gathered training time series of graphs. 
This is done by first finding reference pair mappings $\psi_{t}: G_{t} \to \bar{\delta}_{t}(G_{t}) \equiv G_{t} \to G_{t+1}$ from the training series via graph edit distance approximators\footnote{Approximation is used due to the NP-hard nature of the graph edit distance in general, as shown in \cite{bougleux2017graph}. In practice, exploiting domain knowledge may also lead to sensible mappings $\psi_{t}$. As an example of domain knowledge exploitation, the authors cite \cite{zhang1989simple}.}, and then computing teaching signals via an algorithm, provided in the paper's supplementary material. 

The authors empirically underpin this corollary by showing that the GEN performs well in a series of graph time-series prediction tests. They define several data generating processes (DGPs), from which the GEN attempts to learn the user-defined functions $\psi,$ which remain hidden to the algorithm. The tests can be roughly split into three classes, which have corresponding experiments in section~\ref{sec:results}. The explicit conclusions of the experimental subsection of the original paper are that the GEN outperforms the selected baselines in all of the observed tasks. 

In our work, we compare the GEN to one of the baselines - the modified version of Variational graph autoencoders (VGAE). As in the original work, we observe a modification of the method, suggested by \cite{VGRNN}, where the method attempts to directly infer the the graph in the next step of the time series. In the other experiments, we interpret claims about GEN's performance on different datasets directly.

% The baseline observed for edge replacement tasks, on the other hand, is a Gaussian process prediction approach, introduced in  \cite{paassen2018time}.

Since the graphs, generated by the author-defined DGPs, are of a completely synthetic nature and very limited in scale, the authors also attempt to establish that GENs scale well to real-world networks. In their experimentation, they only pay attention to the scaling efficiency of the architecture and not to the quality of the predictions themselves. From the described conclusions, we identify the following claims, made in the experimental section of the paper, that we will be exploring:

\begin{enumerate}[label=\textbf{Claim} (\roman*):,leftmargin=*,align=left]
    \item GENs, trained with either hinge or crossentropy loss, outperform the modified VGAE on all three dynamical graph system DGPs.
    
    % (cyclic graphs, degree rules, game of life).
    \item GENs, trained with user defined losses, achieve a perfect accuracy score on both dynamical tree DGPs.
    
    % (Peano addition, boolean formulae).
    \item The runtime of forward passes of a GEN, trained on the social network dataset (with or without edge filtering), scales sub-quadratically as the number of nodes in a graph increases.
    \item The runtime of backward passes of a GEN, trained on the social network dataset  with edge filtering, scales approximately linearly, as the number of nodes in a graph increases.
\end{enumerate}

An additional contribution of our work is the thorough study and description of the synthetic datasets, used to evaluate the GENs performance. We pay special attention to this part of the paper, as they were not exhaustively described in the original work. This examination helps us shed light on the performance of the GEN in the discussion section and evaluate the suitability of the used exprimental approachs. It also provides a more in-depth descriptive resource to other researchers in the field, that might find these DGPs useful for their own work.

%\jdcomment{To organizers: I asked my students to connect the main claims and the experiments that supported them. For example, in this list above they could have ``Claim 1, which is supported by Experiment 1 in Figure 1.'' The benefit was that this caused the students to think about what their experiments were showing (as opposed to blindly rerunning each experiment and not considering how it fit into the overall story), but honestly it seemed hard for the students to understand what I was asking for.}

\section{Methodology}

Throughout our reproduction attempt, we have made great use of the code, provided in a supplementary repository to the original paper \cite{gitlab}. To replicate the author's experimental environment, we try to make the same assumptions and hyperparameter choices than those provided either in the original paper, or the documentation of the supplementary repository. A fork of this repository with our changes and additions is available at \cite{Git}.

% Explain your approach - did you use the author's code, or did you aim to re-implement the approach from the description in the paper? Summarize the resources (code, documentation, GPUs) that you used.

\subsection{Model descriptions}
\label{subsec:models}
In the first class of experiments, we train 2 GEN models, one using the adapted cross-entropy loss (GEN-XE) and the other using the adapted hinge loss (GEN), described in the paper. Both models are parametrized by their input, output and hidden dimensionalities, as well as their used nonlinearities. Given the short edit scripts expected in these scenarios, no edge filtering is used in these models.

We also train the Variational Graph autoencoder model, as described in \cite{vgae}. Apart from its input, output and hidden dimensionalities, it is also parametrized by the size of its encoding space, the regularization strength $\beta$ and a scaling factor for the noise on the last layer node features $\sigma$. It also takes a hyperparametric definition of the used nonlinearity. 

The GEN models used in the experiments, governed by the Peano addition and Boolean formulae DGPs, are similar to those in the \textit{Dynamical graph systems} class. The models here, however, use an author-defined loss function, with respect to a custom teaching protocol, with only a single predictive step between graphs. Similarly to before, no edge filtering is used.


In the experiments on the social network dataset, we train two variations of the GEN model. The first sets up two binary classifiers for each node to decide whether to consider changing outgoing/incoming edges or not. This approach is denoted in the results as \textit{flexible edge filtering}. The second model limits the number of permitted edge edits with a fixed upper bound - this is denoted as \textit{fixed edge filtering}. 
The models use a simplified single-step teaching protocol, over which its loss function is defined. In the protocol, all edits, except for node insertion, are processed as expected. For insertion, however, the protocol lets a given node $n$ insert a neighbor $n'$ when there is at least one edge \textit{(n, n')} found in $G_{t+1},$ where $n'$ is not a node found in $G_{t}.$ The authors acknowledge potential shortcomings of this method, but cite the desire of using a single-step protocol as the reason for choosing it.


%  If set to True, this class sets up two binary classifiers for each
%         node to decide whether to make any changes to outgoing/incoming edges
%         or not. This can speed up processing significantly for large graphs,
%         but is more challenging to learn. If set to an integer, the number of
%         nodes with edge changes is limited to that integer.


% Include a description of each model or algorithm used. Be sure to list the type of model, the number of parameters, and other relevant info (e.g. if it's pretrained). 
\begin{figure}
    \centering
    
    \includegraphics[width=0.8\linewidth]{cycles2.png}
    \caption{The three cyclical time series yielded by the Edit Cycles DGP.}
    \label{fig:Cyclical}
\end{figure}
\subsection{Data}
The paper contains three classes of experiments. The first two use user created DGPs, whereas the last one works with an external, well established social network. We describe the dataset and DGPs in accordance to the class of experiments they correspond to, in the following subsections.
\subsubsection{Dynamical graph systems}
\label{sec:DGS}
The \textit{Dynamical graph systems} class of DGPs governs the train and test set generation in Experiments \ref{exp:first}, \ref{exp:our1} and \ref{exp:our2}. The class contains three discrete processes, provided in the supplementary repository in the form of scripts for the python programming language. During training/testing time, the time series generator function is called, always returning a sequence of graphs based on DGP-specific function arguments.

The \textbf{Edit Cycles} DGP always yields one of three author specified cyclical time series, the outputs only differing in length and the starting time index. The edit script $\bar{\delta}_{t}$ between two graphs is always of cardinality $|\bar{\delta}| \leq 2$ and all possible generated graphs consist of between two and four nodes. The cyclical series that the DGP yields are visualized in Figure \ref{fig:Cyclical}.

The \textbf{Degree Rules} data generating function generates a series of a determined length using the edit rules, described in Algorithm \ref{alg:degree}. The generator function accepts parameters, corresponding to the series length and the number of nodes in the initial graph $G_{0}.$ $G_{0}$'s adjacency matrix is then randomly initialized. Consequentially, given a fixed time series length, the returned series is fully dependant on the random initialization of $G_{0},$ as the rules are deterministic. In the examples in section~\ref{subsec:original}, as per the author's source code, the randomization from NumPy's \texttt{random.rand} is used, and all series' initial graphs $G_{0}$ start with exactly 8 nodes. We comment on this choice of randomization and provide our alternative in Section \ref{subsec:our}.

The third and final DGP in this class is inspired by Conway's \textbf{Game of Life} \cite{GoL}. Similarly to \textit{Degree Rules}, it takes an input graph and applies a graph-to-graph mapping function. This one is specified by Algorithm \ref{alg:gol} and is used to create a time-series of a specified length. This function is also deterministic.
In the resulting graphs, the nodes considered \textit{alive} in the Game of Life rule set are denoted with the feature value $x_{n}=1.$ In contrast to degree rules, Game of Life graphs retain their number of nodes throughout evolution, as the graph will always denote the $D \times D$ grid with the neighborhood structure modeling a nodes' 8-neighborhood, and only the nodes' alive/dead state will change. In each time series, a number of random Game of Life oscilators (randomly chosen between 5 candidates) is chosen and made alive. Afterwards, each still dead cell will be made alive with a probability $\text{Pr}(\texttt{repl}_{0,1}(n)) = p$. In the experiments in section~\ref{subsec:original}, we report results using the parameters $p = 0.1, D=10,$ and always placing a single oscillator on the grid at initialization.

\subsubsection{Tree dynamical systems}
\label{subsec:trees}
The \textit{Tree dynamical systems} class of DGPs governs the train and test set generation in Experiments \ref{exp:second} and \ref{exp:our3}. It contains two distinct processes. They are distinguished from the DGP class in the previous section because they both generate strictly tree-structured graphs, with no loops. Furthermore, they both include more complex node attribute encodings in the form of one-hot vectors.

The initial graph in a series, generated by the \textbf{Boolean Formulae} generator function, corresponds to a random Boolean formula. The time series following such a $G_{0}$ represents gradual simplifications of the formula, ending with a logic graph that can not be simplified any longer. An example evolution is given in Figure \ref{fig:bool} for the formula  $(x \lor (y \land \lnot y)) \lor x$. The initial trees are generated via a stochastic regular tree grammar with a $\text{Pr}(\land) = \text{Pr}(\lor) = 0.3$ and $\text{Pr}(x) = \text{Pr}(\lnot x) = \text{Pr}(y) = \text{Pr}(\lnot y) = 0.1$. The generator functions also offer a hyperparametric maximal number of applied rules $p$, where the authors use $p=3$ in the original experiments.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{bool2.png}
    \caption{Example time series evolution of a graph, sampled from the Boolean Formulae DGP. The leftmost graph denotes the logical formula, $(x \lor (y \land \lnot y)) \lor x$, whereas each evolution corresponds to a logical simplification of the previous graph. }
    \label{fig:bool}
\end{figure}

\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
\centering
\caption{The $G_{t} \to G_{t+1}$ mapping for the Degree rules DGP. The function \texttt{shareN} returns true if the nodes share at least one neighbor.}\label{alg:degree}
\begin{algorithmic}[1]
\Require Graph $G_{t},$ containing nodes $n$.
\MyFor{$\text{component } C \in G_{t}$}
\MyFor{$ n \in C$}
\State $d \gets \texttt{degree}(n)$
\If{$d \geq 3$} \texttt{del}($n$)
\ElsIf{\text{ }$\exists n' \in C: \texttt{shareN}(n, n')$}
\MyFor{$n' \in C: \texttt{shareN}(n, n')$}
    \State{$\texttt{eins}(n, n')$}
\EndMyFor
\Else{ $\texttt{ins}_{1}(n^{*})$, \texttt{eins}$(n, n^{*})$}
%\EndFor
\EndIf
\EndMyFor
\EndMyFor
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{The $G_{t} \to G_{t+1}$ mapping for the Game of Life DGP. The \texttt{AliveDegree} function returns the number of neighboring nodes $n'$ with the attribute $x_{n'}=1$. }\label{alg:gol}
   \begin{algorithmic}[1]
\Require $\text{Graph } G_{t}, $ containing nodes $n$.
\MyFor{$n \in G_{t}$}
    \State $d \gets \texttt{AliveDegree}(n)$
    \If{$(x_{n}==1) \Andd (d < 2 \Or 4 \leq d)$}
        \State $\texttt{repl}_{1,0}(n)$
    \ElsIf{$(x_{n}==0)  \Andd (d==3)$}
        \State $\texttt{repl}_{0,1}(n)$
\EndIf
\EndMyFor
% \State $d \gets \text{degree}(n)$
% \If{$d \geq 3$}
%         \State \texttt{delete}($n$)
% \ElsIf{\text{ }$\exists n' \in C: \texttt{shareN}(n, n')$}
% \ForEach{$n' \in C: \texttt{shareN}(n, n')$}
%     \State{$\texttt{addEdge}(n, n')$}
% \EndFor
% \Else{\text{ }$\texttt{addNeighborTo}(n)$}
% \EndIf
% \EndFor
% \EndFor
\end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{table}[h]

\begin{tabular}{@{}rccccc@{}}
\toprule
\textbf{}                    & \textbf{Graph Cycles} & \textbf{Degree Rules} & \textbf{Game of Life}  & \textbf{Boolean Formulae} & \textbf{Peano Addition} \\ \midrule
\textbf{\# of unique graphs} & 9                     & 12346                 & $2^{100}$ & 10788                     & 34353                   \\ \bottomrule
\end{tabular}
\vspace{1mm}
\caption{The number of unique graphs that can appear in the time series, sampled from the DGPs in sections \ref{sec:DGS} and \ref{subsec:trees}. as reported by the authors.}
\label{tab:numunique}

\end{table}

The \textbf{Peano addition} DGP models Peano's recursive definition of addition. The operations are encoded similarly as in the Boolean formulae DGP, where both the operands and the arguments are represented as nodes in the dynamical tree graph.The initial graph generator function receives an argument, specifying the maximal number $n$ of additions. The authors use $n=3$ in their experiments. Peano's addition rules simplify into four edit rules, the edit scripts of which are all upper bound as $|\bar{\delta}| \leq 3.$ The node attributes appearing in the set are the 10 digit values, the summation operation $+(m,n) = m+n$ and the successor operation $succ(m) = m+1$.

The author-reported numbers of possible graphs, appearing in the time series, resultant from the five described DGPs, is tabulated in Table \ref{tab:numunique}. Note, however, that not all of these graphs can be sampled as the initial graphs $G_{0}$ in a given series and that the mappings $\psi: G_{t}\to G_{t+1}$ are deterministic in all DGPs. Hence, the actual number of unique pairs $(G_{t}, G_{t+1})$ is much lower.

\subsubsection{Real-world social network}
For the final class of experiments, the arXiv HEP-Th citation network data set, first described in \cite{hep}, is used. It describes a graph, parsed from the e-print arXiv and covers all mutual citations within a set of 27,700 papers. In it, a paper $x$, that cites paper $y$ is connected with it with an outgoing edge. From this network, the authors parse sub-graphs with a rolling window approach - considering only papers published within $\tau$ months of a given time point between January 1993 to April 2003. The number of nodes naturally grows with $\tau,$ so the result is a collection of graphs with different orders of node-count magnitude. In the presented experiment, these 1554 discovered sub-graphs of node count $N_{G}\in [100, 2786]$ are assumed as undirected.


% Arxiv HEP-TH (high energy physics theory) citation graph is from the e-print arXiv and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.

\subsection{Hyperparameters}
For all the GNN-based models in the first two classes of experiments, the authors use two hidden layers with 64 neurons each. As far as the architecture specification is concerned, the GENs use summation as the aggregation function and concatenation as the merge function. All networks are trained with the Adam optimizer using the learning rate of $10^{-3}$. The weight decay is set to $10^{-5}$ in the graph dynamical systems class of experiments and to $10^{-3}$ in the dynamical tree class of experiments.

The results for the VGAE model are reported using $\beta=10^{-3}, \gamma=10^{-3}$. The dimensionality of its embedding space is always equal to the size of the last hidden layer, so 64. As per the provided code by the authors, all models use the sigmoid nonlinearity in the experiment on the \textit{Game of Life} dataset, whereas we employ ReLU for all other experiments on synthetic data.

In the experiments reported in section~\ref{subsec:original}, both the training and the testing time series are sampled independently from their corresponding DGP, without special assertions of training and testing set discrepancy. 
All models train on 30,000 series, whereas the testing results are reported for 10 samples. We comment on the authors' methods of risk estimation and provide alternatives for these parameters in section~\ref{subsec:our}. For the experiments on the social network dataset, a 3-hidden layer architecture with the tanh nonlinearity, and PyTorch's default learning rate and weight decay are used.

% Describe how the hyperparameter values were set. If there was a hyperparameter search done, be sure to include the range of hyperparameters searched over, the method used to search (e.g. manual search, random search, Bayesian optimization, etc.), and the best hyperparameters found. Include the number of total experiments (e.g. hyperparameter trials). You can also include all results from that search (not just the best-found results).

\subsection{Experimental setup and code}
\label{sec:methodology}
In our experiments, we use the metrics of precision and recall to evaluate the performance on insertion and deletion tasks. The experiments done on \textit{Tree dynamical systems} use the notion of \textit{accuracy}, which is an indicator function, defined at the value 1 when the nodes in the two input graphs match in all their features, and their adjacency matrices are identical. The reported \textit{accuracy} is the average value of these indicator functions across all graph pairs in all time-series in the test set.

The experiments in section~\ref{subsec:original} were run in a loop across an entire class of DGPs, with 5 repetitions being ran for each considered model. In the training phase, a time series was independently generated on each epoch using its corresponding generator function. As per the original paper, the considered stopping criterion was a rolling 10-epoch average stop loss. Upon finishing training, the model was evaluated on time series, generated by the same generator functions as during training.

We recognized this method of risk estimation as potentially problematic, given that there is no special care taken to ensure the discrepancy of the tranining and testing sets. It is for this reason that we change the used approach in some experiments, reported in section~\ref{subsec:our}. In them, we sample our test set of graphs $G_{0}^{\text{Test}}$ ahead of time, and ensure that at each sampled training time series, the function $\psi: G_{0}^{\text{Test}} \to G_{1}^{\text{Test}},\text{  } \forall \text{  } G_{0}^{\text{Test}} \in T$ remains hidden from the algorithm.


% Include a description of how the experiments were set up that's clear enough a reader could replicate the setup. 
% Include a description of the specific measure used to evaluate the experiments (e.g. accuracy, precision@K, BLEU score, etc.). 
% Provide a link to your code.

\subsection{Computational requirements}
All experimentation was done on a desktop machine, running Windows 11, powered by an AMD Ryzen 7 2700X processor and 32 GB of RAM. The code was evaluated locally, in an environment, based on Python 3.8. The code base provided by the authors is dependant on the NumPy, PyTorch, PyTorch Geometric, Edist \cite{Edist} and MatPlotLib packages.

One repetition of running all three considered models on all three Graph dynamical systems (together) takes 90 minutes on average, with the VGAE taking the bulk of time to train, as the hinge-loss GEN usually hits the stop loss threshold and stops training earlier. A single repetition of the experiment on the Peano addition DGP takes approximately 15 minutes, whereas one over the Boolean formulae experiment takes 1 minute. On average, 60 minutes required to compute a full pass over all 12 months on the Social network experiment, for both edit schemas together. Working only with the largest graphs, i.e. $\tau=12$, takes 8 minutes on average.

% Include a description of the hardware used, such as the GPU or CPU the experiments were run on. 
% For each model, include a measure of the average runtime (e.g. average time to predict labels for a given validation set with a particular batch size).
% For each experiment, include the total computational requirements (e.g. the total GPU hours spent).
% (Note: you'll likely have to record this as you run your experiments, so it's better to think about it ahead of time). Generally, consider the perspective of a reader who wants to use the approach described in the paper --- list what they would find useful.
\begin{figure}[t]
\begin{minipage}{0.62\textwidth}



    \includegraphics[width=\linewidth]{wider.png}
    \captionof{figure}{The runtime - graph scale dependence in the experiment \ref{exp:sn}, with overlaid fitted loess models.
    Each facet corresponds to an individual experiment, and the grey bands denote the 95\% confidence interval of the fit.}
    \label{fig:arxiv}


\end{minipage}
\hfill
\begin{minipage}{0.34\textwidth}

% Please add the following required packages to your document preamble:


% Please add the following required packages to your document preamble:
% \usepackage{multirow}




\begin{tabular}{@{}rll@{}}
\toprule
\multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}Pass \\ direction\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}l@{}}Edge \\ filtering\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Log-log \\ linear fit slope\end{tabular}} \\ \midrule
\multirow{2}{*}{\textbf{Forward}}                                                      & Flexible                & $1.38 \pm 0.02$                                                                 \\ \cmidrule(l){2-3} 
                                                                                       & Constant                & $1.31 \pm 0.02$                                                                 \\ \midrule
\multirow{2}{*}{\textbf{Backward}}                                                     & Flexible                & $1.30 \pm 0.01$                                                                 \\ \cmidrule(l){2-3} 
                                                                                       & Constant                & $1.69 \pm 0.10$                                                                 \\ \bottomrule
\end{tabular}
\captionof{table}{Slopes of log-log linear models on the Runtime/Graph scale scatter plot. The uncertainty denotes the standard deviation of slopes accross 5 repetitions of the experiment \ref{exp:sn}.}
\label{tab:sn}

\end{minipage}
\end{figure}
\section{Experiments}
\label{sec:results}
Our results confirm the authors' findings from claims (i) - (iii) when considering the results of the strict reproduction. We find that the scaling of the backward passes from claim (iv) is not linear, but remains sub-quadratic. However, we show that these results are achieved by an architecture that is not able to optimize its loss function successfully. 

Our additional experiments in Subsection \ref{subsec:our} show that the experimental results are stable for different choices of the initial graph $G_{0}$. The results also stand for more robust method of risk estimation. From these additional experiments, we derive important insights about the testing scenarios, presented in Section \ref{sec:discuss}. 

% While our more robust method of risk estimation retrieves differing results, it only contradicts claim (ii).

% Start with a high-level overview of your results. Do your results support the main claims of the original paper? Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next "Discussion" section. 


\subsection{Experiments reproducing original paper}
\label{subsec:original}
% For each experiment, say 1) which claim in section~\ref{sec:claims} it supports, and 2) if it successfully reproduced the associated experiment in the original paper. 
% For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.
% Logically group related results into sections. 

\subsubsection{Precision/Recall on Dynamical Graph System DGPs}
\label{exp:first}
In this task, we aimed to reproduce the results, stated in Claim (i) in Section \ref{sec:claims}. For almost all the metrics, we were able to reproduce the values originally reported in the paper, with the difference $\delta := (\text{our results} - \text{reported results})$ within a standard deviation of 0. The only major discrepancy we noticed was an increase in mean deletion precision and insertion recall for the VGAE model in the edit cycle task, when comparing to the results, reported in the original paper. However, both GEN models still outperformed the VGAE, which supports Claim (i).  

\subsubsection{Accuracy on Tree dynamical system DGPs}
\label{exp:second}
In this task, we address Claim (ii) from Section \ref{sec:claims}. In the original paper, the authors reported a 100\% accuracy for both \textit{Tree dynamical system} scenarios. While our results returned an accuracy of $0.98 \pm 0.02$ in the Boolean Formulae task (and a perfect score for Peano addition), we can conclude that these results are convincing enough to support Claim (ii).





\subsubsection{Scaling of GENs on bigger graphs}
\label{exp:sn}
This experiment addresss claims (iii) and (iv) from Section \ref{sec:claims}. In the original paper, the authors claim that GENs were able to scale sub-quadratically in their forward passes and approximately linearly in their backward passes, when using appropriate edge filtering approaches. Figure \ref{fig:arxiv} shows scatter plots of the runtime-graph scale dependency on a log-log scale. Notice, that the runtime duration of the backward passes with constant edge filtering is very unstable, when compared to other scenarios. This is likely due to a higher difference in the fraction of considered edges, when compared to the flexible filtering approach. The scaling coefficients of the fitted linear models are further tabulated in Table \ref{tab:sn}. These results support Claim (iii) in that the forward passes scale sub-quadratically. However, the lower of the two average coefficients for computing the gradient (the flexible approach) is still substantially larger than one. This indicates an exponential, albeit sub-quadratic scaling of the backward passes. We conclude that these results do not support Claim (iv).

\subsection{Experiments beyond original paper}

\label{subsec:our}
% Often papers don't include enough information to fully specify their experiments, so some additional experimentation may be necessary. For example, it might be the case that batch size was not specified, and so different batch sizes need to be evaluated to reproduce the original results. Include the results of any additional experiments here. Note: this won't be necessary for all reproductions.
 
\subsubsection{Established methods of random graph generation}
\label{exp:our1}
It is a common practice in the social network analysis (SNA) community to, when initializing random graphs, use specific methods of graph generation. Namely, if we want to make general statements about SNA methods, inferred from experiments on random graphs, these should be similar to those that tend to appear in nature. At the very least, it is considered a good practice to use established randomization methods, to more easily compare to results in other publications. In this experiment, we repeat the methods from experiment \ref{exp:first} on the Degree rules DGP. However, instead of randomly initializing the adjecency matrix, we use two established methods of random graph generation: the ErdÅ‘sâ€“RÃ©nyi model \cite{erdHos1959renyi} and the Configuration graph model \cite{newman2003structure}. In our experiment, graphs $G_{0}$ were always initialized with 36 nodes in both models. We set the edge creation probability in the ErdÅ‘s-Renyi model $p=0.5$ and the degree sequence of the Configuration model follows a random power-law sequence with the exponent $\gamma = 3$.

All metrics on these newly generated random graphs remained in the 0.05-neighborhood of the originally reported results. We conclude that the performed experiments are robust to different methods of random graph generation, and that the change in graph generation does not disprove Claim (i).

% Figure \ref{fig:erc} shows the quantiles of discrepancy between the reported values and our experiments. We can see that the change in random graph initialization did decrease the performance of the GEN.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{2.png}
%     \caption{Boxplots of discrepancy between originally reported precision and recalls, and the results, achieved by the two alternative methods of initial random graph generation, on the Degree rules DGP. Positive values indicate a better comparative performance in our experiments.}
%     \label{fig:erc}
% \end{figure}





% \begin{wrapfigure}{R}{0.5\textwidth}
%   \begin{center}
%     \includegraphics[width=0.48\textwidth]{gol_unique_test100_gen.png}
%   \end{center}
%   \caption{Diagnostic $\delta$-boxplot comparing the initially reported scores to our results using the alternative risk estimation method and a larger test set, performed on the Game of Life DGP for the hinge-loss based GEN.}
%   \label{fig:golBoxPlot}
%   \vspace{-25pt}
% \end{wrapfigure}

\begin{figure}[t]
\begin{minipage}{0.43\textwidth}
\begin{center}
    \includegraphics[width=\textwidth]{gol_unique_test100_gen.png}
  \end{center}
  \captionof{figure}{Diagnostic $\delta$-boxplot comparing the initially reported scores to our results using the alternative risk estimation method and a larger test set, performed on the Game of Life DGP for the hinge-loss based GEN.}
  \label{fig:golBoxPlot}
\end{minipage}
\hfill
\begin{minipage}{0.53\textwidth}
\begin{center}
    \includegraphics[width=\textwidth]{newe_peanos2.png}
  \end{center}
  \captionof{figure}{Distributions of time series lengths, sampled from the Tree Dynamic Systems DGPs. The facet rows correspond to the maximal number of operations (3-5).}
  \label{fig:PeanoProblem}
  \vspace{5mm}
\end{minipage}
\end{figure}
\subsubsection{Alternative methods of risk estimation - Dynamical graph systems}
\label{exp:our2}
As established above, no special care is taken to ensure the discrepancy between the training and testing set of time-series in the original results. In this experiment, we re-run experiments \ref{exp:first} and \ref{exp:second} with our changed method of risk assessment, described in Section \ref{sec:methodology}. We also raise the cardinality of testing set to 100, attempting to achieve stable results. We analyze our results by comparing several repetitions of the new experiment with the reported values. As a diagnostic tool, we employed the automatic plotting of $\delta$-boxplots. An example of such a plot - describing the testing scenario where the discrepancy between the reported results and our experiments was the largest, is provided in \ref{fig:golBoxPlot}. Notice that, while our change in the experimental setup did contribute to slightly worse metric scores, these changes are still minimal $(\delta \in [-0.1, 0.05]\text{ for all observed testing scenarios}).$ Consequentially, we conclude that the experimental results are robust for our method of risk estimation. Other diagnostic boxplots are available in the supplementary repository \cite{Git}. The insights of Figure \ref{fig:golBoxPlot} should not be interpreted as solely positive, as we discuss in Section \ref{sec:discuss}.



\subsubsection{Alternative methods of risk estimation - Dynamical tree systems}
\label{exp:our3}
For the Peano addition and Boolean Formulae DGPs, we attempted to employ a similar sampling restriction for training series generation, as described above. During sampling, however, we noticed that our described methodology failed to sample a sufficient amount of training examples. Our troubleshooting lead us towards the realization, that these DGPs were very prone to generating trees that could not be simplified any further, which meant that no mapping pairs $(G_{0}, G_{1})$ could be generated from such a sample. Our diagnostic results in Figure \ref{fig:PeanoProblem} show the overwhelming majority of samples being part of this group, which casts doubt on the claims, made in \ref{tab:numunique}. We evaluated the empirical probability of a unique, simplifyable tree $G_{0},$ being sampled from a DGP. Our results show that the Boolean addition DGP sampled such a tree with probability $\text{Pr}_\text{Boolean}=0.13 \pm 0.003$, while the Peano addition DGP performed at $\text{Pr}_\text{Peano}=0.26 \pm 0.002$. These results are derived over 300,000 DGP samples with uniformly distributed hyperparametric values of maximal permited operations $p \in [3, 5],$ with 3 repetitions.

In an attempt to evaluate the performance of the GEN on this family of data, we loosen our restrictions, set in Subsection \ref{sec:methodology}. Instead, we run 5 repetitions of training, with holdout estimation ($|\text{Test set}| = 100$) on the time series, generated by the unique graphs $G_{0}$, described in the previous paragraph. In this setup, the results were not perfect, but remained in the $\pm 0.05$ standard-deviation-neighborhood of the reported results.


\subsubsection{Performance on the social network dataset}
The authors use the social network data set only to evaluate the scaling capabilities of the GEN, but do not offer any information on the model's performance on the set.\footnote{This concern was also raised to the authors during the paper's submission and review process by \textit{AnonReviewer4}. See the section \textit{Weak points} in: \texttt{https://openreview.net/forum?id=dlEJsyHGeaL\&noteId=Sg922s85khx}} Since the model's scaling may be dependant on specific model parameters (specifically, the used user-defined loss function), we examine if the model is capable of training using gradient descent in this experiment.
\begin{wrapfigure}{R}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{LossCurves.png}
  \end{center}
  \caption{The loss curves for training the GEN with the authors' custom defined loss function. The differently-colored lines correspond to the values $\tau$, with respect to which the model's training set is generated.}
  \label{fig:LossCurves}
  \vspace{3mm}
\end{wrapfigure}
We visualize the loss curves of training the model over 1554 iterations (one pass of each available graph in Figure \ref{fig:LossCurves}, with all hyper parameters similar to the original experiment, and using the Adam optimizer. We see that the model, implemented in the scenario, does not optimize its loss function successfully.


% \subsubsection{Alternative Game of Life objects}

% \subsubsection{GEN's performance on real-world models.}


\section{Discussion}
\label{sec:discuss}

Our experimental results conclusively show that most of the claims in the original work hold. It is imperative, however, to discuss the choice of DGPs on which the model was evaluated to achieve these results.
Consider, for example, the Game of Life DGP, used for evaluating the precision and recall of the test set. While at first glance, a perfect result on a relatively involved system might be impressive, we must recall that node/edge edits and insertions should never appear in an edit script $\bar{\delta}$ between two Game of life graphs, as the only changes in the systems correspond to replication edits. Consequentially, the system in the scenario is only asked to output without any addition or insertion edits. Since experiment \ref{exp:our1} showed that this output does not always appear, this casts a doubt over the model's expressive power. Another example of a somewhat poor test setup is the Edit Cycles DGP, in which the network will always test on transitions $\psi$, to which it was already introduce during training, given that the series are cyclical and Markovian. Adding to this, it is very likely that, due to the nature of the problem they describe, the mappings $\psi$, inferred from the Peano addition and Degree formulae DGPs, are often seen during training. We support this claim with our description of the sampling problems we encountered in Experiment \ref{exp:our2}.

Our experimental results on the arXiv citation network show that the network's runtimes are subquadratically dependant on the number of nodes in the given graph. This partially corroborates the authors' claims. However, we note that these results are achieved by an architecture, that is not able to optimize its loss function correctly. Given that the loss cumulative loss increases with $\tau$ (as one would expect), we hypothesize that this performance is not a result of a simple syntactical error in the author-defined loss function. While this additional insight does not disprove Claim (iii), 
we note that a different, better performing loss function, might.

We propose that th weakneseses we higlighted here be considered in future work, We believe that a more in-depth and practical experimental evaluation of an otherwise elegant and interpretable solution could greatly benefit the machine learning community in the years to come.

% Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was easy}

All the provided code has extensive and clear documentation which made the paper's experiments easy to reproduce. The entire code base is readable, very modular, adheres to established practices on code readability, and goes hand-in-hand with the nomenclature of the paper. While the presented implementations do require intermediate familiarity with common PyTorch constructs, the authors do admirable work in explaining everything else as-they-go, almost always without using unnecessary dependencies or needlessly referencing the reader elsewhere. The authors also provide a moderate amount of clearly written unit tests for all of their models and have already pre-implemented several diagnostic measures, such as execution runtime logging, repetition handling and plotting of training curves, which made our work a lot easier.

% Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code. 

% Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers). 

\subsection{What was difficult}

Even with the extensive supplementary material, we believe that it would have been very difficult to reproduce the exact implementation of GEN and the presented DGPs by reading the paper alone, as we've discovered many important details from the supplementary documentation. 

In the paper, the authors state that all experiments were run on a consumer-grade laptop. While this may be the case, running some of the provided code is prohibitively time consuming to run on such a machine. For example, we were not able to finish a single pairwise distance calculation in a day's worth of computing time (and have thus not reported on the results of that method here) on  the kernel-based baseline from \cite{paassen2018time}. The lack of transparency about the code base's runtimes made our work here much more difficult.

The original paper also uses a direct implementation of the \cite{VGRNN} as a baseline for experiments, relating to \mbox{Claim (i).} This model was not provided in the repository at the beginning of our work. The authors later provided us with the implementation, which encountered runtime errors. Even though the model now works, the trouble-shooting of this part of the code was especially time-consuming. The author's repository also lacks a hierarchical structure of related items. While the purpose of every file is clearly explained, our reproduction would have been easier with some reorganization.

% List part of the reproduction study that took more time than you anticipated or you felt were difficult. 

% Be careful to put your discussion in context. For example, don't say "the maths was difficult to follow", say "the math requires advanced knowledge of calculus to follow". 

\subsection{Communication with original authors}

We contacted Mr. PaaÃŸen, along with his colleagues to inform them about our efforts to reproduce their work in mid-January. He was prompt with his responses, welcomed our work and made himself available for any questions. Upon our request, he forwarded the code with which the authors evaluated a baseline, reported in the paper, but not available in the repository. Upon our discovering of its aforementioned problems, he was prompt to offer solutions and sent us an adapted file in a couple of days. He let us know that the authors plan to update the repository with this working file shortly, which we see as an aditional benefit of our effort reproducing this article.

When asked about their method of risk estimation, the author argued that the combinatorial explosion of possible starting states makes it unlikely that GEN just memorizes the training data without generalization. For the case of the Edit Cycles dataset, where this obviously has to happen, since there is no underlying ground-truth function $\psi$, he offered the insight that generalization was not the main aim of the inclusion of this dataset. Rather, it was intended to test the expresiveness of the edits, as memoriztation alone does not suffice to solve the task of mapping $G_{t} \to G_{t+1}$.

Summing up, we greatly appreciate the authors' responses and their general attitude towards their work being reproduced as a part of this challenge.




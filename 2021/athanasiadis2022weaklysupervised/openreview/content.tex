\section{Reproducibility Summary}

\subsection{Scope of Reproducibility}
% State the main claim(s) of the original paper you are trying to reproduce (typically the main claim(s) of the paper). This is meant to place the work in context, and to tell a reader the objective of the reproduction.
In this work, we experimented with Layer-wise Relevance Propagation and combined it with back-propagation to perform classification and semantic segmentation, following the approach proposed by Chefer H. et al., in \cite{mainpaper} for computer vision. Moreover, we incorporated the concept of pixel affinities, by using ViT-based explainability as visual seeds to drive the generation of pseudo segmentation masks by computing pixel affinities, following the approach described by Ahn J. et al. in \cite{ahn2018learning}.

\subsection{Methodology}
% Briefly describe what you did and which resources you used. For example, did you use author's code? Did you re-implement parts of the pipeline? You can also use this space to list the hardware used, and the total budget (e.g. GPU hours) for the experiments.
In order to reproduce the experiments presented in \cite{mainpaper} and \cite{ahn2018learning}, we initially examined the authors' code thoroughly and based on our understanding, we tried to replicate most parts of the pipeline apart from evaluation metrics for positive and negative perturbation area-under-curve (AUC) results for the predicted and target classes on the ImageNet~\cite{russakovsky2015ImageNet} validation set, as well as Segmentation performance on the ImageNet-segmentation~\cite{imagenet-seg} dataset, which we borrowed from the authors' repository for the work of Chefer H. et al., in \cite{mainpaper}. Regarding hardware, we used private resources to train our ViT-hybrid architecture and Affinity network, as well as perform inference for all our models; Finally, it took roughly 15 GPU hours to reproduce the vision-related results of \cite{mainpaper} whereas it took about 40 GPU hours to train and evaluate the AffinityNet on the Hybrid-ViT architecture.

\subsection{Results}
Overall, we reproduced the experiments related to the vision task as conducted at \cite{mainpaper}. Our results are up to first decimal place identical to those reported in \cite{mainpaper} thus supporting the authors' claim of having implemented a relatively sufficient ViT interpretability method. When it comes to the AffinityNet \cite{ahn2018learning}, the method has been adapted in the context of Hybrid-ViT architectures with our experiments indicating that the weakly-supervised semantic segmentation performance of Hybrid-ViT architectures are inferior to the CNN-based ones.


\subsection{What was easy}
We found particularly easy to run and understand the code provided by the original authors of both \cite{mainpaper} and \cite{ahn2018learning} papers. When it comes to replicating \cite{mainpaper}, the authors provided most of the information required to reproduce the vision-related experiments with the code compensating for what was missing.


\subsection{What was difficult}
The main difficulty of replicating the study presented in \cite{mainpaper} was that details on how to compute the AUC metric were not provided in the paper report. 

\clearpage

\section{Introduction}
\label{section:intro}
% Describe the problem, the approach of the paper, the experiments, and the results. At the high-level talk about what you worked on in your project and why it is important. Then give an overview of your results.
One of the most exciting technological aspects nowadays is Machine Learning's mind-blowing potential in transforming the world we live in, mainly due to its exciting resurgence through Deep Learning. However, as machine learning models become more complex, there is a noticeable trade-off between accuracy and simplicity or interpretability \cite{explanations} and plenty of cutting-edge research papers have been published in top-tier conferences related to this tension. In this project, we primarily experimented with Layer-wise Relevance Propagation (LRP), a mechanism of explaining what pixels are relevant within a 2-dimensional image for reaching a classification decision \cite{renormalizationLRP} and applied it to a Vision Transformer [ViT] \cite{visiontransformer}, combined with gradient back-propagation to perform classification but also semantic segmentation on the respective data in ImageNet \cite{russakovsky2015ImageNet, imagenet-seg}, by reproducing the work of Chefer H. et al, in \cite{mainpaper}.

Furthermore, the task of semantic segmentation refers to clustering the pixels of an input image that correspond to the same semantic category. There are various approaches dedicated to this task with the one proposed in \cite{panoptic} being the current state-of-the-art. However, they all rely on training given ground truth segmentation masks. Considering that annotating images in the form of segmentation masks is a rather expensive and tedious process, capitalizing on weak forms of segmentation would be highly beneficial. In order to address these issues, in this project, we investigated using ViT-based explainability as visual seeds to drive the generation of pseudo segmentation masks by computing pixel affinities, following the approach described in \cite{ahn2018learning}. In particular, we trained a Hybrid ViT-base, where the patches are extracted from a CNN feature map, through relevance propagation and used those as seeds to a network computing pixel affinities, in order to improve quality of the generated segmentation masks. 
%An intuitive example of applying our Hybrid ViT-base and our AffinityNet implementations is illustrated in Fig. \ref{fig:fig1}.
% . On the other hand, but also go beyond this and experiment with weakly supervised semantic segmentation using Pascal VOC dataset, by reproducing the work of Jiwoon A. et al, in \cite{affinitypaper}. Our results indicate \textbf{we will see once affinityNet is ready}.

\section{Related Work}
Semantic segmentation has numerous applications, such as self-driving cars or medical image analysis.  Additionally, the evident importance in providing the machines with the ability to perceive the world along with its challenging nature has attracted many researchers to this domain. Many algorithms have been proposed for this task with Mask R-CNN \cite{he2017mask} being among the most frequently employed ones. Although such  approaches can be trained to extract semantic with high precision, they require an extensive amount of semantically annotated training samples. In their work \cite{ahn2018learning}, the authors capitalize on image-level supervision to construct competent pseudo-segmentation masks that can be further utilized to train the segmentation approaches requiring ground truth labels. More specifically, they use class activation mapping (CAM) \cite{zhou2016learning} seeds to model the relation between neighboring pixels, which enables the refinement of the initial CAM cues into segmentation masks of higher quality. Although the previous approach results in relatively accurate segmentation masks, the initial CAMs seeds tend to highlight only the most descriptive part of an instance, which negatively affects the quality of the generated segmentation masks. With the purpose of mitigating this issue, the essayist of \cite{chang2020weakly} employs a sub-category exploration approach. 

Regarding Deep Neural Networks (DNNs) interpretability, various approaches have been proposed in the literature. GradCAM \cite{selvaraju2017grad} is a popular interpretability method applied to various CNN architectures that weighs feature activations in different pixel regions within an image with the average gradient of the class scores. After these gradients are computed through global average pooling, they are passed to a ReLU\footnote{Rectified Linear Units activation function is: $\text{ReLU}(x) = \max\{x,0\}$.} activation function that intensifies pixels contributing towards increasing the target class activation scores. However GradCAM is restricted to CNN architectures. One more general approach is RISE \cite{rise} that measures pixels' importance by applying element-wise multiplications of the original input with a sampled random binary mask to reduce their intensities to zero and only preserve the most important among them. 

Although CNN-based architectures have demonstrated competent performance in a number of vision-related tasks, they come with an increased inductive bias due to the 2D neighboring structure of the images. On the other hand, transformer-based architectures are able to learn spatial relationships detached from the explicit 2D nature of the images. Transformer architecture, since it was proposed in 2017 by Waswani A. et al., \cite{vaswani2017attention} has become very popular in various deep learning domains, and it is based solely on attention mechanisms, dispensing recurrence and convolutions entirely and weighing the influence of different parts of the input data. Following its recent success in NLP, it was recently adopted in computer vision tasks, and in this work, we focus on particularly re-implementing a Vision Transformer [ViT] \cite{visiontransformer} from scratch. Additionally, we employ the explainability cues derived from a image classification ViT to drive the construction of segmentation masks given solely image-level annotation as we explain hereunder.

\section{Methods}

In this section, we describe the methods utilized in our work. Precisely, in subsection \ref{sub:3.1}, we provide details about Vision Transformer architecture. Subsection \ref{sub:3.2} explains how we perform relevance propagation in our model implementations. Finally, in subsection \ref{sub:3.3}, we present the AffinityNet framework modeling the affinity of neighboring pixels.


\subsection{ViT Classification}
\label{sub:3.1}
As mentioned earlier, a Vision Transformer [ViT] \cite{visiontransformer} is an implementation of transformer networks for computer vision tasks. The transformer encoders in ViT are similar to the original transformer architecture introduced in \cite{vaswani2017attention} with slight modifications in the order of operations. Similarly to how a sentence is split into tokens, in ViT we split an image into patches and provide the linearization of the patches representations as input to stacked transformer encoders after adding positional embeddings. Positional embeddings are learned during training; while processing the input patches in given order $x_0, x_1, x_2, ...$ we learn the respective positional embeddings $\hat{x}_0, \hat{x}_1, \hat{x}_2, ...$ for the patches and compute the loss in a backward fashion. The input is then propagated to the attention heads, where multi-head attention is calculated as the concatenation of self-attention scores computed in each head individually as stated in the formulas below:
\begin{align*}
\text{Attention}(Q, K, V) &= \text{softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)V\\
\text{Multihead}(Q, K, V) &= \text{Concat}(\text{head}_1,...\text{head}_h)\Theta^o \\
\text{where head}_i &= \text{Attention}(Q\Theta_i^Q, K\Theta_i^K, V\Theta_i^V)
\end{align*}

Attention is a mechanism for weighting representations learned in a neural network. It is proportional to the respective weights of the network and really flourished within a variety of NLP tasks, where self-attention and multi-head attention became one of the major breakthroughs in sequence modeling tasks precisely \cite{guidedtransformer}. In our implementation, we use ViT-Base, the smallest ViT model variant, which consists of $12$ stacked encoder layers, as well as $12$ attention heads in every layer, as it is illustrated in table \ref{tbl:models}. We use a \texttt{[CLS]} learnable embedding $\mathbf{z}_0^0
= \mathbf{x}_{\text{class}}$ to the sequence of embedded patches, whose state at the output of the Transformer encoder $\mathbf{z}_0^L$, to which a classification head is attached to represent an image $\mathbf{y} = \text{LayerNorm}(\mathbf{z}_0^L)$. We also employ a hybrid architecture, which again consists of a ViT-Base but the patches are extracted from a CNN feature map, while layer normalization is applied before
every block and residual connections after every block in our implementation as it is described in \cite{visiontransformer}.

\begin{table*}[]
\centering
\small
\begin{tabular}{l c c c c c}
\toprule
Model            & Layers & Hidden size $D$ & MLP size &  Heads  & Params \\
\midrule 
ViT-Base   &   12   &        768      &   3072   &   12    &   86M  \\
\bottomrule
\end{tabular}
\caption{Details of ViT model variants. Table extracted from \cite{visiontransformer}.}
\label{tbl:models}
\AND\end{table*}

\subsection{ViT Explainability}
\label{sub:3.2}
As we explained in section \ref{section:intro}, one of our main goals in this project was to apply LRP \cite{renormalizationLRP} to a ViT-Base model \cite{visiontransformer}, combined with classic gradient back-propagation regime to perform classification but also semantic segmentation on the respective data in ImageNet \cite{russakovsky2015ImageNet, imagenet-seg}, by reproducing the work of Chefer H. et al, in \cite{mainpaper}. Considering the input feature map and weights of layer $n$ in form
of tensors, $\mathbf{X}, \mathbf{\Psi}$ we compute the Deep Taylor Decomposition $R_j^{(n)}$ for relevance propagation as formulated below. This expression satisfies the conservation rule that broadly suggests that relevance will be maintained in consecutive layers.
\begin{align}
    \label{eq:relevance}
    \nonumber
    R_j^{(n)} = \mathcal{G}\left(\mathbf{X}, \mathbf{\Psi}, R^{(n-1)}\right)
    &= \sum_{i} \mathbf{X}_j \frac{\partial L_i^{(n)}(\mathbf{X, \Psi})}{\partial\textbf{X}_j}
\end{align}
Moreover, in cases we have two operators (e.g. skip connections and matrix multiplication) the above expression is used for both the input pairs $(u, v)$ and $(v, u)$ to compute $R_j^{u}^{(n)}$ and $R_j^{v}^{(n)}$. Given two such tensors $u$ and $v$, if we add them in layer $n$ the conservation rule is maintained but not in other cases of operations such as matrix multiplication. To address this lack of conservation we normalize the relevances and get $\bar{R}_j^{u}^{(n)}$ and $\bar{R}_j^{v}^{(n)}$ respectively. In addition, there is a special case related to the matrix multiplication operation, where we get two attribution maps for each of the matrices we multiply, and the sum of the relevances of each matrix equals $R$. Furthermore, to actually normalize the CAMs, all we need to do is divide each of them by $2$, which is what the normalization below would do since $R_j^{u}^{(n)}$ and $R_j^{v}^{(n)}$ have identical sums.
\begin{align*}
    R_j^{u}^{(n)} &= \mathcal{G}\left(u, v, R^{(n-1)}\right) \\
    R_j^{v}^{(n)} &= \mathcal{G}\left(v, u, R^{(n-1)}\right) \\
    \bar{R}_j^{u^{(n)}} &= R_j^{u^{(n)}} \frac{ \abs{\sum_j{R_j^{u^{(n)}}}}}{\abs{\sum_j {R_j^{u^{(n)}}}} + \abs{\sum_k{R_k^{v^{(n)}}}}} \cdot \frac{\sum_i R_i^{(n-1)}}{\sum_j R_j^{u^{(n)}}}\\
    \bar{R}_k^{v^{(n)}} &= R_k^{v^{(n)}} \frac{\abs{\sum_k {R_k^{v^{(n)}}}}}{\abs{\sum_j {R_j^{u^{(n)}}}} + \abs{\sum_k {R_k^{v^{(n)}}}}} \cdot \frac{\sum_i R_i^{(n-1)}}{\sum_k R_k^{v^{(n)}}}
\end{align*}
Following the above formulas, we have computed relevances for all layers of our ViT-Base and have implemented relevance propagation, in order to perform semantic segmentation on the ImageNet-segmentation~\cite{imagenet-seg} dataset following the experiments described in \cite{mainpaper}. An example of a CAM generated by our Hybrid ViT-base, where the patches are extracted from a CNN feature map, through relevance propagation is illustrated in Fig. \ref{fig:fig1}(b).

\subsection{AffinityNet}
\label{sub:3.3}
At this stage, we employed the AffinityNet proposed in \cite{ahn2018learning} with the purpose of refining the initially incomplete explainability cues, derived from the Hybrid-ViT network, into segmentation masks of higher quality. In more detail the AffinityNet aims at modelling the relation between adjacent pixels through leveraging the images' feature representation $f^\text{aff}$ and computing the similarity of $i^\text{th}$ and $j^\text{th}$ pixels as:
\begin{align*}
W_{i,j}=\exp\left(-||{f_i^\text{aff}-f_j^\text{aff}}||\right)
\end{align*}
Conceptually, the AffinityNet is trained to predict the inter-pixel semantic affinities, in a class-agnostic manner, by learning to extract meaningful representations for each pixel. Evidently, target labels are required in order to drive the AffinityNet's weights towards accurately predicting the affinities.

\subsubsection{Semantic Affinity Targets}
Training the AffinityNet to model the inter-pixel relationships, requires supervision in the form of segmentation masks. In our scenario, ground truth segmentations labels were not provided and thus the generated ViT explainability seeds are utilized as our best available source of supervision. Admittedly, the generated explainability cues can be quite incomplete and by no means precisely capture the whole instances, however, we can use the most confident pairs in terms of belonging to the same instance. Assuming $C$ classes with $M_{c}$ corresponding to the explainability cue of class $c$, we construct the background activation map $M_{bg}$ as:
\begin{align*}
\label{eq:dropweight}
M_{bg}(x,y) = \left[1 - \max\limits_{c\in C} M_c(x,y)\right]^\alpha
\end{align*}
The parameter $\alpha$ controls how confident the generated background cues are. Intuitively, when the $\alpha$ parameter is relatively high, a pixel of high activation in the $M_{bg}$ would be a strong indication of the pixel belonging to the background category. On the contrary, when the $\alpha$ parameter is relatively low, a high background activation suggests that background is the dominant semantic of that pixel but not with as much confidence. Next, we make use of the common practice of applying dense conditional random fields (dCRF) \cite{krahenbuhl2011efficient} to refine the activation responses for all $C+1$ classes. Applying the dCRF on these classes' activations with the $M_{bg}$ having been derived from a low $\alpha$, favors classifying the pixels as background. On the other hand, when a high $\alpha$ is used, the dCRF is more prone to classifying a pixel as its most activated class. Having said that, applying dCRF on low $\alpha$ gives rise to the confident pixel of foreground instance while on the other, a high $\alpha$ allows for identifying confident background pixels. In our experiments, we set $\alpha_\text{low} = 4$ and $\alpha_\text{high} = 32$ respectively. Below we provide an indicative illustration of confident background and foreground pixels. 

\begin{figure}[H]
    \centering
    \subfigure[]{\includegraphics[width=0.23\textwidth]{img_plane.jpg}} 
    \subfigure[]{\includegraphics[width=0.23\textwidth]{plane_cam.jpg}} 
    \subfigure[]{\includegraphics[width=0.23\textwidth]{low_1.jpg}} 
    \subfigure[]{\includegraphics[width=0.23\textwidth]{high_0.jpg}}
    \caption{(a) Actual image (b) Hybrid-ViT explainability cue for the "Plane" Class (c) dCRF generated confident foreground (d) dCRF generated confident background (The lighter the color intensity the higher the activation).}
    \label{fig:fig1}
\end{figure}

Next, we extract pairs of pixels belonging to the same category with high confidence. Additionally, we also consider as neutral, those pixels that were classified by the dCRF as background in the presence of low $\alpha$ and as foreground in the opposite case. Finally, the construction of confident common-instance pairs is now feasible. We consider pairs of positive and negative affinity, in a class-agnostic manner, while we ignore any pair containing neutral pixels. It is worth highlighting that only neighboring pairs are extracted with a radius of $5$ pixels. An intuitive figure, showcasing the possible affinities is displayed below.

\begin{figure}[!h]   % !h puts the image in the exactly right place
    \centering
    \subfigure[]{\includegraphics[width=0.4\textwidth]{Affinity_graph.png}} 
    \caption{Concept of pixel-to-pixel affinities [image taken from \cite{ahn2018learning}]}
\end{figure}

\subsubsection{Training AffinityNet}
After having generated the explainability-based affinity targets, we can now train a neural network to generate insignificant $W$ values to those pixels that are semantically unrelated. More specifically, we utilized the CNN-backbone as trained in the Hybrid-ViT image classification task for feature representation $f^\text{aff}$ purposes. In order to adapt to affinity-assignment task, we employed two $1\times1$ convolutions on top of the feature map extracted from the Hybrid backbone. The loss used for training the network incorporates three different types of affinities, namely the negative, the foreground positive and background positive affinities. Additionally, we weighted the loss contributions of these three types based on the amount of negative, foreground, and background affinity labels on each training batch. The intuition behind this approach was to avoid only accounting for the most frequent case of background positive relationships due to images containing mostly background content. Based on these the overall loss was computed as :
\begin{align*}
\label{eq:dropweight}
\mathcal{L}_\text{fg}^+ =- \frac{1}{N_\text{fg}^+}\sum_{i,j}\log(W_{i,j})^{I(i,j\in \mathcal{T}_\text{fg}^+)}
\end{align*}
\begin{align*}
\label{eq:dropweight}
\mathcal{L}_\text{bg}^+ =- \frac{1}{N_\text{bg}^+}\sum_{i,j}\log(W_{i,j})^{I(i,j\in \mathcal{T}_\text{bg}^+) }
\end{align*}
\begin{align*}
\label{eq:dropweight}
\mathcal{L}^- =- \frac{1}{N^-}\sum_{i,j}\log(W_{i,j})^{I(i,j\in \mathcal{T}^-) }
\end{align*}
\begin{align*}
\label{eq:dropweight}
\mathcal{L}^- = \mathcal{L}_\text{fg}^+ + \mathcal{L}_\text{bg}^+ + 2\mathcal{L}^-
\end{align*}
with $I$ being the indicator of $i^\text{th}$ and $j^\text{th}$ pixel sharing the target relationship $\mathcal{T}$. Note that the $\mathcal{L}^-$ contributes twice in order avoid unbalance between positive and negative relationships.

\subsubsection{Refining the Explainability seeds}
At this stage, we utilized the predicted pixel-wise affinities to propagate high explainability activations towards the pixels of identical semantic affinity. In more detail, we regarded the predicted affinities as transition probabilities in a random-walk process. By employing this approach, we were able to propagate the highly activated regions based on the semantic relationships predicted from AffinityNet. The transition matrix derives from the predicted affinities as: 
\begin{align*}
\label{eq:dropweight}
T_{rw}=D_w^{-1}W^\omicron{o\beta}
\end{align*}
with $D_w$ being a diagonal array applying row-wise normalization to $W$. Additionally, the $o\beta$ operator is applied so that low transitional probabilities are ignored. Naturally, the \byta hyperparameter has to be an integer value larger than one. Next, we compute the expected transitional probabilities of $t+1$ iterations of the random walk process as:
\begin{align*}
\label{eq:dropweight}
T_{rw}=T_{rw}^t
\end{align*}
Finally, we extract the semantic segmentation masks through refining the explainability seeds $M_c$ for each $c$ class as:
\begin{align*}
\label{eq:dropweight}
\text{vec}(M_c^\text{new})=T_{rw}\text{vec}(M_c)
\end{align*}
with $\text{vec}(.)$ being the array flatten operator. In our experiments, we used values of $16$ and $8$ for the hyperparameters $\beta$ and $t$ respectively.


\section{Experiments}

\subsection{Data}

In this project, two different datasets were used: ImageNet \cite{russakovsky2015ImageNet} (ILSVRC) 2012 along with its mask-annotated ImageNet-Segmentation \cite{imagenet-seg} split and the PASCAL VOC 2012 \cite{Everingham15}. The ImageNet dataset validation split consists of $1000$ object classes with $50.000$ images while the mask-annotated split contain $4.276$ from 445 classes. The PASCAL VOC, considers $20$ image categories with $10.583$ and $1450$ images in the training and the validation split respectively.


\subsection{Transformer Explainability}
As part of replicating the target paper \cite{mainpaper}, we conducted perturbation and segmentation tests, while the results are presented in tables \ref{tab:perturbations} and \ref{tab:segmentation} respectively. For the former type of tests, we use a pre-trained ViT-Base network to extract visualizations for the validation set of ImageNet 2012 ~\cite{russakovsky2015ImageNet}. Afterwards, we gradually mask out the pixels of the input image, from the one with the highest relevance to the one with the lowest when referring to positive perturbation and vice versa in the case of negative perturbation. Consequently, in the first case, we expect to see a high drop in performance when measuring the mean top-1 accuracy of the network while in the second case we expect the overall performance to remain unaffected. Regarding the latter type of tests, we consider each visualization as a soft segmentation of the image and compare it to the ground truth segmentation mask of the ImageNet segmentation dataset\footnote{ImageNet segmentation dataset was obtained from \href{http://calvin-vision.net/bigstuff/proj-imagenet/data/gtsegs_ijcv.mat}{calvin-vision.net}.}.  In 
table \ref{tab:perturbations} we report the AUC metric for the perturbation tests considering the explainability cues corresponding to both the most confident (predicted) and the ground truth class (target). Additionally, in table \ref{tab:segmentation} we evaluate the segmentation quality of the extracted cues by comparing them with the provided ground truth segmentation masks. In Appendix \ref{ImageNet_results} we provide qualitative results corresponding to explainability cues in ImageNet; generated using our ViT-Base implementation.


\setlength\tabcolsep{2pt}
\begin{table*}[!h]
    \centering
    % \begin{center}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llccccccc}
        \toprule
        &&rollout & raw  & GradCAM & LRP & partial LRP & Target paper & Ours\\
        & &~\cite{samira2005} & attention &~\cite{selvaraju2017grad} &~\cite{binder2016layer} &~\cite{voita2019analyzing} &~\cite{mainpaper} & \\
        \midrule
        \multirow{2}{*}{Negative} &Predicted & 53.1 & 45.55 & 41.52 & 43.49 & 50.49 & \textbf{54.16} & 54.13\\
        &Target & - & - & 42.02 & 43.49 & 50.49 & \textbf{55.04} & 55.03 \\
        \midrule
        \multirow{2}{*}{Positive} &Predicted & 20.05 & 23.99 & 34.06 & 41.94 & 19.64 & \textbf{17.03} & \textbf{17.03}\\
        &Target & - & - & 33.56 & 41.93 & 19.64 & \textbf{16.04} & 16.38\\
        \bottomrule
    \end{tabular*}
    % \end{center}
    \caption{Positive and Negative perturbation AUC results (percents) for the predicted and target classes, on the ImageNet~\cite{russakovsky2015ImageNet} validation set. For positive perturbation lower is better, and for negative perturbation higher is better. Table partly extracted from \cite{mainpaper}.}
    \label{tab:perturbations}
\end{table*}
    \medskip

\begin{table*}[!h]
    % \begin{center}
    % \centering
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lccccccc}
        \toprule
        &rollout & raw  & GradCAM & LRP & partial LRP & Target paper & Ours\\
        &~\cite{samira2005} & attention &~\cite{selvaraju2017grad} &~\cite{binder2016layer} &~\cite{voita2019analyzing} &~\cite{mainpaper} & \\
        \midrule
        pixel accuracy & 73.54 & 67.84 & 64.44 & 51.09 & 76.31 & 79.70 & \textbf{79.73}\\
        mAP & 84.76 & 80.24 & 71.60 & 55.68 & 84.67 & \textbf{86.03} & \textbf{86.03}\\
        mIoU & 55.42 & 46.37 & 40.82 & 32.89 & 57.94 & 61.95 & \textbf{62.01}\\
        \bottomrule
    \end{tabular*}
    % \end{center}
    \caption{Segmentation performance on the ImageNet-segmentation~\cite{imagenet-seg} dataset (percent). Higher is better. Table partly extracted from \cite{mainpaper}.}
    \label{tab:segmentation}
\end{table*}

\subsection{AffinityNet by ViT explainability}
For the purpose of generating competent segmentation masks given only image-level supervision, we relied on AffinityNet to refine the initially incomplete explainability cues derived from the Hybrid-ViT image classification network. We evaluated the class-wise mIoU in the PASCAL VOC validation dataset in table \ref{tab:Affinity} where we compare the mIoU performance of the explainability cues prior and post employing the AffinityNet-based refinement \cite{ahn2018learning}. In Appendix \ref{Pascal_results} we provide qualitative results corresponding to the refinement of the ViT-derived explainability cues via the AffinityNet.


\begin{table*}[!h]
    % \begin{center}
    % \centering
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcccc}
        \toprule
        &CAM \cite{zhou2016learning} & AffinityNet \cite{ahn2018learning} & Ours & Ours AffinityNet \\
        &[VGG-16] & [VGG-16] & [ViT-Hybrid] & [ViT-Hybrid] \\  

        
        \midrule
        mIoU & 46.60 & \textbf{54.00} & 44.60 & 50.90\\
        \bottomrule
    \end{tabular*}
    % \end{center}
    \caption{Segmentation performance on the Pascal VOC segmentation \cite{Everingham15} dataset (percent). Higher is better.}
    \label{tab:Affinity}
\end{table*}


\subsection{Implementation Details}
Regarding the replication of paper \cite{mainpaper}, no training was required as we relied on the available ViT weights pre-trained on the ImageNet dataset. When it comes to utilizing explainability cues derived from ViT architectures for training the AffinityNet, we trained a hybrid-ViT architecture on PascalVoc while capitalizing on the weights as pretrained on ImageNet. More specifically, we trained for $20$ epochs with a learning rate $5e-3$. The AffinityNet was trained on Pascal VOC training split for $7$ epochs with a learning rate of $0.1$ using the affinity labels as generated by the ViT explainability cues. In both these training setups, the batch size was set to $8$, the weight decay to $1e-4$ while the SGD optimizer was used. Finally, during training, images were resized to $244\times244$ and $448\times448$ resolution for ImageNet and Pascal VOC respectively. Moreover, the images were normalized to have $0.5$ mean and $0.5$ standard deviation for all channels while  random horizontal flip and color jittering were employed for data augmentation purposes.


\section{Conclusions}
In the context of this study, we replicated the ViT explainability approach proposed in \cite{mainpaper}. Additionally, we capitalized on the explainability seeds derived from a Hybrid-ViT architecture to generate competent semantic segmentation labels for weak-supervision. More specifically, the AffinityNet \cite{ahn2018learning} was employed with the purpose of refining the initially incomplete explainability cues into segmentation masks of higher quality. The quantitative results provided in tables \ref{tab:perturbations} and table \ref{tab:segmentation} indicate that we have successfully implemented the explainability method described in \cite{mainpaper} since our results are identical to those originally reported in the latter for all the considered metrics. Regarding the AffinityNet, we evaluated the class-wise mIoU performance that we have achieved based on the explainability seeds as generated by the Hybrid-ViT architecture. 

Furthermore, according to table \ref{tab:Affinity}, we observe that the performance we achieved is lower compared to the one reported in \cite{ahn2018learning}, however segmentation masks of improved quality were generated. One reason for that could be the lower quality of ViT explainability seeds compared to the CNN-based ones. Another potential reason for the lacking performance of the AffinityNet, when given explainability cues from ViT architecture, could be that the feature map $f^\text{aff}$ in our case, derives from low-level image representation where as in the original paper \cite{ahn2018learning} feature representation from multiple levels were aggregated. Such multi-level aggregation was not feasible in our scenario due to the nature of the transformer architecture. 

Concluding, in this work we have demonstrated the feasibility of using ViT-derived explainability cues with the purpose of training the AffinityNet. Although, we were able to increase the quality of the ViT explainability cues by refining them with the AffinityNet, the CNN-based architectures perform better while using lighter models.

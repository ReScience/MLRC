\begin{thebibliography}{10}

\bibitem{explanations}
L.~H. Gilpin, D.~Bau, B.~Z. Yuan, A.~Bajwa, M.~A. Specter, and L.~Kagal,
  ``Explaining explanations: An approach to evaluating interpretability of
  machine learning,'' {\em CoRR}, vol.~abs/1806.00069, 2018.

\bibitem{renormalizationLRP}
A.~Binder, G.~Montavon, S.~Bach, K.~M{\"{u}}ller, and W.~Samek, ``Layer-wise
  relevance propagation for neural networks with local renormalization
  layers,'' {\em CoRR}, vol.~abs/1604.00825, 2016.

\bibitem{visiontransformer}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image
  recognition at scale,'' {\em CoRR}, vol.~abs/2010.11929, 2020.

\bibitem{russakovsky2015ImageNet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei, ``Imagenet
  large scale visual recognition challenge,'' {\em International Journal of
  Computer Vision}, vol.~115, pp.~211--252, Dec 2015.

\bibitem{imagenet-seg}
M.~Guillaumin, D.~K{\"u}ttel, and V.~Ferrari, ``Imagenet auto-annotation with
  segmentation propagation,'' {\em International Journal of Computer Vision},
  vol.~110, pp.~328--348, Dec. 2014.

\bibitem{mainpaper}
H.~Chefer, S.~Gur, and L.~Wolf, ``Transformer interpretability beyond attention
  visualization,'' {\em CoRR}, vol.~abs/2012.09838, 2020.

\bibitem{panoptic}
B.~Cheng, M.~D. Collins, Y.~Zhu, T.~Liu, T.~S. Huang, H.~Adam, and L.~Chen,
  ``Panoptic-deeplab: {A} simple, strong, and fast baseline for bottom-up
  panoptic segmentation,'' {\em CoRR}, vol.~abs/1911.10194, 2019.

\bibitem{ahn2018learning}
J.~Ahn and S.~Kwak, ``Learning pixel-level semantic affinity with image-level
  supervision for weakly supervised semantic segmentation,'' in {\em
  Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, pp.~4981--4990, 2018.

\bibitem{he2017mask}
K.~He, G.~Gkioxari, P.~Doll{\'a}r, and R.~Girshick, ``Mask r-cnn,'' in {\em
  Proceedings of the IEEE international conference on computer vision},
  pp.~2961--2969, 2017.

\bibitem{zhou2016learning}
B.~Zhou, A.~Khosla, A.~Lapedriza, A.~Oliva, and A.~Torralba, ``Learning deep
  features for discriminative localization,'' in {\em Proceedings of the IEEE
  conference on computer vision and pattern recognition}, pp.~2921--2929, 2016.

\bibitem{chang2020weakly}
Y.-T. Chang, Q.~Wang, W.-C. Hung, R.~Piramuthu, Y.-H. Tsai, and M.-H. Yang,
  ``Weakly-supervised semantic segmentation via sub-category exploration,'' in
  {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.~8991--9000, 2020.

\bibitem{selvaraju2017grad}
R.~R. Selvaraju, A.~Das, R.~Vedantam, M.~Cogswell, D.~Parikh, and D.~Batra,
  ``Grad-cam: Why did you say that? visual explanations from deep networks via
  gradient-based localization,'' {\em CoRR}, vol.~abs/1610.02391, 2016.

\bibitem{rise}
V.~Petsiuk, A.~Das, and K.~Saenko, ``{RISE:} randomized input sampling for
  explanation of black-box models,'' {\em CoRR}, vol.~abs/1806.07421, 2018.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' 2017.

\bibitem{guidedtransformer}
H.~Hashemi, H.~Zamani, and W.~B. Croft, ``Guided transformer: Leveraging
  multiple external sources for representation learning in conversational
  search,'' {\em CoRR}, vol.~abs/2006.07548, 2020.

\bibitem{krahenbuhl2011efficient}
P.~Kr{\"a}henb{\"u}hl and V.~Koltun, ``Efficient inference in fully connected
  crfs with gaussian edge potentials,'' {\em Advances in neural information
  processing systems}, vol.~24, pp.~109--117, 2011.

\bibitem{Everingham15}
M.~Everingham, S.~M.~A. Eslami, L.~Van~Gool, C.~K.~I. Williams, J.~Winn, and
  A.~Zisserman, ``The pascal visual object classes challenge: A
  retrospective,'' {\em International Journal of Computer Vision}, vol.~111,
  pp.~98--136, Jan. 2015.

\bibitem{samira2005}
S.~Abnar and W.~H. Zuidema, ``Quantifying attention flow in transformers,''
  {\em CoRR}, vol.~abs/2005.00928, 2020.

\bibitem{binder2016layer}
A.~Binder, G.~Montavon, S.~Lapuschkin, K.-R. M{\"u}ller, and W.~Samek,
  ``Layer-wise relevance propagation for neural networks with local
  renormalization layers,'' in {\em Artificial Neural Networks and Machine
  Learning -- ICANN 2016} (A.~E. Villa, P.~Masulli, and A.~J. Pons~Rivero,
  eds.), (Cham), pp.~63--71, Springer International Publishing, 2016.

\bibitem{voita2019analyzing}
E.~Voita, D.~Talbot, F.~Moiseev, R.~Sennrich, and I.~Titov, ``Analyzing
  multi-head self-attention: Specialized heads do the heavy lifting, the rest
  can be pruned,'' {\em CoRR}, vol.~abs/1905.09418, 2019.

\end{thebibliography}

\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
%\usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
    % \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{graphicx}       % Allows including images
\usepackage{amsmath}
\usepackage{bm}             % bold and italics maths at the same time
\usepackage{amssymb}        % the definition equal triangle symbol
\usepackage{xcolor}         % colors
\usepackage{hyperref}       % links
% \usepackage{babel,blindtext} % test gia figure horizontally
\usepackage{babel} % test gia figure horizontally
\usepackage{float} % fix position

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{amsthm}
\usepackage[dvipsnames]{xcolor}
\usepackage{commath}
\usepackage{subfig}
\usepackage{subfigure}
\usepackage[labelformat=simple]{subcaption}
\usepackage{capt-of}
\usepackage{multirow}
\usepackage{booktabs} % for professional tables

\usepackage{caption}
\usepackage{makecell}

\newcommand\m[1]{\begin{pmatrix}#1\end{pmatrix}} 
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}[theorem]{Lemma}

\hypersetup{                % with colors
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{Weakly-Supervised Semantic Segmentation via Transformer Explainability}
% \\Deep Learning Advanced Course DD2412}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
   Athanasiadis Ioannis, Moschovis Georgios, Tuoma Alexander \\
%   \thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
  Department of Electrical and Computer Engineering\\
  KTH Royal Institute of Technology\\
  Stockholm, SE 11428 \\
  \texttt{\{iath,geomos,tuoma\}@kth.se} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
%  Give an overview of what you have done in the project with the key results and findings of your work. Should be no more than 300 words.

Transformers have been an object of extensive research among deep generative models during the last few years. Precisely, they became a state-of-the-art model for Natural Language Processing (NLP) tasks in 2017 and adopted the mechanism of attention, weighing the influence of different parts of the input data. In this project, we address the use of transformers in a different domain, computer vision, to perform weakly supervised semantic segmentation. Towards this goal, we combine classic back-propagation recipe through the chain rule with relevance propagation, another technique that is based on Deep Taylor Decomposition, to achieve significant performance, comparable to state-of-the-art CNN architectures, using transformers. Last but not least, we incorporate the concept of pixel affinities, which further improves performance in terms of Intersection over Union (IoU). All code used for our experiments is available on GitHub. \footnote{ \color{blue}{\href{https://github.com/athaioan/Transformer-Explainability/tree/main/ours}{https://github.com/athaioan/Transformer-Explainability/tree/main/ours}}.}

\end{abstract}

\section{Introduction}
\label{section:intro}
% Describe the problem, the approach of the paper, the experiments, and the results. At the high-level talk about what you worked on in your project and why it is important. Then give an overview of your results.
One of the most exciting technological aspects nowadays is Machine Learning's mind-blowing potential in transforming the world we live in, mainly due to its exciting resurgence through Deep Learning. However, as machine learning models become more complex, there is a noticeable trade-off between accuracy and simplicity or interpretability \cite{explanations} and plenty of cutting-edge research papers have been published in top-tier conferences related to this tension. In this project, we primarily experimented with Layer-wise Relevance Propagation (LRP), a mechanism of explaining what pixels are relevant within a 2-dimensional image for reaching a classification decision \cite{renormalizationLRP} and applied it to a Vision Transformer [ViT] \cite{visiontransformer}, combined with gradient back-propagation to perform classification but also semantic segmentation on the respective data in ImageNet \cite{russakovsky2015ImageNet, imagenet-seg}, by reproducing the work of Chefer H. et al, in \cite{mainpaper}.

Furthermore, the task of semantic segmentation refers to clustering the pixels of an input image that correspond to the same semantic category. There are various approaches dedicated to this task with the one proposed in \cite{panoptic} being the current state-of-the-art. However, they all rely on training given ground truth segmentation masks. Considering that annotating images in the form of segmentation masks is a rather expensive and tedious process, capitalizing on weak forms of segmentation would be highly beneficial. In order to address these issues, in this project, we investigated using ViT-based explainability as visual seeds to drive the generation of pseudo segmentation masks by computing pixel affinities, following the approach described in \cite{ahn2018learning}. In particular, we trained a Hybrid ViT-base, where the patches are extracted from a CNN feature map, through relevance propagation and used those as seeds to a network computing pixel affinities, in order to improve quality of the generated segmentation masks. 
%An intuitive example of applying our Hybrid ViT-base and our AffinityNet implementations is illustrated in Fig. \ref{fig:fig1}.
% . On the other hand, but also go beyond this and experiment with weakly supervised semantic segmentation using Pascal VOC dataset, by reproducing the work of Jiwoon A. et al, in \cite{affinitypaper}. Our results indicate \textbf{we will see once affinityNet is ready}.

\section{Related Work}
Semantic segmentation has numerous applications, such as self-driving cars or medical image analysis.  Additionally, the evident importance in providing the machines with the ability to perceive the world along with its challenging nature has attracted many researchers to this domain. Many algorithms have been proposed for this task with Mask R-CNN \cite{he2017mask} being among the most frequently employed ones. Although such  approaches can be trained to extract semantic with high precision, they require an extensive amount of semantically annotated training samples. In their work \cite{ahn2018learning}, the authors capitalize on image-level supervision to construct competent pseudo-segmentation masks that can be further utilized to train the segmentation approaches requiring ground truth labels. More specifically, they use class activation mapping (CAM) \cite{zhou2016learning} seeds to model the relation between neighboring pixels, which enables the refinement of the initial CAM cues into segmentation masks of higher quality. Although the previous approach results in relatively accurate segmentation masks, the initial CAMs seeds tend to highlight only the most descriptive part of an instance, which negatively affects the quality of the generated segmentation masks. With the purpose of mitigating this issue, the essayist of \cite{chang2020weakly} employs a sub-category exploration approach. 

Regarding Deep Neural Networks (DNNs) interpretability, various approaches have been proposed in the literature. GradCAM \cite{selvaraju2017grad} is a popular interpretability method applied to various CNN architectures that weighs feature activations in different pixel regions within an image with the average gradient of the class scores. After these gradients are computed through global average pooling, they are passed to a ReLU\footnote{Rectified Linear Units activation function is: $\text{ReLU}(x) = \max\{x,0\}$.} activation function that intensifies pixels contributing towards increasing the target class activation scores. However GradCAM is restricted to CNN architectures. One more general approach is RISE \cite{rise} that measures pixels' importance by applying element-wise multiplications of the original input with a sampled random binary mask to reduce their intensities to zero and only preserve the most important among them. 

Although CNN-based architectures have demonstrated competent performance in a number of vision-related tasks, they come with an increased inductive bias due to the 2D neighboring structure of the images. On the other hand, transformer-based architectures are able to learn spatial relationships detached from the explicit 2D nature of the images. Transformer architecture, since it was proposed in 2017 by Waswani A. et al., \cite{vaswani2017attention} has become very popular in various deep learning domains, and it is based solely on attention mechanisms, dispensing recurrence and convolutions entirely and weighing the influence of different parts of the input data. Following its recent success in NLP, it was recently adopted in computer vision tasks, and in this work, we focus on particularly re-implementing a Vision Transformer [ViT] \cite{visiontransformer} from scratch. Additionally, we employ the explainability cues derived from a image classification ViT to drive the construction of segmentation masks given solely image-level annotation as we explain hereunder.

% Furthermore, the core idea behind interpretability is to specify how a particular model makes decisions, how certain it is about them, whether and to what extent people can trust it, or how users may correct it, especially in mission-critical applications. In this context, interpretability could be beneficial to particular applications of Deep Learning, for instance, in Diagnostic Captioning \cite{dc_pavlopoulos} models, where it is essential for an AI system to explain its generated captions, in the sense of reasoning its decisions to ensure trust with the medical community. The principal goals of interpretable AI boil down to identifying failure modes that enables developers to further improve their systems, establishing the appropriate trust and confidence to users, including the direction of fairness, which requires eliminating the biases to be incorporated in a model, as well as “machine teaching a human” on improving their own decisions \cite{selvaraju2017grad}.




% - Image Classification / CNN, ViT
% - Explainability for visual data (CAM, Grad-CAM, RISE)
% - Explability targeted at ViT (attention mechanism etc.))

% - Semantic Segmentation (Mask RCNN, U-NET)
% - Weakly supervised semantic Segmentation

\section{Methods}

In this section, we describe the methods utilized in our work. Precisely, in subsection \ref{sub:3.1}, we provide details about Vision Transformer architecture. Subsection \ref{sub:3.2} explains how we perform relevance propagation in our model implementations. Finally, in subsection \ref{sub:3.3}, we present the AffinityNet framework modeling the affinity of neighboring pixels.


\subsection{ViT Classification}
\label{sub:3.1}
As mentioned earlier, a Vision Transformer [ViT] \cite{visiontransformer} is an implementation of transformer networks for computer vision tasks. The transformer encoders in ViT are similar to the original transformer architecture introduced in \cite{vaswani2017attention} with slight modifications in the order of operations. Similarly to how a sentence is split into tokens, in ViT we split an image into patches and provide the linearization of the patches representations as input to stacked transformer encoders after adding positional embeddings. Positional embeddings are learned during training; while processing the input patches in given order $x_0, x_1, x_2, ...$ we learn the respective positional embeddings $\hat{x}_0, \hat{x}_1, \hat{x}_2, ...$ for the patches and compute the loss in a backward fashion. The input is then propagated to the attention heads, where multi-head attention is calculated as the concatenation of self-attention scores computed in each head individually as stated in the formulas below:
\begin{align*}
\text{Attention}(Q, K, V) &= \text{softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)V\\
\text{Multihead}(Q, K, V) &= \text{Concat}(\text{head}_1,...\text{head}_h)\Theta^o \\
\text{where head}_i &= \text{Attention}(Q\Theta_i^Q, K\Theta_i^K, V\Theta_i^V)
\end{align*}

Attention is a mechanism for weighting representations learned in a neural network. It is proportional to the respective weights of the network and really flourished within a variety of NLP tasks, where self-attention and multi-head attention became one of the major breakthroughs in sequence modeling tasks precisely \cite{guidedtransformer}. In our implementation, we use ViT-Base, the smallest ViT model variant, which consists of $12$ stacked encoder layers, as well as $12$ attention heads in every layer, as it is illustrated in table \ref{tbl:models}. We use a \texttt{[CLS]} learnable embedding $\mathbf{z}_0^0
= \mathbf{x}_{\text{class}}$ to the sequence of embedded patches, whose state at the output of the Transformer encoder $\mathbf{z}_0^L$, to which a classification head is attached to represent an image $\mathbf{y} = \text{LayerNorm}(\mathbf{z}_0^L)$. We also employ a hybrid architecture, which again consists of a ViT-Base but the patches are extracted from a CNN feature map, while layer normalization is applied before
every block and residual connections after every block in our implementation as it is described in \cite{visiontransformer}.

\begin{table*}[]
\centering
\small
\begin{tabular}{l c c c c c}
\toprule
Model            & Layers & Hidden size $D$ & MLP size &  Heads  & Params \\
\midrule 
ViT-Base   &   12   &        768      &   3072   &   12    &   86M  \\
\bottomrule
\end{tabular}
\caption{Details of ViT model variants. Table extracted from \cite{visiontransformer}.}
\label{tbl:models}
\AND\end{table*}

\subsection{ViT Explainability}
\label{sub:3.2}
As we explained in section \ref{section:intro}, one of our main goals in this project was to apply LRP \cite{renormalizationLRP} to a ViT-Base model \cite{visiontransformer}, combined with classic gradient back-propagation regime to perform classification but also semantic segmentation on the respective data in ImageNet \cite{russakovsky2015ImageNet, imagenet-seg}, by reproducing the work of Chefer H. et al, in \cite{mainpaper}. Considering the input feature map and weights of layer $n$ in form
of tensors, $\mathbf{X}, \mathbf{\Psi}$ we compute the Deep Taylor Decomposition $R_j^{(n)}$ for relevance propagation as formulated below. This expression satisfies the conservation rule that broadly suggests that relevance will be maintained in consecutive layers.
\begin{align}
    \label{eq:relevance}
    \nonumber
    R_j^{(n)} = \mathcal{G}\left(\mathbf{X}, \mathbf{\Psi}, R^{(n-1)}\right)
    &= \sum_{i} \mathbf{X}_j \frac{\partial L_i^{(n)}(\mathbf{X, \Psi})}{\partial\textbf{X}_j}
\end{align}
Moreover, in cases we have two operators (e.g. skip connections and matrix multiplication) the above expression is used for both the input pairs $(u, v)$ and $(v, u)$ to compute $R_j^{u}^{(n)}$ and $R_j^{v}^{(n)}$. Given two such tensors $u$ and $v$, if we add them in layer $n$ the conservation rule is maintained but not in other cases of operations such as matrix multiplication. To address this lack of conservation we normalize the relevances and get $\bar{R}_j^{u}^{(n)}$ and $\bar{R}_j^{v}^{(n)}$ respectively. In addition, there is a special case related to the matrix multiplication operation, where we get two attribution maps for each of the matrices we multiply, and the sum of the relevances of each matrix equals $R$. Furthermore, to actually normalize the CAMs, all we need to do is divide each of them by $2$, which is what the normalization below would do since $R_j^{u}^{(n)}$ and $R_j^{v}^{(n)}$ have identical sums.
\begin{align*}
    R_j^{u}^{(n)} &= \mathcal{G}\left(u, v, R^{(n-1)}\right) \\
    R_j^{v}^{(n)} &= \mathcal{G}\left(v, u, R^{(n-1)}\right) \\
    \bar{R}_j^{u^{(n)}} &= R_j^{u^{(n)}} \frac{ \abs{\sum_j{R_j^{u^{(n)}}}}}{\abs{\sum_j {R_j^{u^{(n)}}}} + \abs{\sum_k{R_k^{v^{(n)}}}}} \cdot \frac{\sum_i R_i^{(n-1)}}{\sum_j R_j^{u^{(n)}}}\\
    \bar{R}_k^{v^{(n)}} &= R_k^{v^{(n)}} \frac{\abs{\sum_k {R_k^{v^{(n)}}}}}{\abs{\sum_j {R_j^{u^{(n)}}}} + \abs{\sum_k {R_k^{v^{(n)}}}}} \cdot \frac{\sum_i R_i^{(n-1)}}{\sum_k R_k^{v^{(n)}}}
\end{align*}
Following the above formulas, we have computed relevances for all layers of our ViT-Base and have implemented relevance propagation, in order to perform semantic segmentation on the ImageNet-segmentation~\cite{imagenet-seg} dataset following the experiments described in \cite{mainpaper}. An example of a CAM generated by our Hybrid ViT-base, where the patches are extracted from a CNN feature map, through relevance propagation is illustrated in Fig. \ref{fig:fig1}(b).

\subsection{AffinityNet}
\label{sub:3.3}
At this stage, we employed the AffinityNet proposed in \cite{ahn2018learning} with the purpose of refining the initially incomplete explainability cues, derived from the Hybrid-ViT network, into segmentation masks of higher quality. In more detail the AffinityNet aims at modelling the relation between adjacent pixels through leveraging the images' feature representation $f^\text{aff}$ and computing the similarity of $i^\text{th}$ and $j^\text{th}$ pixels as:
\begin{align*}
W_{i,j}=\exp\left(-||{f_i^\text{aff}-f_j^\text{aff}}||\right)
\end{align*}
Conceptually, the AffinityNet is trained to predict the inter-pixel semantic affinities, in a class-agnostic manner, by learning to extract meaningful representations for each pixel. Evidently, target labels are required in order to drive the AffinityNet's weights towards accurately predicting the affinities.

\subsubsection{Semantic Affinity Targets}
Training the AffinityNet to model the inter-pixel relationships, requires supervision in the form of segmentation masks. In our scenario, ground truth segmentations labels were not provided and thus the generated ViT explainability seeds are utilized as our best available source of supervision. Admittedly, the generated explainability cues can be quite incomplete and by no means precisely capture the whole instances, however, we can use the most confident pairs in terms of belonging to the same instance. Assuming $C$ classes with $M_{c}$ corresponding to the explainability cue of class $c$, we construct the background activation map $M_{bg}$ as:
\begin{align*}
\label{eq:dropweight}
M_{bg}(x,y) = \left[1 - \max\limits_{c\in C} M_c(x,y)\right]^\alpha
\end{align*}
The parameter $\alpha$ controls how confident the generated background cues are. Intuitively, when the $\alpha$ parameter is relatively high, a pixel of high activation in the $M_{bg}$ would be a strong indication of the pixel belonging to the background category. On the contrary, when the $\alpha$ parameter is relatively low, a high background activation suggests that background is the dominant semantic of that pixel but not with as much confidence. Next, we make use of the common practice of applying dense conditional random fields (dCRF) \cite{krahenbuhl2011efficient} to refine the activation responses for all $C+1$ classes. Applying the dCRF on these classes' activations with the $M_{bg}$ having been derived from a low $\alpha$, favors classifying the pixels as background. On the other hand, when a high $\alpha$ is used, the dCRF is more prone to classifying a pixel as its most activated class. Having said that, applying dCRF on low $\alpha$ gives rise to the confident pixel of foreground instance while on the other, a high $\alpha$ allows for identifying confident background pixels. In our experiments, we set $\alpha_\text{low} = 4$ and $\alpha_\text{high} = 32$ respectively. Below we provide an indicative illustration of confident background and foreground pixels. 

\begin{figure}[H]
    \centering
    \subfigure[]{\includegraphics[width=0.23\textwidth]{img_plane.jpg}} 
    \subfigure[]{\includegraphics[width=0.23\textwidth]{plane_cam.jpg}} 
    \subfigure[]{\includegraphics[width=0.23\textwidth]{low_1.jpg}} 
    \subfigure[]{\includegraphics[width=0.23\textwidth]{high_0.jpg}}
    \caption{(a) Actual image (b) Hybrid-ViT explainability cue for the "Plane" Class (c) dCRF generated confident foreground (d) dCRF generated confident background (The lighter the color intensity the higher the activation).}
    \label{fig:fig1}
\end{figure}

Next, we extract pairs of pixels belonging to the same category with high confidence. Additionally, we also consider as neutral, those pixels that were classified by the dCRF as background in the presence of low $\alpha$ and as foreground in the opposite case. Finally, the construction of confident common-instance pairs is now feasible. We consider pairs of positive and negative affinity, in a class-agnostic manner, while we ignore any pair containing neutral pixels. It is worth highlighting that only neighboring pairs are extracted with a radius of $5$ pixels. An intuitive figure, showcasing the possible affinities is displayed below.

\begin{figure}[!h]   % !h puts the image in the exactly right place
    \centering
    \subfigure[]{\includegraphics[width=0.4\textwidth]{Affinity_graph.png}} 
    \caption{Concept of pixel-to-pixel affinities [image taken from \cite{ahn2018learning}]}
\end{figure}

\subsubsection{Training AffinityNet}
After having generated the explainability-based affinity targets, we can now train a neural network to generate insignificant $W$ values to those pixels that are semantically unrelated. More specifically, we utilized the CNN-backbone as trained in the Hybrid-ViT image classification task for feature representation $f^\text{aff}$ purposes. In order to adapt to affinity-assignment task, we employed two $1\times1$ convolutions on top of the feature map extracted from the Hybrid backbone. The loss used for training the network incorporates three different types of affinities, namely the negative, the foreground positive and background positive affinities. Additionally, we weighted the loss contributions of these three types based on the amount of negative, foreground, and background affinity labels on each training batch. The intuition behind this approach was to avoid only accounting for the most frequent case of background positive relationships due to images containing mostly background content. Based on these the overall loss was computed as :
\begin{align*}
\label{eq:dropweight}
\mathcal{L}_\text{fg}^+ =- \frac{1}{N_\text{fg}^+}\sum_{i,j}\log(W_{i,j})^{I(i,j\in \mathcal{T}_\text{fg}^+)}
\end{align*}
\begin{align*}
\label{eq:dropweight}
\mathcal{L}_\text{bg}^+ =- \frac{1}{N_\text{bg}^+}\sum_{i,j}\log(W_{i,j})^{I(i,j\in \mathcal{T}_\text{bg}^+) }
\end{align*}
\begin{align*}
\label{eq:dropweight}
\mathcal{L}^- =- \frac{1}{N^-}\sum_{i,j}\log(W_{i,j})^{I(i,j\in \mathcal{T}^-) }
\end{align*}
\begin{align*}
\label{eq:dropweight}
\mathcal{L}^- = \mathcal{L}_\text{fg}^+ + \mathcal{L}_\text{bg}^+ + 2\mathcal{L}^-
\end{align*}
with $I$ being the indicator of $i^\text{th}$ and $j^\text{th}$ pixel sharing the target relationship $\mathcal{T}$. Note that the $\mathcal{L}^-$ contributes twice in order avoid unbalance between positive and negative relationships.

\subsubsection{Refining the Explainability seeds}
At this stage, we utilized the predicted pixel-wise affinities to propagate high explainability activations towards the pixels of identical semantic affinity. In more detail, we regarded the predicted affinities as transition probabilities in a random-walk process. By employing this approach, we were able to propagate the highly activated regions based on the semantic relationships predicted from AffinityNet. The transition matrix derives from the predicted affinities as: 
\begin{align*}
\label{eq:dropweight}
T_{rw}=D_w^{-1}W^\omicron{o\beta}
\end{align*}
with $D_w$ being a diagonal array applying row-wise normalization to $W$. Additionally, the $o\beta$ operator is applied so that low transitional probabilities are ignored. Naturally, the \byta hyperparameter has to be an integer value larger than one. Next, we compute the expected transitional probabilities of $t+1$ iterations of the random walk process as:
\begin{align*}
\label{eq:dropweight}
T_{rw}=T_{rw}^t
\end{align*}
Finally, we extract the semantic segmentation masks through refining the explainability seeds $M_c$ for each $c$ class as:
\begin{align*}
\label{eq:dropweight}
\text{vec}(M_c^\text{new})=T_{rw}\text{vec}(M_c)
\end{align*}
with $\text{vec}(.)$ being the array flatten operator. In our experiments, we used values of $16$ and $8$ for the hyperparameters $\beta$ and $t$ respectively.


\section{Experiments}

\subsection{Data}

In this project, two different datasets were used: ImageNet \cite{russakovsky2015ImageNet} (ILSVRC) 2012 along with its mask-annotated ImageNet-Segmentation \cite{imagenet-seg} split and the PASCAL VOC 2012 \cite{Everingham15}. The ImageNet dataset validation split consists of $1000$ object classes with $50.000$ images while the mask-annotated split contain $4.276$ from 445 classes. The PASCAL VOC, considers $20$ image categories with $10.583$ and $1450$ images in the training and the validation split respectively.


\subsection{Transformer Explainability}
As part of replicating the target paper \cite{mainpaper}, we conducted perturbation and segmentation tests, while the results are presented in tables \ref{tab:perturbations} and \ref{tab:segmentation} respectively. For the former type of tests, we use a pre-trained ViT-Base network to extract visualizations for the validation set of ImageNet 2012 ~\cite{russakovsky2015ImageNet}. Afterwards, we gradually mask out the pixels of the input image, from the one with the highest relevance to the one with the lowest when referring to positive perturbation and vice versa in the case of negative perturbation. Consequently, in the first case, we expect to see a high drop in performance when measuring the mean top-1 accuracy of the network while in the second case we expect the overall performance to remain unaffected. Regarding the latter type of tests, we consider each visualization as a soft segmentation of the image and compare it to the ground truth segmentation mask of the ImageNet segmentation dataset\footnote{ImageNet segmentation dataset was obtained from \href{http://calvin-vision.net/bigstuff/proj-imagenet/data/gtsegs_ijcv.mat}{calvin-vision.net}.}.  In 
table \ref{tab:perturbations} we report the AUC metric for the perturbation tests considering the explainability cues corresponding to both the most confident (predicted) and the ground truth class (target). Additionally, in table \ref{tab:segmentation} we evaluate the segmentation quality of the extracted cues by comparing them with the provided ground truth segmentation masks. In Appendix \ref{ImageNet_results} we provide qualitative results corresponding to explainability cues in ImageNet; generated using our ViT-Base implementation.


\setlength\tabcolsep{2pt}
\begin{table*}[!h]
    \centering
    % \begin{center}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llccccccc}
        \toprule
        &&rollout & raw  & GradCAM & LRP & partial LRP & Target paper & Ours\\
        & &~\cite{samira2005} & attention &~\cite{selvaraju2017grad} &~\cite{binder2016layer} &~\cite{voita2019analyzing} &~\cite{mainpaper} & \\
        \midrule
        \multirow{2}{*}{Negative} &Predicted & 53.1 & 45.55 & 41.52 & 43.49 & 50.49 & \textbf{54.16} & 54.13\\
        &Target & - & - & 42.02 & 43.49 & 50.49 & \textbf{55.04} & 55.03 \\
        \midrule
        \multirow{2}{*}{Positive} &Predicted & 20.05 & 23.99 & 34.06 & 41.94 & 19.64 & \textbf{17.03} & \textbf{17.03}\\
        &Target & - & - & 33.56 & 41.93 & 19.64 & \textbf{16.04} & 16.38\\
        \bottomrule
    \end{tabular*}
    % \end{center}
    \caption{Positive and Negative perturbation AUC results (percents) for the predicted and target classes, on the ImageNet~\cite{russakovsky2015ImageNet} validation set. For positive perturbation lower is better, and for negative perturbation higher is better. Table partly extracted from \cite{mainpaper}.}
    \label{tab:perturbations}
\end{table*}
    \medskip

\begin{table*}[!h]
    % \begin{center}
    % \centering
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lccccccc}
        \toprule
        &rollout & raw  & GradCAM & LRP & partial LRP & Target paper & Ours\\
        &~\cite{samira2005} & attention &~\cite{selvaraju2017grad} &~\cite{binder2016layer} &~\cite{voita2019analyzing} &~\cite{mainpaper} & \\
        \midrule
        pixel accuracy & 73.54 & 67.84 & 64.44 & 51.09 & 76.31 & 79.70 & \textbf{79.73}\\
        mAP & 84.76 & 80.24 & 71.60 & 55.68 & 84.67 & \textbf{86.03} & \textbf{86.03}\\
        mIoU & 55.42 & 46.37 & 40.82 & 32.89 & 57.94 & 61.95 & \textbf{62.01}\\
        \bottomrule
    \end{tabular*}
    % \end{center}
    \caption{Segmentation performance on the ImageNet-segmentation~\cite{imagenet-seg} dataset (percent). Higher is better. Table partly extracted from \cite{mainpaper}.}
    \label{tab:segmentation}
\end{table*}

\subsection{AffinityNet by ViT explainability}
For the purpose of generating competent segmentation masks given only image-level supervision, we relied on AffinityNet to refine the initially incomplete explainability cues derived from the Hybrid-ViT image classification network. We evaluated the class-wise mIoU in the PASCAL VOC validation dataset in table \ref{tab:Affinity} where we compare the mIoU performance of the explainability cues prior and post employing the AffinityNet-based refinement \cite{ahn2018learning}. In Appendix \ref{Pascal_results} we provide qualitative results corresponding to the refinement of the ViT-derived explainability cues via the AffinityNet.


\begin{table*}[!h]
    % \begin{center}
    % \centering
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcccc}
        \toprule
        &CAM \cite{zhou2016learning} & AffinityNet \cite{ahn2018learning} & Ours & Ours AffinityNet \\
        &[VGG-16] & [VGG-16] & [ViT-Hybrid] & [ViT-Hybrid] \\  

        
        \midrule
        mIoU & 46.60 & \textbf{54.00} & 44.60 & 50.90\\
        \bottomrule
    \end{tabular*}
    % \end{center}
    \caption{Segmentation performance on the Pascal VOC segmentation \cite{Everingham15} dataset (percent). Higher is better.}
    \label{tab:Affinity}
\end{table*}


\subsection{Implementation Details}
Regarding the replication of paper \cite{mainpaper}, no training was required as we relied on the available ViT weights pre-trained on the ImageNet dataset. When it comes to utilizing explainability cues derived from ViT architectures for training the AffinityNet, we trained a hybrid-ViT architecture on PascalVoc while capitalizing on the weights as pretrained on ImageNet. More specifically, we trained for $20$ epochs with a learning rate $5e-3$. The AffinityNet was trained on Pascal VOC training split for $7$ epochs with a learning rate of $0.1$ using the affinity labels as generated by the ViT explainability cues. In both these training setups, the batch size was set to $8$, the weight decay to $1e-4$ while the SGD optimizer was used. Finally, during training, images were resized to $244\times244$ and $448\times448$ resolution for ImageNet and Pascal VOC respectively. Moreover, the images were normalized to have $0.5$ mean and $0.5$ standard deviation for all channels while  random horizontal flip and color jittering were employed for data augmentation purposes.


\section{Conclusions}
% Summarize your key results - what have you learned? What points do you think one should consider when using the approach of the paper you chose for your project? Suggest ideas for future extensions or new applications of your ideas.

% In this project, we investigated how the ViT architecture can be used for weakly supervised semantic segmentation, combining ideas from previous works as \cite{mainpaper, ahn2018learning, visiontransformer}, to name a few. From our results, we undoubtedly realized the importance of pre-training and knowledge transfer, as we relied on pre-trained weights for all our models, ViT-base, Hybrid ViT-Base and AffinityNet, as well as the beneficial role of producing already high-quality CAMs through our Hybrid ViT-Base model, which were improved after discovering pixel affinities through our AffinityNet implementation, highlighting the success of a proposed model. With our best performing architecture, the Hybrid ViT-Base + Affinity model performs $\~6\%$  better than the baseline (Hybrid ViT-Base) model as trained on Pascal-VOC dataset, see Table \ref{tab:Affinity}. 


% In this work, we conclude that the main results from \cite{mainpaper} for the computer vision part is qualitatively reproducible. We combined the approach with AffinityNet \cite{ahn2018learning} to improve the performance of weakly supervised semantic segmentation in terms of IoU. According to the table \ref{tab:Affinity}, we observe that the performance we achieved is lower compared to the one reported in \cite{ahn2018learning}. One reason for that could be the lower quality of ViT explainability seeds compared to the CNN-based ones. For future work, we would like to use the approach in other domains such as video, and reinforcement learning.


%%%% 
In the context of this study, we replicated the ViT explainability approach proposed in \cite{mainpaper}. Additionally, we capitalized on the explainability seeds derived from a Hybrid-ViT architecture to generate competent semantic segmentation labels for weak-supervision. More specifically, the AffinityNet \cite{ahn2018learning} was employed with the purpose of refining the initially incomplete explainability cues into segmentation masks of higher quality. The quantitative results provided in tables \ref{tab:perturbations} and table \ref{tab:segmentation} indicate that we have successfully implemented the explainability method described in \cite{mainpaper} since our results are identical to those originally reported in the latter for all the considered metrics. Regarding the AffinityNet, we evaluated the class-wise mIoU performance that we have achieved based on the explainability seeds as generated by the Hybrid-ViT architecture. 

Furthermore, according to table \ref{tab:Affinity}, we observe that the performance we achieved is lower compared to the one reported in \cite{ahn2018learning}, however segmentation masks of improved quality were generated. One reason for that could be the lower quality of ViT explainability seeds compared to the CNN-based ones. Another potential reason for the lacking performance of the AffinityNet, when given explainability cues from ViT architecture, could be that the feature map $f^\text{aff}$ in our case, derives from low-level image representation where as in the original paper \cite{ahn2018learning} feature representation from multiple levels were aggregated. Such multi-level aggregation was not feasible in our scenario due to the nature of the transformer architecture. 

Concluding, in this work we have demonstrated the feasibility of using ViT-derived explainability cues with the purpose of training the AffinityNet. Although, we were able to increase the quality of the ViT explainability cues by refining them with the AffinityNet, the CNN-based architectures perform better while using lighter models.



\section{Ethical consideration, societal impact, alignment with UN SDG targets}
%Think and research! Are there any ethical considerations for the original paper, its problem or method, its way of conducting experiments? How about the experiments you did? What societal impact can you imagine about the original paper and its contributions and results? How about your project report? How do you think this paper can push the UN SDG targets?

Explainability for deep learning models could have both positive and negative societal impacts. Our work, for example, may be used for analyzing medical image data or understanding climate models for predicting future climate patterns. This is related to the $3^\text{rd}$ goal of United Nations Sustainability Goals (UNSG) on ensuring good health and well-being and the $16^\text{th}$ goal to combat climate change. It might also be possible to use this method to detect and remove bias in datasets, which relates to the $5^\text{th}$ goal to achieve gender equality. On the other hand, weapon industry can highly benefit from training automated system to perform high quality target detection through levaraging the concept of combining ViT-explanability and weakly supervision. The aforementioned scenario would negatively impact the sustainability goal for promoting peace.

\newpage
\section{Appendix}
\subsection*{Qualitative Results - ImageNet}
\label{ImageNet_results}

\begin{figure}[H]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{gt_bug.png}
  \caption{Image of a bug from ImageNet \\segmentation dataset \cite{imagenet-seg}.}\label{fig:gt_bug}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{heat_map_bug.png}
  \caption{Segmentation map generated by our ViT- \\base for the bug image.}\label{fig:map_bug}
\endminipage\hfill
\end{figure}

\begin{figure}[H]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{gt_cow.png}
  \caption{Image of a cow from ImageNet \\segmentation dataset \cite{imagenet-seg}.}\label{fig:gt_cow}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{heat_map_cow.png}
  \caption{Segmentation map generated by our ViT- \\base for the cow image.}\label{fig:map_cow}
\endminipage\hfill
\end{figure}

% \begin{figure}[!h]
% \minipage{0.5\textwidth}
%   \includegraphics[width=\linewidth]{gt_dog.png}
%   \caption{Ground truth segmentation map for \\a dog image from ImageNet \cite{imagenet-seg}.}\label{fig:gt_dog}
% \endminipage\hfill
% \minipage{0.5\textwidth}
%   \includegraphics[width=\linewidth]{heat_map_dog.png}
%   \caption{Segmentation map generated by our ViT- \\base for the dog image.}\label{fig:map_dog}
% \endminipage\hfill
% \end{figure}

\begin{figure}[H]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{gt_reinder.png}
  \caption{Image of a reindeer from ImageNet \\segmentation dataset \cite{imagenet-seg}.}\label{fig:gt_reinder}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{heat_map_reinder.png}
  \caption{Segmentation map generated by our \\ViT-base for the reindeer image.}\label{fig:map_reinder}
\endminipage\hfill
\end{figure}

\begin{figure}[H]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{gt_sheep.png}
  \caption{Image of a sheep from ImageNet \\segmentation dataset \cite{imagenet-seg}.}\label{fig:gt_sheep}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{heat_map_sheep.png}
  \caption{Segmentation map generated by our \\ViT-base for the sheep image.}\label{fig:map_sheep}
\endminipage\hfill
\end{figure}

\begin{figure}[H]
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{gt_squirel.png}
  \caption{Image of a squirrel from ImageNet \\segmentation dataset \cite{imagenet-seg}.}\label{fig:gt_squirel}
\endminipage\hfill
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{heat_map_squirel.png}
  \caption{Segmentation map generated by our \\ViT-base for the squirrel image.}\label{fig:map_squirel}
\endminipage\hfill
\end{figure}

\subsection*{Qualitative Results - Pascal VOC}
\label{Pascal_results}

\begin{figure}[H]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{airplane_gt_affinity.png}
  \caption{Image of an airplane from Pascal VOC segmentation \\dataset \cite{ahn2018learning}.}
  \label{fig:plane_gt}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{airplane_heat_map.png}
  \caption{Segmentation map generated by our ViT-base for the airplane image.}
  \label{fig:phane_map}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{plane_affinity.png}
  \caption{Affinity map generated by our AffinityNet for the airplane image.}
  \label{fig:plane_aff}
\endminipage
\end{figure}

\begin{figure}[H]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{screen_gt.png}
  \caption{Image of an screen from Pascal VOC segmentation \\dataset \cite{ahn2018learning}.}
  \label{fig:screen_gt}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{screen_heat_map.png}
  \caption{Segmentation map generated by our ViT-base for the screen image.}
  \label{fig:screen_map}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{screen_affinity.png}
  \caption{Affinity map generated by our AffinityNet for the screen image.}
  \label{fig:screen_aff}
\endminipage
\end{figure}

\begin{figure}[H]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{sheep_gt.png}
  \caption{Image of a sheep from Pascal VOC segmentation \\dataset \cite{ahn2018learning}.}
  \label{fig:sheep_gt}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{sheep_heat_map.png}
  \caption{Segmentation map generated by our ViT-base for the sheep image.}
  \label{fig:sheep_heat_map}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{sheep_affinity.png}
  \caption{Affinity map generated by our AffinityNet for the sheep image.}
  \label{fig:sheep_aff}
\endminipage
\end{figure}

\begin{figure}[H]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{train_gt.png}
  \caption{Image of a train from Pascal VOC segmentation \\dataset \cite{ahn2018learning}.}
  \label{fig:train_gt}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{train_heat_map.png}
  \caption{Segmentation map generated by our ViT-base for the train image.}
  \label{fig:train_heat_map}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{train_affinity.png}
  \caption{Affinity map generated by our AffinityNet for the train image.}
  \label{fig:train_aff}
\endminipage
\end{figure}

\subsection*{Self Assessment}

%Use the Project Grading Guideline again and assess your project along those guidelines. Argue what grade you think your project deserves and why.
We believe that our project is worth the \textit{A} grade since Transformer Networks are hard to be integrated as part of a larger architecture, as well as trained, though we achieved both.  We combined knowledge gained during the project sessions and previous knowledge and developed a new model architecture from scratch; combining ideas from the target paper \cite{mainpaper}, but also implementing a ViT-base, as well as developing a Hybrid-ViT network and implementing the AffinityNet \cite{ahn2018learning} by ourselves as well to further improve the performance of weakly supervised semantic segmentation in terms of IoU as it is highlighed in table \ref{tab:Affinity}.

% We believe the fair grade to award to our project is the maximum (A).

Precisely, we successfully managed to re-implement relatively advanced concepts, LRP, ViT and AffinityNet, and apart from replicating the work presented in \cite{mainpaper} (perturbation and segmentation tests) we furthermore implemented a Hybrid ViT-Base architecture and the AffinityNet presented in \cite{ahn2018learning} resulting in higher quality of cams, as can be illustrated for example in Fig. \ref{fig:train_gt}, \ref{fig:train_heat_map}, \ref{fig:train_aff}. Finally, considering the difficulty entailed in re-implementing and combining ViT interpretability and weakly supervised semantic segmentation, we strongly believe that the project is well-suited for the targeted grade.

% In the feedback that we received on our project proposal, the TA Federico stated that if we implement everything as planned, which we did, that will suffice for the highest grade (A). 

% On top of that, we re-implemented a ViT-base model from scratch and a Hybrid-ViT network that we used for our experiments on multiple datasets. By combining the Hybrid-Vit network and the AffinityNet \cite{ahn2018learning}, which we re-implemented from scratch, we improved the performance of weakly supervised semantic segmentation in terms of IoU, illustrated in table \ref{tab:Affinity}.

% Finally, we think that our report is well written with a thorough related work and methods section, which indicated that we had reviewed the subject area we investigated in depth.





% We have shared our ideas with TA Federico and he has %confirmed our claim as well.


%e.g. replicated two heavy papers
%e.g. ran experiments on two different datasets etc.


% \section{Methods   --- old kept as template}

% \subsection{Loss function}
% Consider a dataset $\mathbf{x} = \{\mathbf{x}_i\}_{i=1}^N$ where we suppose $\mathbf{x} \in \mathbb{R}^D $ is a vector of $D$ observable variables and $ \mathbf{z} \in \mathbb{R}^M $ a vector of stochastic latent variables. Our goal is to maximize the marginal log-likelihood $\log p(\mathbf{x})$ and since the probabilistic encoder $q_\phi(\mathbf{z}|\mathbf{x})$ and the generative decoder $p_\theta(\mathbf{x}|\mathbf{z})$ are neural networks with weights $\phi$ and $\theta$ respectively, we may use Variational Inference in order to overcome the problem of $\log p(\mathbf{x})$ being intractable. Consequently, we ought to optimize the \textit{variational lower bound} objective given using Jensen inequality and logarithm properties:
% \begin{align*}
% \log p(\mathbf{x}) &= \int p_{\theta}(\mathbf{x|z}) p_{\lambda}(\mathbf{z}) \,d\mathbf{z} \\
% % &= \log\int \dfrac{q_{\phi}(\mathbf{z|x})}{q_{\phi}(\mathbf{z|x})} \times p_{\theta}(\mathbf{x|z}) \times p_{\lambda}(\mathbf{z}) \,d\mathbf{z} \\
% &\geq \int q_{\phi}(\mathbf{z|x})\log\dfrac{p_{\theta}(\mathbf{x|z}) p_{\lambda}(\mathbf{z})}{q_{\phi}(\mathbf{z|x})} \,d\mathbf{z} \\
% % &= \int q_{\phi}(\mathbf{z|x})\left[\log\dfrac{p_{\lambda}(\mathbf{z})}{q_{\phi}(\mathbf{z|x})} + \log p_{\theta}(\mathbf{x|z})\right] \,d\mathbf{z} \\
% % &= \int q_{\phi}(\mathbf{z|x})\left[-\log\dfrac{q_{\phi}(\mathbf{z|x})}{p_{\lambda}(\mathbf{z})} + \log p_{\theta}(\mathbf{x|z})\right] \,d\mathbf{z} \\
% % &= \int \mathbb{E}_{q_{\phi}(\mathbf{z|x})}\left[-\log\dfrac{q_{\phi}(\mathbf{z|x})}{p_{\lambda}(\mathbf{z})} + \log p_{\theta}(\mathbf{x|z})\right] \,d\mathbf{z} \\
% &= \int -q_{\phi}(\mathbf{z|x})\log\dfrac{q_{\phi}(\mathbf{z|x})}{p_{\lambda}(\mathbf{z})} + q_{\phi}(\mathbf{z|x}) \log p_{\theta}(\mathbf{x|z}) \,d\mathbf{z} \\
% &= -KL[q_{\phi}(\mathbf{z|x})||p_{\lambda}(\mathbf{z})] + \mathbb{E}_{z \sim q_{\phi}(\mathbf{z|x})}[\log p_{\theta}(\mathbf{x|z})]
% \end{align*}

% Furthermore, concerning the expectation over $q_\phi(\mathbf{x}|\mathbf{z})$, we consider a Monte Carlo estimate using $L$ sample points. Under this assumption, we need to reformulate the above lower bound and starting from the Kullback-Leibler divergence, it can be approximated via Monte-Carlo (MC) estimation by $D_{MC, L}$, which correctly approximates the $D_{KL} \triangleq KL[q_{\phi}(\mathbf{z|x})||p_{\lambda}(\mathbf{z})]$ according to the law of large numbers as $L$ tends to infinity$^{\text{[5]}}$: 
% \begin{align*}
% -D_{KL} \triangleq -KL[q_{\phi}(\mathbf{z|x})||p_{\lambda}(\mathbf{z})]&= \int -q_{\phi}(\mathbf{z|x})\log\dfrac{q_{\phi}(\mathbf{z|x})}{p_{\lambda}(\mathbf{z})} \,d\mathbf{z} \\
% &= \int \mathbb{E}_{q_{\phi}(\mathbf{z|x})}\left[-\log\dfrac{q_{\phi}(\mathbf{z|x})}{p_{\lambda}(\mathbf{z})}\right] \,d\mathbf{z} 
% \end{align*}

% \begin{align*}
% -D_{MC, L} &= \dfrac{1}{L}\int-\log\dfrac{q_{\phi}(\mathbf{z|x})}{p_{\lambda}(\mathbf{z})} \,d\mathbf{z} \\
% &=  \dfrac{1}{L}\int-\log q_{\phi}(\mathbf{z|x})+\log p_{\lambda}(\mathbf{z}) \,d\mathbf{z} \\
% &= \dfrac{1}{L}\int-\log \mathcal{N}(\mathbf{z}; \mu, \sigma^2)+\log \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{I}) \,d\mathbf{z} \\
% % &= \dfrac{1}{L} \left[\dfrac{L}{2}\log(2\pi) + \sum_{i=1}^{L}\dfrac{\log(\sigma_i^2)}{2} + \sum_{i=1}^{L}\dfrac{(z_{\phi}^{(i)} - \mu)^2}{2\sigma_i^2} - \dfrac{L}{2}\log(2\pi) - \sum_{i=1}^{L}\dfrac{(z_{\phi}^{(i)})^2}{2}\right] \\
% % &= \dfrac{1}{L} \left[\sum_{i=1}^{L}\dfrac{\log(\sigma_i^2)}{2} + \sum_{i=1}^{L}\dfrac{(z_{\phi}^{(i)} - \mu)^2}{2\sigma_i^2} - \sum_{i=1}^{L}\dfrac{(z_{\phi}^{(i)})^2}{2}\right] \\
% &= \dfrac{1}{2L}\sum_{i=1}^{L}\log(\sigma_i^2) + \dfrac{(z_{\phi}^{(i)} - \mu)^2}{\sigma_i^2} - (z_{\phi}^{(i)})^2
% \end{align*}


\bibliographystyle{ieeetr}
\bibliography{reflist}

\end{document}
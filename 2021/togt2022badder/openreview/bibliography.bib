

@misc{honnibalSpaCyIndustrialstrengthNatural2020,
  title = {{{spaCy}}: {{Industrial-strength Natural Language Processing}} in {{Python}}},
  shorttitle = {{{spaCy}}},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  year = {2020},
  doi = {10.5281/zenodo.1212303},
  copyright = {MIT}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}



@inproceedings{antoniak-mimno-2021-bad,
    title = "Bad Seeds: Evaluating Lexical Methods for Bias Measurement",
    author = "Antoniak, Maria  and
      Mimno, David",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.148",
    doi = "10.18653/v1/2021.acl-long.148",
    pages = "1889--1904",
    abstract = "A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements.",
}


@inproceedings{joseph_girls_2017,
	address = {New York, NY, USA},
	series = {{CSCW} '17},
	title = {Girls {Rule}, {Boys} {Drool}: {Extracting} {Semantic} and {Affective} {Stereotypes} from {Twitter}},
	isbn = {978-1-4503-4335-0},
	shorttitle = {Girls {Rule}, {Boys} {Drool}},
	url = {https://doi.org/10.1145/2998181.2998187},
	doi = {10.1145/2998181.2998187},
	abstract = {Social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. In the present work, we develop a method to extract the stereotypes of Twitter users. Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities' affective meanings and the other that represents stereotypes as semantic relationships between identities. After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies. Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.},
	urldate = {2022-01-27},
	booktitle = {Proceedings of the 2017 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Joseph, Kenneth and Wei, Wei and Carley, Kathleen M.},
	month = feb,
	year = {2017},
	keywords = {computational social science, identity, social psychology, stereotype, twitter},
	pages = {1362--1374},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/W6CMLXBK/Joseph et al. - 2017 - Girls Rule, Boys Drool Extracting Semantic and Af.pdf:application/pdf},
}

@article{ethayarajh2019understanding,
  title={Understanding undesirable word embedding associations},
  author={Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
  journal={arXiv preprint arXiv:1908.06361},
  year={2019}
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	copyright = {Copyright © 2017, American Association for the Advancement of Science},
	url = {https://www.science.org/doi/abs/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	abstract = {Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.},
	language = {EN},
	urldate = {2022-01-13},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Accepted Version:/home/uni/Zotero/storage/CHYGWQ8K/Caliskan et al. - 2017 - Semantics derived automatically from language corp.pdf:application/pdf},
}

@inproceedings{ribeiro_why_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://doi.org/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2022-01-13},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
	pages = {1135--1144},
}


@inproceedings{bolukbasi_man_2016,
  title = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  shorttitle = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/thesofakillers/Zotero/storage/DJFID7V3/Paper 2152CRv3_supp.pdf;/Users/thesofakillers/Zotero/storage/FXNMS5MU/Bolukbasi et al_2016_Man is to Computer Programmer as Woman is to Homemaker.pdf}
}


@article{narayanan_semantics_nodate,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	url = {https://core.ac.uk/reader/161916836?utm_source=linkout},
	urldate = {2022-01-13},
	author = {Narayanan, Arvind},
	file = {Full Text PDF:/home/uni/Zotero/storage/YJKRIZWJ/Narayanan - Semantics derived automatically from language corp.pdf:application/pdf;Snapshot:/home/uni/Zotero/storage/TLZIXHDG/161916836.html:text/html},
}

@inproceedings{rudinger_social_2017,
	address = {Valencia, Spain},
	title = {Social {Bias} in {Elicited} {Natural} {Language} {Inferences}},
	url = {http://aclweb.org/anthology/W17-1609},
	doi = {10.18653/v1/W17-1609},
	abstract = {We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The human-elicitation protocol employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.},
	language = {en},
	urldate = {2022-01-18},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rudinger, Rachel and May, Chandler and Van Durme, Benjamin},
	year = {2017},
	pages = {74--79},
	file = {Rudinger et al. - 2017 - Social Bias in Elicited Natural Language Inference.pdf:/home/uni/Zotero/storage/XHZJTM2W/Rudinger et al. - 2017 - Social Bias in Elicited Natural Language Inference.pdf:application/pdf},
}

@inproceedings{manzini_black_2019,
	address = {Minneapolis, Minnesota},
	title = {Black is to {Criminal} as {Caucasian} is to {Police}: {Detecting} and {Removing} {Multiclass} {Bias} in {Word} {Embeddings}},
	shorttitle = {Black is to {Criminal} as {Caucasian} is to {Police}},
	url = {http://aclweb.org/anthology/N19-1062},
	doi = {10.18653/v1/N19-1062},
	abstract = {Online texts—across genres, registers, domains, and styles—are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting, such as binary gender. Next, we propose a novel methodology for the evaluation of multiclass debiasing. We demonstrate that our multiclass debiasing is robust and maintains the efﬁcacy in standard NLP tasks.},
	language = {en},
	urldate = {2022-01-18},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Manzini, Thomas and Yao Chong, Lim and Black, Alan W and Tsvetkov, Yulia},
	year = {2019},
	pages = {615--621},
	file = {Manzini et al. - 2019 - Black is to Criminal as Caucasian is to Police De.pdf:/home/uni/Zotero/storage/5N3ILQ5V/Manzini et al. - 2019 - Black is to Criminal as Caucasian is to Police De.pdf:application/pdf},
}

@incollection{ahmad_fairness_2020,
	address = {New York, NY, USA},
	title = {Fairness in {Machine} {Learning} for {Healthcare}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3406461},
	abstract = {The issue of bias and fairness in healthcare has been around for centuries. With the integration of AI in healthcare the potential to discriminate and perpetuate unfair and biased practices in healthcare increases many folds The tutorial focuses on the challenges, requirements and opportunities in the area of fairness in healthcare AI and the various nuances associated with it. The problem healthcare as a multi-faceted systems level problem that necessitates careful of different notions of fairness in healthcare to corresponding concepts in machine learning is elucidated via different real world examples.},
	urldate = {2022-01-25},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ahmad, Muhammad Aurangzeb and Patel, Arpit and Eckert, Carly and Kumar, Vikas and Teredesai, Ankur},
	month = aug,
	year = {2020},
	keywords = {fate ml, fatml, healthcare ai, machine learning in healthcare, fairness},
	pages = {3529--3530},
	file = {Full Text PDF:/home/uni/Zotero/storage/DWHFSEUJ/Ahmad et al. - 2020 - Fairness in Machine Learning for Healthcare.pdf:application/pdf},
}

@article{kusner_counterfactual_nodate,
	title = {Counterfactual {Fairness}},
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our deﬁnition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	language = {en},
	author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	pages = {11},
	file = {Kusner et al. - Counterfactual Fairness.pdf:/home/uni/Zotero/storage/IV43I3AC/Kusner et al. - Counterfactual Fairness.pdf:application/pdf},
}

@inproceedings{kasirzadeh_use_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {The {Use} and {Misuse} of {Counterfactuals} in {Ethical} {Machine} {Learning}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445886},
	doi = {10.1145/3442188.3445886},
	abstract = {The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.},
	urldate = {2022-01-25},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Kasirzadeh, Atoosa and Smart, Andrew},
	month = mar,
	year = {2021},
	keywords = {Algorithmic Fairness, Counterfactuals, Ethical AI, Ethics of AI, Explainable AI, Explanation, Fairness, Machine learning, Philosophy, Philosophy of AI, Social category, Social kind, Social ontology},
	pages = {228--236},
	file = {Full Text PDF:/home/uni/Zotero/storage/78R82SD6/Kasirzadeh and Smart - 2021 - The Use and Misuse of Counterfactuals in Ethical M.pdf:application/pdf},
}

@article{DBLP:journals/corr/MerityXBS16,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  journal   = {CoRR},
  volume    = {abs/1609.07843},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07843},
  eprinttype = {arXiv},
  eprint    = {1609.07843},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MerityXBS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3240323.3240369,
author = {Wan, Mengting and McAuley, Julian},
title = {Item Recommendation on Monotonic Behavior Chains},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240369},
doi = {10.1145/3240323.3240369},
abstract = { 'Explicit' and 'implicit' feedback in recommender systems have been studied for many years, as two relatively isolated areas. However many real-world systems involve a spectrum of both implicit and explicit signals, ranging from clicks and purchases, to ratings and reviews. A natural question is whether implicit signals (which are dense but noisy) might help to predict explicit signals (which are sparse but reliable), or vice versa. Thus in this paper, we propose an item recommendation framework which jointly models this full spectrum of interactions. Our main observation is that in many settings, feedback signals exhibit monotonic dependency structures, i.e., any signal necessarily implies the presence of a weaker (or more implicit) signal (a 'review' action implies a 'purchase' action, which implies a 'click' action, etc.). We refer to these structures as 'monotonic behavior chains,' for which we develop new algorithms that exploit these dependencies. Using several new and existing datasets that exhibit a variety of feedback types, we demonstrate the quantitative performance of our approaches. We also perform qualitative analysis to uncover the relationships between different stages of implicit vs. explicit signals.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {86–94},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{wan-etal-2019-fine,
    title = "Fine-Grained Spoiler Detection from Large-Scale Review Corpora",
    author = "Wan, Mengting  and
      Misra, Rishabh  and
      Nakashole, Ndapa  and
      McAuley, Julian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1248",
    doi = "10.18653/v1/P19-1248",
    pages = "2605--2610",
    abstract = "This paper presents computational approaches for automatically detecting critical plot twists in reviews of media products. First, we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level, as well as book and (anonymized) user information. Second, we carefully analyzed this dataset, and found that: spoiler language tends to be book-specific; spoiler distributions vary greatly across books and review authors; and spoiler sentences tend to jointly appear in the latter part of reviews. Third, inspired by these findings, we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora. Quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines.",
}

@inproceedings{rehurek2010software,
  title={Software framework for topic modelling with large corpora},
  author={Rehurek, Radim and Sojka, Petr},
  booktitle={In Proceedings of the LREC 2010 workshop on new challenges for NLP frameworks},
  year={2010},
  organization={Citeseer}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}


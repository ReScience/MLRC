@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={60},
  year={2019},
  publisher={Springer}
}

@misc{sener2018active,
      title={Active Learning for Convolutional Neural Networks: A Core-Set Approach}, 
      author={Ozan Sener and Silvio Savarese},
      year={2018},
      eprint={1708.00489},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{ash2020deep,
      title={Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds}, 
      author={Jordan T. Ash and Chicheng Zhang and Akshay Krishnamurthy and John Langford and Alekh Agarwal},
      year={2020},
      eprint={1906.03671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{original-paper,
 author = {Ash, Jordan and Adams, Ryan P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {3884--3894},
 publisher = {Curran Associates, Inc.},
 title = {On Warm-Starting Neural Network Training},
 url = {https://proceedings.neurips.cc/paper/2020/file/288cd2567953f06e460a33951f55daaf-Paper.pdf},
 volume = {33},
 year = {2020}
}




@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {9781467388511},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	urldate = {2020-12-16},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@inproceedings{kingma_adam:_2015,
	author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://proceedings.mlr.press/v37/ioffe15.html},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layerâ€™s inputs changes during training, as the parameters of the previous layers change. This slows down the t...},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = jun,
	year = {2015},
	pages = {448--456},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2020-12-16},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
}

@article{hinton_learning_2007,
	title = {Learning multiple layers of representation},
	volume = {11},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661307002173},
	doi = {10.1016/j.tics.2007.09.004},
	language = {en},
	number = {10},
	urldate = {2020-12-16},
	journal = {Trends in Cognitive Sciences},
	author = {Hinton, Geoffrey E.},
	month = oct,
	year = {2007},
	pages = {428--434},
}

@inproceedings{oquab_learning_2014,
	address = {Columbus, OH, USA},
	title = {Learning and {Transferring} {Mid}-level {Image} {Representations} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {9781479951185},
	url = {https://ieeexplore.ieee.org/document/6909618},
	doi = {10.1109/CVPR.2014.222},
	urldate = {2020-12-16},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
	month = jun,
	year = {2014},
	pages = {1717--1724},
}

@inproceedings{huang_cross-language_2013,
	title = {Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers},
	doi = {10.1109/ICASSP.2013.6639081},
	abstract = {In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5\%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6\% to 28\% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Huang, J. and Li, J. and Yu, D. and Deng, L. and Gong, Y.},
	month = may,
	year = {2013},
	note = {ISSN: 2379-190X},
	keywords = {hidden Markov models, linguistics, neural nets, speech recognition, DNN, error reductions, recognition accuracy, learned hidden layers, language specific data, log-linear classifier, complex feature transformations, multilingual deep neural network, cross-language knowledge transfer, Training, Training data, Speech recognition, Neural networks, Acoustics, Speech, Hidden Markov models, deep neural network, CD-DNN-HMM, multilingual speech recognition, multitask learning, transfer learning},
	pages = {7304--7308},
}

@inproceedings{long_unsupervised_2016,
	address = {Barcelona, Spain},
	series = {{NIPS}'16},
	title = {Unsupervised domain adaptation with residual transfer networks},
	isbn = {9781510838819},
	abstract = {The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.},
	urldate = {2020-12-15},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
	month = dec,
	year = {2016},
	pages = {136--144},
}

@inproceedings{svhn,
title	= {Reading Digits in Natural Images with Unsupervised Feature Learning},
author	= {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
year	= {2011},
URL	= {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
booktitle	= {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}
}


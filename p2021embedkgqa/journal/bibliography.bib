@inproceedings{saxena-etal-2020-improving,
title = {Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings},
author = {Saxena, Apoorv  and
  Tripathi, Aditay  and
  Talukdar, Partha},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
month = 7,
year = {2020},
address = {Online},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.acl-main.412},
doi = {10.18653/v1/2020.acl-main.412},
pages = {4498--4507},
}

@ARTICLE{NumPy,
author  = {Harris, Charles R. and Millman, K. Jarrod and
        van der Walt, Stéfan J and Gommers, Ralf and
        Virtanen, Pauli and Cournapeau, David and
        Wieser, Eric and Taylor, Julian and Berg, Sebastian and
        Smith, Nathaniel J. and Kern, Robert and Picus, Matti and
        Hoyer, Stephan and van Kerkwijk, Marten H. and
        Brett, Matthew and Haldane, Allan and
        Fernández del Río, Jaime and Wiebe, Mark and
        Peterson, Pearu and Gérard-Marchant, Pierre and
        Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and
        Abbasi, Hameer and Gohlke, Christoph and
        Oliphant, Travis E.},
title   = {Array programming with {NumPy}},
journal = {Nature},
year    = {2020},
volume  = {585},
pages   = {357–362},
doi     = {10.1038/s41586-020-2649-2}
}

@misc{wang2019relational,
  title={A Relational Tucker Decomposition for Multi-Relational Link Prediction}, 
  author={Yanjie Wang and Samuel Broscheit and Rainer Gemulla},
  year={2019},
  eprint={1902.00898},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{GRU,
  title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, 
  author={Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
  year={2014},
  eprint={1412.3555},
  archivePrefix={arXiv},
  primaryClass={cs.NE}
}

@article{lstm,
added-at = {2016-11-15T08:49:43.000+0100},
author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
url = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
interhash = {0692b471c4b9ae65d00affebc09fb467},
intrahash = {a4a80026d24955b267cae636aa8abe4a},
journal = {Neural computation},
keywords = {lstm rnn},
number = 8,
pages = {1735--1780},
publisher = {MIT Press},
timestamp = {2016-11-15T08:49:43.000+0100},
title = {Long short-term memory},
volume = 9,
year = 1997
}



@misc{kg-embedding-evaluation,
  title={On Evaluating Embedding Models for Knowledge Base Completion}, 
  author={Yanjie Wang and Daniel Ruffinelli and Rainer Gemulla and Samuel Broscheit and Christian Meilicke},
  year={2019},
  eprint={1810.07180},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}

@misc{simple-embedding,
  title={SimplE Embedding for Link Prediction in Knowledge Graphs}, 
  author={Seyed Mehran Kazemi and David Poole},
  year={2018},
  eprint={1802.04868},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

@ARTICLE{SBERT-WK,
author={B. {Wang} and C. -. J. {Kuo}},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
title={{SBERT-WK}: A Sentence Embedding Method by Dissecting {BERT}-Based Word Models}, 
year={2020},
volume={28},
pages={2146-2157}
}

@inproceedings{yago,
author = {Suchanek, Fabian M. and Kasneci, Gjergji and Weikum, Gerhard},
title = {Yago: A Core of Semantic Knowledge},
year = {2007},
isbn = {9781595936547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1242572.1242667},
doi = {10.1145/1242572.1242667},
abstract = {We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95\%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.},
booktitle = {Proceedings of the 16th International Conference on World Wide Web},
pages = {697–706},
numpages = {10},
keywords = {WordNet, wikipedia},
location = {Banff, Alberta, Canada},
series = {WWW '07}
}

@article{NELL,
author = {Mitchell, T. and Cohen, W. and Hruschka, E. and Talukdar, P. and Yang, B. and Betteridge, J. and Carlson, A. and Dalvi, B. and Gardner, M. and Kisiel, B. and Krishnamurthy, J. and Lao, N. and Mazaitis, K. and Mohamed, T. and Nakashole, N. and Platanios, E. and Ritter, A. and Samadi, M. and Settles, B. and Wang, R. and Wijaya, D. and Gupta, A. and Chen, X. and Saparov, A. and Greaves, M. and Welling, J.},
title = {Never-Ending Learning},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3191513},
doi = {10.1145/3191513},
abstract = {Whereas people learn many different types of knowledge from diverse experiences over many years, and become better learners over time, most current machine learning systems are much more narrow, learning just a single function or data model based on statistical analysis of a single data set. We suggest that people learn better than computers precisely because of this difference, and we suggest a key direction for machine learning research is to develop software architectures that enable intelligent agents to also learn many types of knowledge, continuously over many years, and to become better learners over time. In this paper we define more precisely this never-ending learning paradigm for machine learning, and we present one case study: the Never-Ending Language Learner (NELL), which achieves a number of the desired properties of a never-ending learner. NELL has been learning to read the Web 24hrs/day since January 2010, and so far has acquired a knowledge base with 120mn diverse, confidence-weighted beliefs (e.g., servedWith(tea,biscuits)), while learning thousands of interrelated functions that continually improve its reading competence over time. NELL has also learned to reason over its knowledge base to infer new beliefs it has not yet read from those it has, and NELL is inventing new relational predicates to extend the ontology it uses to represent beliefs. We describe the design of NELL, experimental results illustrating its behavior, and discuss both its successes and shortcomings as a case study in never-ending learning. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.},
journal = {Commun. ACM},
month = 4,
pages = {103–115},
numpages = {13}
}

@misc{freebase:datadumps,
author = {Google},
title = {Freebase Data Dumps},
url = {https://developers.google.com/freebase/data},
edition = {Oct 28, 2013},
year = {2013}
}

@article{dbpedia2015,
title={Dbpedia--a large-scale, multilingual knowledge base extracted from wikipedia},
author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, S{\"o}ren and others},
journal={Semantic web},
volume={6},
number={2},
pages={167--195},
year={2015},
publisher={IOS Press}
}

@inproceedings{webqsp-dataset,
title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
author = {Yih, Wen-tau  and
  Richardson, Matthew  and
  Meek, Chris  and
  Chang, Ming-Wei  and
  Suh, Jina},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
month = 8,
year = {2016},
address = {Berlin, Germany},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/P16-2033},
doi = {10.18653/v1/P16-2033},
pages = {201--206},
}

@inproceedings{metaqa-dataset,
title={Variational Reasoning for Question Answering with Knowledge Graph},
author={Zhang, Yuyu and Dai, Hanjun and Kozareva, Zornitsa and Smola, Alexander J and Song, Le},
booktitle={AAAI},
year={2018}
}

@inproceedings{huggingface-transformers-package-2020,
title = {Transformers: State-of-the-Art Natural Language Processing},
author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
month = 10,
year = {2020},
address = {Online},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
pages = {38--45},
}

@misc{beltagy2020longformer,
  title={Longformer: The Long-Document Transformer}, 
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year={2020},
  eprint={2004.05150},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{openKE,
title={Openke: An open toolkit for knowledge embedding},
author={Han, Xu and Cao, Shulin and Lv, Xin and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Li, Juanzi},
booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing: system demonstrations},
pages={139--144},
year={2018}
}

@misc{ampligraph,
author= {Luca Costabello and
      Sumit Pai and
      Chan Le Van and
      Rory McGrath and
      Nicholas McCarthy and
      Pedro Tabacof},
title = {{AmpliGraph: a Library for Representation Learning on Knowledge Graphs}},
month = 3,
year  = 2019,
doi   = {10.5281/zenodo.2595043},
url   = {https://doi.org/10.5281/zenodo.2595043}
}

@article{ali2020pykeen,
title={PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Emebddings},
author={Ali, Mehdi and Berrendorf, Max and Hoyt, Charles Tapley and Vermue, Laurent and Sharifzadeh, Sahand and Tresp, Volker and Lehmann, Jens},
journal={arXiv preprint arXiv:2007.14175},
year={2020}
}

@inproceedings{libkge,
title={LibKGE-A knowledge graph embedding library for reproducible research},
author={Broscheit, Samuel and Ruffinelli, Daniel and Kochsiek, Adrian and Betz, Patrick and Gemulla, Rainer},
booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
pages={165--174},
year={2020}
}

@inproceedings{ruffinelli2019you,
title={You CAN teach an old dog new tricks! on training knowledge graph embeddings},
author={Ruffinelli, Daniel and Broscheit, Samuel and Gemulla, Rainer},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{albert,
title={Albert: A lite bert for self-supervised learning of language representations},
author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
journal={arXiv preprint arXiv:1909.11942},
year={2019}
}

@inproceedings{bert-2019,
title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
author = {Devlin, Jacob  and
  Chang, Ming-Wei  and
  Lee, Kenton  and
  Toutanova, Kristina},
booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
month = 6,
year = {2019},
address = {Minneapolis, Minnesota},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/N19-1423},
doi = {10.18653/v1/N19-1423},
pages = {4171--4186},
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
}

@inproceedings{kge_compression,
title={Knowledge Graph Embedding Compression},
author={Sachan, Mrinmaya},
booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
pages={2681--2691},
year={2020}
}

@inproceedings{xlnet,
title={Xlnet: Generalized autoregressive pretraining for language understanding},
author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
booktitle={Advances in neural information processing systems},
pages={5753--5763},
year={2019}
}

@misc{reformer,
  title={Reformer: The Efficient Transformer}, 
  author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
  year={2020},
  eprint={2001.04451},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{performer,
  title={Rethinking Attention with Performers}, 
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
  year={2020},
  eprint={2009.14794},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year={2019},
  eprint={1907.11692},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{distmult,
  title={Embedding Entities and Relations for Learning and Inference in Knowledge Bases}, 
  author={Bishan Yang and Wen-tau Yih and Xiaodong He and Jianfeng Gao and Li Deng},
  year={2015},
  eprint={1412.6575},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{tucker2019,
title={Tucker: Tensor factorization for knowledge graph completion},
author={Bala{\v{z}}evi{\'c}, Ivana and Allen, Carl and Hospedales, Timothy M},
journal={arXiv preprint arXiv:1901.09590},
year={2019}
}

@inproceedings{ComplEx2016,
author = {Trouillon, Th\'{e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, \'{E}ric and Bouchard, Guillaume},
title = {Complex Embeddings for Simple Link Prediction},
year = {2016},
publisher = {JMLR.org},
abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {2071–2080},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}


@inproceedings{reimers-2019-sentence-bert,
title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
author = {Reimers, Nils and Gurevych, Iryna},
booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
month = {11},
year = {2019},
publisher = {Association for Computational Linguistics},
url = {https://arxiv.org/abs/1908.10084},
}

@article{leimeister2018skip,
title={Skip-gram word embeddings in hyperbolic space},
author={Leimeister, Matthias and Wilson, Benjamin J},
journal={arXiv preprint arXiv:1809.01498},
year={2018}
}

@inproceedings{multi-poincare-emb,
title={Multi-relational poincar{\'e} graph embeddings},
author={Balazevic, Ivana and Allen, Carl and Hospedales, Timothy},
booktitle={Advances in Neural Information Processing Systems},
pages={4463--4473},
year={2019}
}

@inproceedings{NIPS2017_poincare_embedding,
author = {Nickel, Maximillian and Kiela, Douwe},
booktitle = {Advances in Neural Information Processing Systems},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6338--6347},
publisher = {Curran Associates, Inc.},
title = {Poincar\'{e} Embeddings for Learning Hierarchical Representations},
url = {https://proceedings.neurips.cc/paper/2017/file/59dfa2df42d9e3d41f5b02bfc32229dd-Paper.pdf},
volume = {30},
year = {2017}
}


@inproceedings{dhingra-etal-2018-embedding,
title = {Embedding Text in Hyperbolic Spaces},
author = {Dhingra, Bhuwan  and
  Shallue, Christopher  and
  Norouzi, Mohammad  and
  Dai, Andrew  and
  Dahl, George},
booktitle = {Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12)},
month = 6,
year = {2018},
address = {New Orleans, Louisiana, USA},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/W18-1708},
doi = {10.18653/v1/W18-1708},
pages = {59--69},
abstract = {Natural language text exhibits hierarchical structure in a variety of respects. Ideally, we could incorporate our prior knowledge of this hierarchical structure into unsupervised learning algorithms that work on text data. Recent work by Nickel and Kiela (2017) proposed using hyperbolic instead of Euclidean embedding spaces to represent hierarchical data and demonstrated encouraging results when embedding graphs. In this work, we extend their method with a re-parameterization technique that allows us to learn hyperbolic embeddings of arbitrarily parameterized objects. We apply this framework to learn word and sentence embeddings in hyperbolic space in an unsupervised manner from text corpora. The resulting embeddings seem to encode certain intuitive notions of hierarchy, such as word-context frequency and phrase constituency. However, the implicit continuous hierarchy in the learned hyperbolic space makes interrogating the model{'}s learned hierarchies more difficult than for models that learn explicit edges between items. The learned hyperbolic embeddings show improvements over Euclidean embeddings in some {--} but not all {--} downstream tasks, suggesting that hierarchical organization is more useful for some tasks than others.},
}

@misc{wang2020coke,
  title={CoKE: Contextualized Knowledge Graph Embedding}, 
  author={Quan Wang and Pingping Huang and Haifeng Wang and Songtai Dai and Wenbin Jiang and Jing Liu and Yajuan Lyu and Yong Zhu and Hua Wu},
  year={2020},
  eprint={1911.02168},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}

@inproceedings{interacte2020,
title={InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions},
author={Vashishth, Shikhar and Sanyal, Soumya and Nitin, Vikram and Agrawal, Nilesh and Talukdar, Partha},
booktitle={Proceedings of the 34th AAAI Conference on Artificial Intelligence},
pages={3009--3016},
publisher={AAAI Press},
url={https://aaai.org/ojs/index.php/AAAI/article/view/5694},
year={2020}
}

@inproceedings{gupta-etal-2019-care,
title = {{C}a{R}e: Open Knowledge Graph Embeddings},
author = {Gupta, Swapnil  and
  Kenkre, Sreyash  and
  Talukdar, Partha},
booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
month = 11,
year = {2019},
address = {Hong Kong, China},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/D19-1036},
doi = {10.18653/v1/D19-1036},
pages = {378--388},
abstract = {Open Information Extraction (OpenIE) methods are effective at extracting (noun phrase, relation phrase, noun phrase) triples from text, e.g., (Barack Obama, took birth in, Honolulu). Organization of such triples in the form of a graph with noun phrases (NPs) as nodes and relation phrases (RPs) as edges results in the construction of Open Knowledge Graphs (OpenKGs). In order to use such OpenKGs in downstream tasks, it is often desirable to learn embeddings of the NPs and RPs present in the graph. Even though several Knowledge Graph (KG) embedding methods have been recently proposed, all of those methods have targeted Ontological KGs, as opposed to OpenKGs. Straightforward application of existing Ontological KG embedding methods to OpenKGs is challenging, as unlike Ontological KGs, OpenKGs are not canonicalized, i.e., a real-world entity may be represented using multiple nodes in the OpenKG, with each node corresponding to a different NP referring to the entity. For example, nodes with labels Barack Obama, Obama, and President Obama may refer to the same real-world entity Barack Obama. Even though canonicalization of OpenKGs has received some attention lately, output of such methods has not been used to improve OpenKG embed- dings. We fill this gap in the paper and propose Canonicalization-infused Representations (CaRe) for OpenKGs. Through extensive experiments, we observe that CaRe enables existing models to adapt to the challenges in OpenKGs and achieve substantial improvements for the link prediction task.},
}

@inproceedings{chami-etal-2020-low,
title = {Low-Dimensional Hyperbolic Knowledge Graph Embeddings},
author = {Chami, Ines  and
  Wolf, Adva  and
  Juan, Da-Cheng  and
  Sala, Frederic  and
  Ravi, Sujith  and
  R{\'e}, Christopher},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
month = 7,
year = {2020},
address = {Online},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.acl-main.617},
doi = {10.18653/v1/2020.acl-main.617},
pages = {6901--6914},
abstract = {Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1{\%} in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6{\%} on WN18RR and 57.7{\%} on YAGO3-10.},
}